# 📋 프로젝트 TODO 리스트

> **마지막 업데이트**: 2025-10-12
> **현재 버전**: v1.0 - 크롤러 성능 최적화 완료 (51.3% 개선)

---

## 🎯 현재 상태

### ✅ 완료된 최적화 (2025-10-12)
- [x] **domcontentloaded 적용** - networkidle → domcontentloaded (초기 로딩 5-10초 단축)
- [x] **API 대기 시간 단축** - 2.5s → 1.5s
- [x] **빠른 종료 조건** - 8회 → 3회 연속 체크
- [x] **동적 대기 시간** - API 감지(0.3s) / 미감지(1.0s)
- [x] **스크롤 거리 최적화** - 500px → 800px (스크롤 횟수 35% 감소)
- [x] **성공/실패 카운팅 로직 수정**

### 📊 성능 지표
```
원본:     516,854ms (8분 37초)
현재:     251,633ms (4분 12초)
개선률:   -51.3% ✅

처리 속도: 0.99 매물/초 (초기 0.48의 2배)
성공률:    100% (5/5 단지)
데이터:    250개 매물 수집 (무결성 100%)
```

---

## 🎯 우선순위 변경

> **사용자 요청**: 실거래가 API 등 데이터 활용 기능은 나중으로 미루고,
> DB 도입, 알림, 대시보드, 스케줄링, AI 예측 등 인프라 고도화를 우선 진행

**현재 작업 순서**:
1. ✅ Phase 1 (데이터 활용) → **보류** 🔄
2. 🚀 Phase 2 (사용자 경험) → **진행 중**
3. 🚀 Phase 3 (인프라 & 안정성) → **진행 중**
4. 🚀 Phase 4 (고급 기능) → **진행 중**

---

## 📅 Phase 1: 인프라 & 데이터베이스 (우선순위: 최우선 🔥)

### 1️⃣ 실거래가 데이터 수집 구현 ⭐⭐⭐
> **목표**: Mock 데이터를 실제 공공데이터포털 API로 교체
> **예상 시간**: 3-4시간
> **담당 파일**: `app/api/real-price/route.ts`, `components/RealPriceAnalysis.tsx`

- [ ] 공공데이터포털 API 키 발급
  - [ ] 국토교통부 아파트 실거래가 API 신청
  - [ ] API 키 환경 변수 설정 (`.env.local`)
- [ ] API 연동 로직 구현
  - [ ] `/app/api/real-price/route.ts` 실제 API 호출로 수정
  - [ ] 단지명 → 법정동코드 변환 로직
  - [ ] 날짜 범위 파라미터 처리
- [ ] 데이터 캐싱 구현
  - [ ] 중복 요청 방지 (메모리 or Redis)
  - [ ] 캐시 TTL 설정 (1시간)
- [ ] 에러 처리
  - [ ] API 한도 초과 처리
  - [ ] 데이터 없음 처리
  - [ ] Fallback UI 표시
- [ ] 테스트
  - [ ] 5개 단지 실거래가 수집 테스트
  - [ ] 에러 케이스 테스트

**완료 조건**: 실거래가 그래프에 실제 데이터가 표시되고, 평균/최고/최저가 통계가 정확함

---

### 2️⃣ 데이터 필터링 & 검색 강화 ⭐⭐⭐
> **목표**: 사용자가 원하는 매물을 빠르게 찾기
> **예상 시간**: 2-3시간
> **담당 파일**: `app/complexes/page.tsx`, `components/ComplexTable.tsx`

- [ ] 검색 기능
  - [ ] 단지명 실시간 검색 (debounce 적용)
  - [ ] 검색 하이라이트 표시
- [ ] 필터 컴포넌트 구현
  - [ ] 가격 범위 슬라이더 (min-max)
  - [ ] 면적 범위 슬라이더
  - [ ] 거래 유형 체크박스 (매매/전세/월세)
  - [ ] 즐겨찾기만 보기 토글
- [ ] 정렬 기능
  - [ ] 가격순 (오름차순/내림차순)
  - [ ] 면적순
  - [ ] 최신순 (크롤링 날짜)
- [ ] UI/UX 개선
  - [ ] 필터 적용 상태 표시 (뱃지)
  - [ ] 필터 초기화 버튼
  - [ ] 결과 개수 표시
- [ ] 상태 관리
  - [ ] URL 쿼리 파라미터로 필터 상태 저장
  - [ ] 페이지 새로고침 시 필터 유지

**완료 조건**: 모든 필터가 정상 작동하고, URL로 필터 상태 공유 가능

---

### 3️⃣ CSV 내보내기 개선 ⭐⭐
> **목표**: 엑셀에서 분석 가능한 형태로 제공
> **예상 시간**: 1-2시간
> **담당 파일**: `app/complexes/page.tsx`, `lib/csvExport.ts` (신규)

- [ ] CSV 내보내기 유틸 작성
  - [ ] UTF-8 BOM 추가 (한글 깨짐 방지)
  - [ ] 엑셀 친화적 포맷 변환
  - [ ] 날짜 포맷 변환
- [ ] 프론트엔드 다운로드 버튼
  - [ ] "CSV 다운로드" 버튼 추가
  - [ ] 필터링된 결과만 내보내기
  - [ ] 파일명에 날짜 포함
- [ ] 추가 컬럼
  - [ ] 크롤링 날짜
  - [ ] 단지 세대수/동수
  - [ ] 계산된 평당 가격
- [ ] 테스트
  - [ ] 엑셀에서 한글 깨짐 확인
  - [ ] 대용량 데이터 (1000+ 행) 테스트

**완료 조건**: 엑셀에서 한글이 깨지지 않고, 필터링된 데이터가 정확히 내보내짐

---

## 📅 Phase 2: 사용자 경험 개선 (우선순위: 중간)

### 4️⃣ 크롤링 히스토리 관리 ⭐⭐
> **목표**: 이전 크롤링 결과 보관 및 비교
> **예상 시간**: 4-5시간
> **담당 파일**: `app/history/page.tsx` (신규), `logic/nas_playwright_crawler.py`

- [ ] 히스토리 저장 로직
  - [ ] 크롤링 결과 DB 저장 (또는 파일 시스템)
  - [ ] 크롤링 메타데이터 저장 (날짜, 성공률, 소요 시간)
  - [ ] 최대 보관 기간 설정 (예: 30일)
- [ ] 히스토리 목록 페이지
  - [ ] 날짜별 크롤링 결과 목록
  - [ ] 각 결과의 통계 요약 (단지 수, 매물 수, 소요 시간)
  - [ ] 결과 상세 보기 링크
- [ ] 결과 비교 기능
  - [ ] 두 날짜 선택 UI
  - [ ] 가격 변동 표시 (상승/하락)
  - [ ] 신규/삭제 매물 하이라이트
  - [ ] 변동 그래프 (Recharts)
- [ ] 데이터 정리 기능
  - [ ] 오래된 히스토리 자동 삭제
  - [ ] 수동 삭제 버튼

**완료 조건**: 과거 크롤링 결과를 조회하고, 두 시점을 비교하여 가격 변동을 확인 가능

---

### 5️⃣ 알림 & 모니터링 기능 ⭐⭐
> **목표**: 관심 매물 변동 시 알림
> **예상 시간**: 3-4시간
> **담당 파일**: `app/alerts/page.tsx` (신규), `app/api/alerts/route.ts` (신규)

- [ ] 알림 조건 설정 UI
  - [ ] 가격대 설정 (min-max)
  - [ ] 면적 설정
  - [ ] 단지 선택 (멀티 셀렉트)
  - [ ] 거래 유형 선택
- [ ] 알림 로직 구현
  - [ ] 크롤링 후 알림 조건 체크
  - [ ] 조건 만족 매물 찾기
  - [ ] 가격 하락 감지 (히스토리 비교)
- [ ] 알림 전송
  - [ ] 브라우저 알림 (Notification API)
  - [ ] 이메일 알림 (옵션: nodemailer)
  - [ ] 웹훅 지원 (Slack, Discord 등)
- [ ] 알림 히스토리
  - [ ] 발송된 알림 목록
  - [ ] 알림 읽음 상태

**완료 조건**: 설정한 조건에 맞는 매물이 나타나면 알림을 받음

---

### 6️⃣ 대시보드 통계 강화 ⭐
> **목표**: 데이터 인사이트 제공
> **예상 시간**: 2-3시간
> **담당 파일**: `app/page.tsx`, `components/StatCard.tsx`

- [ ] 추가 통계 차트
  - [ ] 단지별 평균 가격 막대 차트
  - [ ] 거래 유형 분포 파이 차트
  - [ ] 면적대별 가격 분포 히스토그램
- [ ] Top 랭킹
  - [ ] 가장 비싼 매물 Top 5
  - [ ] 가장 저렴한 매물 Top 5
  - [ ] 매물 많은 단지 Top 5
- [ ] 크롤링 통계
  - [ ] 시간대별 성공률 그래프
  - [ ] 평균 소요 시간
  - [ ] 수집 속도 추이
- [ ] UI 개선
  - [ ] 반응형 차트 레이아웃
  - [ ] 툴팁 상세 정보
  - [ ] 다크 모드 지원

**완료 조건**: 대시보드에서 한눈에 데이터 인사이트를 파악 가능

---

## 📅 Phase 3: 인프라 & 안정성 (우선순위: 낮음)

### 7️⃣ 데이터베이스 도입 ⭐⭐⭐
> **목표**: JSON 파일 → DB로 전환
> **예상 시간**: 5-6시간
> **기술 스택**: PostgreSQL + Prisma (권장)

- [ ] DB 선택 및 설정
  - [ ] PostgreSQL 설치 (Docker)
  - [ ] Prisma 설치 및 초기화
  - [ ] 환경 변수 설정
- [ ] 스키마 설계
  - [ ] `complexes` 테이블 (단지 정보)
  - [ ] `articles` 테이블 (매물 정보)
  - [ ] `crawl_history` 테이블 (크롤링 히스토리)
  - [ ] `favorites` 테이블 (즐겨찾기)
  - [ ] `alerts` 테이블 (알림 설정)
- [ ] 마이그레이션
  - [ ] 기존 JSON 데이터 → DB 이관 스크립트
  - [ ] 데이터 검증
- [ ] API 수정
  - [ ] `/api/results` → DB 쿼리로 변경
  - [ ] `/api/crawl` → DB 저장으로 변경
  - [ ] 기타 API 엔드포인트 수정
- [ ] 인덱스 최적화
  - [ ] 검색용 인덱스 (단지명, 가격)
  - [ ] 정렬용 인덱스
- [ ] 테스트
  - [ ] CRUD 작동 확인
  - [ ] 성능 테스트 (1000+ 레코드)

**완료 조건**: 모든 데이터가 DB에 저장되고, JSON 파일 의존성 제거

---

### 8️⃣ 에러 복구 & 재시도 로직 ⭐⭐
> **목표**: 일시적 오류 자동 복구
> **예상 시간**: 2-3시간
> **담당 파일**: `logic/nas_playwright_crawler.py`

- [ ] 단지별 독립 실행
  - [ ] try-except로 각 단지 감싸기
  - [ ] 1개 실패해도 나머지 계속 진행
  - [ ] 실패한 단지 목록 반환
- [ ] 재시도 로직
  - [ ] 네트워크 오류 시 자동 재시도 (최대 3회)
  - [ ] Exponential backoff (1s, 2s, 4s)
  - [ ] 재시도 카운터 로깅
- [ ] 상세 에러 로깅
  - [ ] 실패 이유 분류 (네트워크/타임아웃/DOM 오류)
  - [ ] 스크린샷 저장 (실패 시)
  - [ ] 에러 스택 트레이스
- [ ] 부분 성공 처리
  - [ ] 부분 결과도 JSON 저장
  - [ ] 성공/실패 단지 구분
  - [ ] UI에 경고 표시
- [ ] 재시도 API
  - [ ] 실패한 단지만 재크롤링 엔드포인트
  - [ ] UI에 "재시도" 버튼 추가

**완료 조건**: 일시적 네트워크 오류가 발생해도 자동 복구되고, 부분 성공 결과가 저장됨

---

### 9️⃣ 스케줄링 & 자동화 ⭐
> **목표**: 정기적 자동 크롤링
> **예상 시간**: 2-3시간
> **기술 스택**: node-cron or Linux cron

- [ ] 스케줄러 구현
  - [ ] node-cron 설치
  - [ ] cron 표현식 파서
  - [ ] 스케줄 저장 (DB or JSON)
- [ ] 설정 UI
  - [ ] 스케줄 설정 페이지
  - [ ] cron 표현식 빌더
  - [ ] 실행 히스토리 표시
- [ ] 자동 실행
  - [ ] 크롤링 자동 트리거
  - [ ] 실행 로그 저장
  - [ ] 실패 시 재시도
- [ ] 알림
  - [ ] 실행 완료 이메일 리포트
  - [ ] 실패 시 긴급 알림
  - [ ] 통계 요약 포함
- [ ] 제어
  - [ ] 스케줄 일시 중지/재개
  - [ ] 수동 실행 버튼

**완료 조건**: 매일 오전 9시에 자동으로 크롤링이 실행되고, 결과 이메일을 받음

---

## 📅 Phase 4: 고급 기능 (선택적)

### 🔟 AI 시세 예측 ⭐
> **목표**: 머신러닝으로 시세 예측
> **예상 시간**: 10+ 시간
> **기술 스택**: Python (scikit-learn or TensorFlow)

- [ ] 데이터 수집
  - [ ] 과거 실거래가 데이터 (최소 1년)
  - [ ] 현재 매물 가격
  - [ ] 단지 정보 (세대수, 동수, 준공연도 등)
  - [ ] 외부 데이터 (금리, 인구 등)
- [ ] 데이터 전처리
  - [ ] 결측치 처리
  - [ ] 이상치 제거
  - [ ] 정규화
  - [ ] Feature engineering
- [ ] 모델 선택 및 학습
  - [ ] 선형 회귀 (베이스라인)
  - [ ] Random Forest
  - [ ] LSTM (시계열 예측)
  - [ ] 하이퍼파라미터 튜닝
- [ ] 평가 및 검증
  - [ ] Train/Test split
  - [ ] 교차 검증
  - [ ] RMSE, MAE 계산
- [ ] API 통합
  - [ ] 예측 API 엔드포인트
  - [ ] 결과 시각화
  - [ ] 신뢰 구간 표시
- [ ] UI 구현
  - [ ] 예측 결과 페이지
  - [ ] 예측 vs 실제 비교 차트
  - [ ] 모델 정확도 표시

**완료 조건**: 단지별 향후 3개월 시세를 예측하고, 정확도가 80% 이상

---

## 📅 Phase 5: 문서화 & 배포

### 1️⃣1️⃣ 문서 업데이트 ✅
> **목표**: 모든 변경사항을 문서에 반영
> **예상 시간**: 1-2시간

- [x] TODO.md 작성 (현재 문서)
- [ ] docs/CHANGELOG.md 작성
  - [ ] 성능 최적화 내역 기록
  - [ ] 버전별 변경사항
- [ ] docs/API.md 업데이트
  - [ ] 새로운 API 엔드포인트 문서화
  - [ ] 요청/응답 예시
- [ ] docs/PERFORMANCE.md 작성
  - [ ] 최적화 전/후 비교
  - [ ] 성능 측정 방법
- [ ] README.md 업데이트
  - [ ] 새로운 기능 추가
  - [ ] 스크린샷 업데이트

**완료 조건**: 모든 기능이 문서화되고, 새로운 개발자가 문서만으로 프로젝트 이해 가능

---

### 1️⃣2️⃣ 프로덕션 배포 준비
> **목표**: 실제 서비스 런칭 준비
> **예상 시간**: 3-4시간

- [ ] 환경 변수 정리
  - [ ] `.env.example` 작성
  - [ ] 민감 정보 제거
  - [ ] 환경별 설정 (dev/prod)
- [ ] Docker 최적화
  - [ ] Multi-stage build
  - [ ] 이미지 크기 최적화
  - [ ] Health check 추가
- [ ] 모니터링 설정
  - [ ] 로그 수집 (Winston or Pino)
  - [ ] 에러 트래킹 (Sentry)
  - [ ] 성능 모니터링 (Prometheus)
- [ ] CI/CD 구축
  - [ ] GitHub Actions 워크플로우
  - [ ] 자동 테스트
  - [ ] 자동 배포
- [ ] 보안 강화
  - [ ] API Rate limiting
  - [ ] CORS 설정
  - [ ] Helmet.js 적용
- [ ] 백업 전략
  - [ ] DB 자동 백업
  - [ ] 크롤링 결과 백업
  - [ ] 복구 테스트

**완료 조건**: 프로덕션 환경에 배포되고, 모니터링 대시보드에서 헬스 체크 확인

---

## 🎯 추천 작업 순서

### Week 1 (8-10시간)
```
✅ 1. TODO.md 작성 (현재 완료)
□ 2. 실거래가 API 연동 (3-4h)
□ 3. 필터링 & 검색 (2-3h)
□ 4. CSV 내보내기 (1-2h)
```

### Week 2 (10-12시간)
```
□ 5. DB 도입 (5-6h)
□ 6. 히스토리 관리 (4-5h)
□ 7. 문서 업데이트 (1-2h)
```

### Week 3 (8-10시간)
```
□ 8. 알림 기능 (3-4h)
□ 9. 대시보드 강화 (2-3h)
□ 10. 에러 복구 로직 (2-3h)
```

### Week 4+ (선택적)
```
□ 11. 스케줄링 (2-3h)
□ 12. 배포 준비 (3-4h)
□ 13. AI 예측 (10+h)
```

---

## 📝 메모

### 보류된 최적화
- **병렬 처리**: 네이버 차단 위험으로 보류 (현재 순차 처리 유지)
- **더 공격적인 대기 시간**: 데이터 무결성 우선

### 참고 링크
- [공공데이터포털](https://www.data.go.kr/)
- [Prisma 공식 문서](https://www.prisma.io/docs)
- [Recharts 공식 문서](https://recharts.org/)
- [Next.js App Router](https://nextjs.org/docs/app)

---

## 📞 문의 및 제안

새로운 기능 제안이나 버그 리포트는 이슈로 등록해주세요.
