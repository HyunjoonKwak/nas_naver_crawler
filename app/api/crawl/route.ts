import { NextRequest, NextResponse } from 'next/server';
import { exec } from 'child_process';
import { promisify } from 'util';
import { prisma } from '@/lib/prisma';
import { requireAuth } from '@/lib/auth-utils';
import { rateLimit, rateLimitPresets } from '@/lib/rate-limit';
import { createLogger } from '@/lib/logger';
import fs from 'fs/promises';
import path from 'path';
import {
  detectArticleChanges,
  filterChangesForAlerts,
  getComplexInfo,
  saveNotificationLog,
} from '@/lib/article-tracker';
import {
  sendDiscordNotification,
  createNewArticleEmbed,
  createDeletedArticleEmbed,
  createPriceChangedEmbed,
  createCrawlSummaryEmbed,
} from '@/lib/discord';
import { eventBroadcaster } from '@/lib/eventBroadcaster';
import { calculateDynamicTimeout } from '@/lib/timeoutCalculator';

const execAsync = promisify(exec);
const logger = createLogger('CRAWL');

export const dynamic = 'force-dynamic';

// Global crawl state for progress tracking
let currentCrawlId: string | null = null;

// í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (Batch Insert ë°©ì‹)
async function saveCrawlResultsToDB(crawlId: string, complexNos: string[], userId: string) {
  const baseDir = process.env.NODE_ENV === 'production' ? '/app' : process.cwd();
  const crawledDataDir = path.join(baseDir, 'crawled_data');

  let totalArticles = 0;
  let totalComplexes = 0;
  let errors: string[] = [];

  try {
    // Update status: saving to DB
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'saving',
        currentStep: 'Reading crawl result files',
      },
    });

    // ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ ì°¾ê¸° (complexes_N_ë‚ ì§œ.json í˜•ì‹)
    const files = await fs.readdir(crawledDataDir);
    const jsonFiles = files
      .filter(f => f.startsWith('complexes_') && f.endsWith('.json'))
      .map(f => {
        const fullPath = path.join(crawledDataDir, f);
        const stats = require('fs').statSync(fullPath);
        return { name: f, mtime: stats.mtime };
      })
      .sort((a, b) => b.mtime.getTime() - a.mtime.getTime())  // Sort by modification time (newest first)
      .map(f => f.name);

    if (jsonFiles.length === 0) {
      console.log('No crawl result files found');
      return { totalArticles: 0, totalComplexes: 0, errors: ['No data files found'] };
    }

    // ê°€ì¥ ìµœì‹  íŒŒì¼ ì‚¬ìš© (ìˆ˜ì • ì‹œê°„ ê¸°ì¤€)
    const latestFile = path.join(crawledDataDir, jsonFiles[0]);
    console.log(`Processing file: ${jsonFiles[0]} (latest by mtime)`);

    const rawData = await fs.readFile(latestFile, 'utf-8');
    const crawlData = JSON.parse(rawData);

    // ë°ì´í„°ê°€ ë°°ì—´ì¸ì§€ í™•ì¸
    const dataArray = Array.isArray(crawlData) ? crawlData : [crawlData];
    console.log(`Found ${dataArray.length} complexes in file`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Processing ${dataArray.length} complexes`,
      },
    });

    // ëª¨ë“  ë‹¨ì§€ ì •ë³´ë¥¼ ë¨¼ì € ìˆ˜ì§‘
    const complexesToUpsert: any[] = [];
    const articlesToCreate: any[] = [];
    const complexNoToIdMap = new Map<string, string>();

    for (let i = 0; i < dataArray.length; i++) {
      const data = dataArray[i];

      if (!data.overview || !data.articles) {
        console.log('Skipping item without overview or articles');
        continue;
      }

      const overview = data.overview;
      const articleList = data.articles.articleList || [];

      // ë‹¨ì§€ ì •ë³´ ì¤€ë¹„
      complexesToUpsert.push({
        complexNo: overview.complexNo,
        complexName: overview.complexName,
        totalHousehold: overview.totalHousehold,
        totalDong: overview.totalDong,
        latitude: overview.location?.latitude,
        longitude: overview.location?.longitude,
        address: overview.address,
        roadAddress: overview.roadAddress,
        jibunAddress: overview.jibunAddress,
        beopjungdong: overview.beopjungdong,
        haengjeongdong: overview.haengjeongdong,
        pyeongs: overview.pyeongs || [],
        userId: userId, // í¬ë¡¤ë§ ì‹¤í–‰í•œ ì‚¬ìš©ì ID
      });

      // Note: processedComplexes is updated by Python crawler in real-time
      // No need to update here to avoid progress bar confusion
    }

    // Batch upsert complexes
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Saving complex information',
      },
    });

    for (const complexData of complexesToUpsert) {
      const complex = await prisma.complex.upsert({
        where: { complexNo: complexData.complexNo },
        update: complexData,
        create: complexData,
      });
      complexNoToIdMap.set(complexData.complexNo, complex.id);
      totalComplexes++;
    }

    // ëª¨ë“  ë§¤ë¬¼ ë°ì´í„° ì¤€ë¹„ (complexId ë§¤í•‘)
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Preparing article data',
      },
    });

    for (const data of dataArray) {
      if (!data.overview || !data.articles) continue;

      const overview = data.overview;
      const articleList = data.articles.articleList || [];
      const complexId = complexNoToIdMap.get(overview.complexNo);

      if (!complexId) {
        logger.error(`Complex ID not found for ${overview.complexNo}`);
        continue;
      }

      for (const article of articleList) {
        articlesToCreate.push({
          articleNo: article.articleNo,
          complexId: complexId,
          realEstateTypeName: article.realEstateTypeName || 'ì•„íŒŒíŠ¸',
          tradeTypeName: article.tradeTypeName,
          dealOrWarrantPrc: article.dealOrWarrantPrc,
          rentPrc: article.rentPrc,
          area1: parseFloat(article.area1) || 0,
          area2: article.area2 ? parseFloat(article.area2) : null,
          floorInfo: article.floorInfo,
          direction: article.direction,
          articleConfirmYmd: article.articleConfirmYmd,
          buildingName: article.buildingName,
          sameAddrCnt: article.sameAddrCnt ? parseInt(article.sameAddrCnt) : null,
          realtorName: article.realtorName,
          articleFeatureDesc: article.articleFeatureDesc,
          tagList: article.tagList || [],
        });
      }
    }

    // Batch insert articles (ë‹¨ì§€ë³„ë¡œ ê¸°ì¡´ ë§¤ë¬¼ ì „ì²´ ì‚­ì œ í›„ ì¬ìƒì„±)
    // 1. í¬ë¡¤ë§ëœ ë‹¨ì§€ë“¤ì˜ complexId ëª©ë¡ ì¶”ì¶œ
    const crawledComplexIds = Array.from(new Set(articlesToCreate.map(a => a.complexId)));

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Deleting old articles from ${crawledComplexIds.length} complexes`,
      },
    });

    // 2. í•´ë‹¹ ë‹¨ì§€ë“¤ì˜ ëª¨ë“  ê¸°ì¡´ ë§¤ë¬¼ ì‚­ì œ (ì‚­ì œëœ ë§¤ë¬¼ ë°˜ì˜)
    await prisma.article.deleteMany({
      where: {
        complexId: {
          in: crawledComplexIds,
        },
      },
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Saving articles (${articlesToCreate.length} items)`,
      },
    });

    // 3. Batch insert (skipDuplicatesë¡œ ì•ˆì „í•˜ê²Œ)
    const articlesBeforeInsert = articlesToCreate.length;
    const result = await prisma.article.createMany({
      data: articlesToCreate,
      skipDuplicates: true,
    });

    totalArticles = result.count;

    // ì¤‘ë³µ ì œê±°ëœ ë§¤ë¬¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
    if (articlesBeforeInsert > totalArticles) {
      console.log(`âš ï¸  Duplicates skipped: ${articlesBeforeInsert - totalArticles} articles (${articlesBeforeInsert} â†’ ${totalArticles})`);
    }

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Database save completed',
        processedArticles: totalArticles,
      },
    });

    console.log(`âœ… Batch save completed: ${totalComplexes} complexes, ${totalArticles} articles`);

  } catch (error: any) {
    logger.error('Failed to process crawl data', error);
    errors.push(`Database save error: ${error.message}`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'failed',
        errorMessage: errors.join(', '),
      },
    });
  }

  return { totalArticles, totalComplexes, errors };
}

// ì•Œë¦¼ ë°œì†¡ í•¨ìˆ˜
async function sendAlertsForChanges(complexNos: string[]) {
  console.log('ğŸ”” Checking for alerts...');

  for (const complexNo of complexNos) {
    try {
      // 1. í˜„ì¬ ë§¤ë¬¼ ë°ì´í„° ì¡°íšŒ
      const complexInfo = await getComplexInfo(complexNo);
      if (!complexInfo) {
        console.log(`Complex not found: ${complexNo}`);
        continue;
      }

      const currentArticles = await prisma.article.findMany({
        where: { complexId: complexInfo.id },
      });

      // 2. ë³€ê²½ì‚¬í•­ ê°ì§€
      const changes = await detectArticleChanges(complexNo, currentArticles);

      console.log(`ğŸ“Š Changes for ${complexInfo.complexName}:`, {
        new: changes.newArticles.length,
        deleted: changes.deletedArticles.length,
        priceChanged: changes.priceChangedArticles.length,
      });

      // ë³€ê²½ì‚¬í•­ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
      if (
        changes.newArticles.length === 0 &&
        changes.deletedArticles.length === 0 &&
        changes.priceChangedArticles.length === 0
      ) {
        console.log(`No changes for ${complexInfo.complexName}, skipping alerts`);
        continue;
      }

      // 3. ì•Œë¦¼ ì¡°ê±´ì— ë§ëŠ” ë³€ê²½ì‚¬í•­ í•„í„°ë§
      const alertTargets = await filterChangesForAlerts(complexNo, changes);

      if (alertTargets.length === 0) {
        console.log(`No active alerts for ${complexInfo.complexName}`);
        continue;
      }

      console.log(`ğŸ“¬ Sending alerts to ${alertTargets.length} alert(s)`);

      // 4. ê° ì•Œë¦¼ì— ëŒ€í•´ Discord ì›¹í›… ì „ì†¡
      for (const target of alertTargets) {
        if (!target.alert.webhookUrl) {
          console.log(`No webhook URL for alert: ${target.alert.name}`);
          continue;
        }

        try {
          const embeds: any[] = [];

          // ì‹ ê·œ ë§¤ë¬¼ ì•Œë¦¼
          for (const article of target.newArticles) {
            embeds.push(
              createNewArticleEmbed(article, complexInfo.complexName, complexNo)
            );
          }

          // ì‚­ì œëœ ë§¤ë¬¼ ì•Œë¦¼
          for (const article of target.deletedArticles) {
            embeds.push(
              createDeletedArticleEmbed(article, complexInfo.complexName, complexNo)
            );
          }

          // ê°€ê²© ë³€ë™ ì•Œë¦¼
          for (const { old: oldArticle, new: newArticle } of target.priceChangedArticles) {
            embeds.push(
              createPriceChangedEmbed(
                oldArticle,
                newArticle,
                complexInfo.complexName,
                complexNo
              )
            );
          }

          // ìš”ì•½ ì„ë² ë“œ ì¶”ê°€
          embeds.push(
            createCrawlSummaryEmbed({
              complexName: complexInfo.complexName,
              complexNo,
              newCount: target.newArticles.length,
              deletedCount: target.deletedArticles.length,
              priceChangedCount: target.priceChangedArticles.length,
              totalArticles: currentArticles.length,
              duration: 0, // í¬ë¡¤ë§ ì‹œê°„ì€ ì—¬ê¸°ì„  ë¶ˆí•„ìš”
            })
          );

          // Discordë¡œ ì „ì†¡ (í•œ ë²ˆì— ìµœëŒ€ 10ê°œ embed)
          for (let i = 0; i < embeds.length; i += 10) {
            const batch = embeds.slice(i, i + 10);

            const result = await sendDiscordNotification(target.alert.webhookUrl, {
              username: 'ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬',
              content:
                i === 0
                  ? `ğŸ”” **${target.alert.name}** ì•Œë¦¼\n${complexInfo.complexName}ì— ë³€ê²½ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤!`
                  : undefined,
              embeds: batch,
            });

            // ì•Œë¦¼ ë¡œê·¸ ì €ì¥
            await saveNotificationLog(
              target.alertId,
              'webhook',
              result.success ? 'sent' : 'failed',
              result.success
                ? `Sent ${batch.length} notifications`
                : result.error || 'Unknown error'
            );

            if (!result.success) {
              console.error(`Failed to send alert: ${result.error}`);
            } else {
              console.log(`âœ… Sent ${batch.length} notification(s) for alert: ${target.alert.name}`);
            }

            // Discord API ì†ë„ ì œí•œ ë°©ì§€
            if (i + 10 < embeds.length) {
              await new Promise((resolve) => setTimeout(resolve, 1000));
            }
          }
        } catch (error: any) {
          console.error(`Failed to send alert for ${target.alert.name}:`, error);
          await saveNotificationLog(
            target.alertId,
            'webhook',
            'failed',
            error.message || 'Unknown error'
          );
        }
      }
    } catch (error: any) {
      console.error(`Failed to process alerts for ${complexNo}:`, error);
    }
  }

  console.log('âœ… Alert processing completed');
}

export async function POST(request: NextRequest) {
  const startTime = Date.now();
  let crawlId: string | null = null;

  try {
    // Rate Limiting (ë¶„ë‹¹ 10íšŒ)
    const rateLimitResponse = rateLimit(request, rateLimitPresets.crawl);
    if (rateLimitResponse) return rateLimitResponse;

    // ì‚¬ìš©ì ì¸ì¦ í™•ì¸
    const currentUser = await requireAuth();

    const body = await request.json();
    const { complexNumbers } = body;

    if (!complexNumbers || complexNumbers.length === 0) {
      return NextResponse.json(
        { error: 'ë‹¨ì§€ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.' },
        { status: 400 }
      );
    }

    const complexNosArray = Array.isArray(complexNumbers)
      ? complexNumbers
      : complexNumbers.split(',').map((n: string) => n.trim());

    const complexNos = complexNosArray.join(',');

    // 1. í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ìƒì„± (ì§„í–‰ ì¤‘ ìƒíƒœ)
    const crawlHistory = await prisma.crawlHistory.create({
      data: {
        complexNos: complexNosArray,
        totalComplexes: complexNosArray.length,
        successCount: 0,
        errorCount: 0,
        totalArticles: 0,
        duration: 0,
        status: 'crawling',
        currentStep: 'Starting crawler',
        processedArticles: 0,
        processedComplexes: 0,
        userId: currentUser.id, // í¬ë¡¤ë§ ì‹¤í–‰í•œ ì‚¬ìš©ì ID
      },
    });

    crawlId = crawlHistory.id;
    currentCrawlId = crawlId;

    // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì‹œì‘
    eventBroadcaster.notifyCrawlStart(crawlId, complexNosArray.length);

    // 2. Python í¬ë¡¤ëŸ¬ ì‹¤í–‰ (crawl_id ì „ë‹¬)
    const baseDir = process.env.NODE_ENV === 'production' ? '/app' : process.cwd();
    const command = `python3 ${baseDir}/logic/nas_playwright_crawler.py "${complexNos}" "${crawlId}"`;

    logger.info('Starting crawler', { crawlId, complexNos: complexNosArray.length });

    // ë™ì  íƒ€ì„ì•„ì›ƒ ê³„ì‚°
    const dynamicTimeout = await calculateDynamicTimeout(complexNosArray.length);
    logger.info('Dynamic timeout calculated', {
      seconds: Math.floor(dynamicTimeout / 1000),
      minutes: Math.floor(dynamicTimeout / 60000)
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Crawling ${complexNosArray.length} complexes`,
      },
    });

    const { stdout, stderr } = await execAsync(command, {
      cwd: baseDir,
      maxBuffer: 10 * 1024 * 1024, // 10MB
      timeout: dynamicTimeout, // ë™ì  íƒ€ì„ì•„ì›ƒ (í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ê³„ì‚°)
    });

    // Python ì¶œë ¥ì„ ë¡œê·¸ì— í‘œì‹œ
    if (stdout) {
      console.log('=== Python Crawler Output ===');
      console.log(stdout);
    }
    if (stderr) {
      console.error('=== Python Crawler Errors ===');
      console.error(stderr);
    }

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Crawling completed',
      },
    });

    // 3. í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (Batch Insert)
    logger.info('Saving results to database');
    const dbResult = await saveCrawlResultsToDB(crawlId, complexNosArray, currentUser.id);

    const duration = Date.now() - startTime;
    const status = dbResult.errors.length > 0 ? 'partial' : 'success';

    // 4. ìµœì¢… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        successCount: dbResult.totalComplexes,
        errorCount: complexNosArray.length - dbResult.totalComplexes,
        totalArticles: dbResult.totalArticles,
        duration: Math.floor(duration / 1000), // ì´ˆ ë‹¨ìœ„
        status,
        errorMessage: dbResult.errors.length > 0 ? dbResult.errors.join(', ') : null,
        currentStep: 'Completed',
      },
    });

    currentCrawlId = null;

    // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì™„ë£Œ
    eventBroadcaster.notifyCrawlComplete(crawlId, dbResult.totalArticles);

    logger.info('Crawl completed and saved to DB', {
      complexes: dbResult.totalComplexes,
      articles: dbResult.totalArticles,
      durationSeconds: Math.floor(duration / 1000),
      status
    });

    // 5. ì•Œë¦¼ ë°œì†¡ (ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ì§€ì—° ë°©ì§€)
    sendAlertsForChanges(complexNosArray).catch((error) => {
      logger.error('Failed to send alerts', error);
    });

    return NextResponse.json({
      success: true,
      message: 'í¬ë¡¤ë§ì´ ì™„ë£Œë˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
      crawlId,
      data: {
        complexes: dbResult.totalComplexes,
        articles: dbResult.totalArticles,
        duration: Math.floor(duration / 1000),
        errors: dbResult.errors,
      },
    });

  } catch (error: any) {
    logger.error('Crawling error', error);

    // ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if (crawlId) {
      try {
        const duration = Date.now() - startTime;
        await prisma.crawlHistory.update({
          where: { id: crawlId },
          data: {
            duration: Math.floor(duration / 1000),
            status: 'failed',
            errorMessage: error.message,
            currentStep: 'Failed',
          },
        });

        // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì‹¤íŒ¨
        eventBroadcaster.notifyCrawlFailed(crawlId, error.message);
      } catch (historyError) {
        logger.error('Failed to update error history', historyError);
      }
    }

    currentCrawlId = null;

    return NextResponse.json(
      {
        error: 'í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: error.message,
        crawlId,
      },
      { status: 500 }
    );
  }
}

export async function GET() {
  return NextResponse.json({
    message: 'POST ìš”ì²­ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.',
    example: {
      complexNumbers: ['22065', '12345']
    }
  });
}
