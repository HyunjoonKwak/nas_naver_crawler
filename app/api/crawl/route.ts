import { NextRequest, NextResponse } from 'next/server';
import { exec } from 'child_process';
import { promisify } from 'util';
import { prisma } from '@/lib/prisma';
import fs from 'fs/promises';
import path from 'path';

const execAsync = promisify(exec);

export const dynamic = 'force-dynamic';

// Global crawl state for progress tracking
let currentCrawlId: string | null = null;

// í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (Batch Insert ë°©ì‹)
async function saveCrawlResultsToDB(crawlId: string, complexNos: string[]) {
  const baseDir = process.env.NODE_ENV === 'production' ? '/app' : process.cwd();
  const crawledDataDir = path.join(baseDir, 'crawled_data');

  let totalArticles = 0;
  let totalComplexes = 0;
  let errors: string[] = [];

  try {
    // Update status: saving to DB
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'saving',
        currentStep: 'Reading crawl result files',
      },
    });

    // ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ ì°¾ê¸° (complexes_N_ë‚ ì§œ.json í˜•ì‹)
    const files = await fs.readdir(crawledDataDir);
    const jsonFiles = files
      .filter(f => f.startsWith('complexes_') && f.endsWith('.json'))
      .sort()
      .reverse();

    if (jsonFiles.length === 0) {
      console.log('No crawl result files found');
      return { totalArticles: 0, totalComplexes: 0, errors: ['No data files found'] };
    }

    // ê°€ì¥ ìµœì‹  íŒŒì¼ ì‚¬ìš©
    const latestFile = path.join(crawledDataDir, jsonFiles[0]);
    console.log(`Processing file: ${jsonFiles[0]}`);

    const rawData = await fs.readFile(latestFile, 'utf-8');
    const crawlData = JSON.parse(rawData);

    // ë°ì´í„°ê°€ ë°°ì—´ì¸ì§€ í™•ì¸
    const dataArray = Array.isArray(crawlData) ? crawlData : [crawlData];
    console.log(`Found ${dataArray.length} complexes in file`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Processing ${dataArray.length} complexes`,
      },
    });

    // ëª¨ë“  ë‹¨ì§€ ì •ë³´ë¥¼ ë¨¼ì € ìˆ˜ì§‘
    const complexesToUpsert: any[] = [];
    const articlesToCreate: any[] = [];
    const complexNoToIdMap = new Map<string, string>();

    for (let i = 0; i < dataArray.length; i++) {
      const data = dataArray[i];

      if (!data.overview || !data.articles) {
        console.log('Skipping item without overview or articles');
        continue;
      }

      const overview = data.overview;
      const articleList = data.articles.articleList || [];

      // ë‹¨ì§€ ì •ë³´ ì¤€ë¹„
      complexesToUpsert.push({
        complexNo: overview.complexNo,
        complexName: overview.complexName,
        totalHousehold: overview.totalHousehold,
        totalDong: overview.totalDong,
        latitude: overview.location?.latitude,
        longitude: overview.location?.longitude,
        address: overview.address,
        roadAddress: overview.roadAddress,
        jibunAddress: overview.jibunAddress,
        beopjungdong: overview.beopjungdong,
        haengjeongdong: overview.haengjeongdong,
      });

      // Progress update
      await prisma.crawlHistory.update({
        where: { id: crawlId },
        data: {
          currentStep: `Preparing complex ${i + 1}/${dataArray.length}`,
          processedComplexes: i + 1,
        },
      });
    }

    // Batch upsert complexes
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Saving complex information',
      },
    });

    for (const complexData of complexesToUpsert) {
      const complex = await prisma.complex.upsert({
        where: { complexNo: complexData.complexNo },
        update: complexData,
        create: complexData,
      });
      complexNoToIdMap.set(complexData.complexNo, complex.id);
      totalComplexes++;
    }

    // ëª¨ë“  ë§¤ë¬¼ ë°ì´í„° ì¤€ë¹„ (complexId ë§¤í•‘)
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Preparing article data',
      },
    });

    for (const data of dataArray) {
      if (!data.overview || !data.articles) continue;

      const overview = data.overview;
      const articleList = data.articles.articleList || [];
      const complexId = complexNoToIdMap.get(overview.complexNo);

      if (!complexId) {
        console.error(`Complex ID not found for ${overview.complexNo}`);
        continue;
      }

      for (const article of articleList) {
        articlesToCreate.push({
          articleNo: article.articleNo,
          complexId: complexId,
          realEstateTypeName: article.realEstateTypeName || 'ì•„íŒŒíŠ¸',
          tradeTypeName: article.tradeTypeName,
          dealOrWarrantPrc: article.dealOrWarrantPrc,
          rentPrc: article.rentPrc,
          area1: parseFloat(article.area1) || 0,
          area2: article.area2 ? parseFloat(article.area2) : null,
          floorInfo: article.floorInfo,
          direction: article.direction,
          articleConfirmYmd: article.articleConfirmYmd,
          tagList: article.tagList || [],
        });
      }
    }

    // Batch insert articles (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ deleteMany í›„ createMany)
    // 1. ê¸°ì¡´ articleNoë“¤ ê°€ì ¸ì˜¤ê¸°
    const articleNos = articlesToCreate.map(a => a.articleNo);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Deleting old articles (${articleNos.length} items)`,
      },
    });

    // 2. ê¸°ì¡´ ë§¤ë¬¼ ì‚­ì œ (upsert ëŒ€ì‹  delete + createë¡œ ì„±ëŠ¥ í–¥ìƒ)
    await prisma.article.deleteMany({
      where: {
        articleNo: {
          in: articleNos,
        },
      },
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Saving articles (${articlesToCreate.length} items)`,
      },
    });

    // 3. Batch insert (skipDuplicatesë¡œ ì•ˆì „í•˜ê²Œ)
    const result = await prisma.article.createMany({
      data: articlesToCreate,
      skipDuplicates: true,
    });

    totalArticles = result.count;

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Database save completed',
        processedArticles: totalArticles,
      },
    });

    console.log(`âœ… Batch save completed: ${totalComplexes} complexes, ${totalArticles} articles`);

  } catch (error: any) {
    console.error('Failed to process crawl data:', error);
    errors.push(`Database save error: ${error.message}`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'failed',
        errorMessage: errors.join(', '),
      },
    });
  }

  return { totalArticles, totalComplexes, errors };
}

export async function POST(request: NextRequest) {
  const startTime = Date.now();
  let crawlId: string | null = null;

  try {
    const body = await request.json();
    const { complexNumbers } = body;

    if (!complexNumbers || complexNumbers.length === 0) {
      return NextResponse.json(
        { error: 'ë‹¨ì§€ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.' },
        { status: 400 }
      );
    }

    const complexNosArray = Array.isArray(complexNumbers)
      ? complexNumbers
      : complexNumbers.split(',').map((n: string) => n.trim());

    const complexNos = complexNosArray.join(',');

    // 1. í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ìƒì„± (ì§„í–‰ ì¤‘ ìƒíƒœ)
    const crawlHistory = await prisma.crawlHistory.create({
      data: {
        complexNos: complexNosArray,
        totalComplexes: complexNosArray.length,
        successCount: 0,
        errorCount: 0,
        totalArticles: 0,
        duration: 0,
        status: 'crawling',
        currentStep: 'Starting crawler',
        processedArticles: 0,
        processedComplexes: 0,
      },
    });

    crawlId = crawlHistory.id;
    currentCrawlId = crawlId;

    // 2. Python í¬ë¡¤ëŸ¬ ì‹¤í–‰ (crawl_id ì „ë‹¬)
    const baseDir = process.env.NODE_ENV === 'production' ? '/app' : process.cwd();
    const command = `python3 ${baseDir}/logic/nas_playwright_crawler.py "${complexNos}" "${crawlId}"`;

    console.log('ğŸš€ Starting crawler...');
    console.log(`   - Crawl ID: ${crawlId}`);
    console.log(`   - Complexes: ${complexNos}`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Crawling ${complexNosArray.length} complexes`,
      },
    });

    const { stdout, stderr } = await execAsync(command, {
      cwd: baseDir,
      maxBuffer: 10 * 1024 * 1024, // 10MB
      timeout: 1800000, // 30ë¶„ íƒ€ì„ì•„ì›ƒ (15ë¶„ â†’ 30ë¶„ìœ¼ë¡œ ì¦ê°€)
    });

    // Python ì¶œë ¥ì„ ë¡œê·¸ì— í‘œì‹œ
    if (stdout) {
      console.log('=== Python Crawler Output ===');
      console.log(stdout);
    }
    if (stderr) {
      console.error('=== Python Crawler Errors ===');
      console.error(stderr);
    }

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Crawling completed',
      },
    });

    // 3. í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (Batch Insert)
    console.log('ğŸ’¾ Saving results to database...');
    const dbResult = await saveCrawlResultsToDB(crawlId, complexNosArray);

    const duration = Date.now() - startTime;
    const status = dbResult.errors.length > 0 ? 'partial' : 'success';

    // 4. ìµœì¢… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        successCount: dbResult.totalComplexes,
        errorCount: complexNosArray.length - dbResult.totalComplexes,
        totalArticles: dbResult.totalArticles,
        duration: Math.floor(duration / 1000), // ì´ˆ ë‹¨ìœ„
        status,
        errorMessage: dbResult.errors.length > 0 ? dbResult.errors.join(', ') : null,
        currentStep: 'Completed',
      },
    });

    currentCrawlId = null;

    console.log('âœ… Crawl completed and saved to DB');
    console.log(`   - Complexes: ${dbResult.totalComplexes}`);
    console.log(`   - Articles: ${dbResult.totalArticles}`);
    console.log(`   - Duration: ${Math.floor(duration / 1000)}s`);

    return NextResponse.json({
      success: true,
      message: 'í¬ë¡¤ë§ì´ ì™„ë£Œë˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
      crawlId,
      data: {
        complexes: dbResult.totalComplexes,
        articles: dbResult.totalArticles,
        duration: Math.floor(duration / 1000),
        errors: dbResult.errors,
      },
    });

  } catch (error: any) {
    console.error('âŒ Crawling error:', error);

    // ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if (crawlId) {
      try {
        const duration = Date.now() - startTime;
        await prisma.crawlHistory.update({
          where: { id: crawlId },
          data: {
            duration: Math.floor(duration / 1000),
            status: 'failed',
            errorMessage: error.message,
            currentStep: 'Failed',
          },
        });
      } catch (historyError) {
        console.error('Failed to update error history:', historyError);
      }
    }

    currentCrawlId = null;

    return NextResponse.json(
      {
        error: 'í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: error.message,
        crawlId,
      },
      { status: 500 }
    );
  }
}

export async function GET() {
  return NextResponse.json({
    message: 'POST ìš”ì²­ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.',
    example: {
      complexNumbers: ['22065', '12345']
    }
  });
}
