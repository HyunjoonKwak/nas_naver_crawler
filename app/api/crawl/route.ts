import { NextRequest, NextResponse } from 'next/server';
import { prisma } from '@/lib/prisma';
import { requireAuth } from '@/lib/auth-utils';
import { rateLimit, rateLimitPresets } from '@/lib/rate-limit';
import { createLogger } from '@/lib/logger';
import { parsePriceToWonBigInt } from '@/lib/price-utils';
import fs from 'fs/promises';
import path from 'path';
import { eventBroadcaster } from '@/lib/eventBroadcaster';
import { sendDiscordNotification, createCrawlSummaryEmbed } from '@/lib/discord';

const logger = createLogger('CRAWL');

export const dynamic = 'force-dynamic';

// Global crawl state for progress tracking
let currentCrawlId: string | null = null;
let isCurrentlyCrawling: boolean = false;

/**
 * NOTE: executeCrawlInBackground, saveCrawlResultsToDB, and sendAlertsForChanges
 * have been extracted to services/:
 * - services/crawler-executor.ts
 * - services/crawl-db-service.ts
 * - services/alert-service.ts
 * - services/crawl-workflow.ts (orchestration)
 */

// Legacy function retained for sendScheduleCrawlCompleteNotification compatibility
async function saveCrawlResultsToDB(crawlId: string, complexNos: string[], userId: string) {
  const baseDir = process.env.NODE_ENV === 'production' ? '/app' : process.cwd();
  const crawledDataDir = path.join(baseDir, 'crawled_data');

  let totalArticles = 0;
  let totalComplexes = 0;
  let errors: string[] = [];

  try {
    // Update status: saving to DB
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'saving',
        currentStep: 'Reading crawl result files',
      },
    });

    // ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ ì°¾ê¸° (complexes_N_ë‚ ì§œ.json í˜•ì‹)
    const files = await fs.readdir(crawledDataDir);
    const jsonFiles = files
      .filter(f => f.startsWith('complexes_') && f.endsWith('.json'))
      .map(f => {
        const fullPath = path.join(crawledDataDir, f);
        const stats = require('fs').statSync(fullPath);
        return { name: f, mtime: stats.mtime };
      })
      .sort((a, b) => b.mtime.getTime() - a.mtime.getTime())  // Sort by modification time (newest first)
      .map(f => f.name);

    if (jsonFiles.length === 0) {
      console.log('No crawl result files found');
      return { totalArticles: 0, totalComplexes: 0, errors: ['No data files found'] };
    }

    // ê°€ì¥ ìµœì‹  íŒŒì¼ ì‚¬ìš© (ìˆ˜ì • ì‹œê°„ ê¸°ì¤€)
    const latestFile = path.join(crawledDataDir, jsonFiles[0]);
    console.log(`Processing file: ${jsonFiles[0]} (latest by mtime)`);

    const rawData = await fs.readFile(latestFile, 'utf-8');
    const crawlData = JSON.parse(rawData);

    // ë°ì´í„°ê°€ ë°°ì—´ì¸ì§€ í™•ì¸
    const dataArray = Array.isArray(crawlData) ? crawlData : [crawlData];
    console.log(`Found ${dataArray.length} complexes in file`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Processing ${dataArray.length} complexes`,
      },
    });

    // ëª¨ë“  ë‹¨ì§€ ì •ë³´ë¥¼ ë¨¼ì € ìˆ˜ì§‘
    const complexesToUpsert: any[] = [];
    const articlesToCreate: any[] = [];
    const complexNoToIdMap = new Map<string, string>();

    for (let i = 0; i < dataArray.length; i++) {
      const data = dataArray[i];

      // Overviewì™€ Articles ì¤‘ ìµœì†Œ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
      if (!data.overview && !data.articles) {
        console.log('Skipping item without overview and articles');
        continue;
      }

      const overview = data.overview;
      const articleList = data.articles?.articleList || [];

      // complexNo ìš°ì„ ìˆœìœ„: overview > crawling_info
      const complexNo = overview?.complexNo || data.crawling_info?.complex_no;

      if (!complexNo) {
        console.log(`âš ï¸  Skipping item ${i}: complexNo not found in overview or crawling_info`);
        errors.push(`Complex ${i}: Missing complexNo`);
        continue;
      }

      // ìœ„ì¹˜ ì •ë³´: overview ë˜ëŠ” ì²« ë²ˆì§¸ ë§¤ë¬¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
      const firstArticle = articleList[0];
      const latitude = overview?.location?.latitude || overview?.latitude ||
                      (firstArticle?.latitude ? parseFloat(firstArticle.latitude) : null);
      const longitude = overview?.location?.longitude || overview?.longitude ||
                       (firstArticle?.longitude ? parseFloat(firstArticle.longitude) : null);

      // ë‹¨ì§€ ì •ë³´ ì¤€ë¹„ (Overview ì—†ì–´ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì €ì¥)
      complexesToUpsert.push({
        complexNo: complexNo,
        complexName: overview?.complexName || `ë‹¨ì§€ ${complexNo}`,
        totalHousehold: overview?.totalHouseHoldCount || overview?.totalHousehold || null,
        totalDong: overview?.totalDongCount || overview?.totalDong || null,
        latitude: latitude,
        longitude: longitude,
        address: overview?.address || null,
        roadAddress: overview?.roadAddress || null,
        jibunAddress: overview?.jibunAddress || null,
        beopjungdong: overview?.beopjungdong || null,
        haengjeongdong: overview?.haengjeongdong || null,
        pyeongs: overview?.pyeongs || [],
        userId: userId, // í¬ë¡¤ë§ ì‹¤í–‰í•œ ì‚¬ìš©ì ID
      });

      // Note: processedComplexes is updated by Python crawler in real-time
      // No need to update here to avoid progress bar confusion
    }

    // âœ… ì—­ì§€ì˜¤ì½”ë”©: ì¢Œí‘œê°€ ìˆì§€ë§Œ ë²•ì •ë™ ì •ë³´ê°€ ì—†ëŠ” ë‹¨ì§€ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ì£¼ì†Œ ì •ë³´ ì¶”ê°€
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Reverse geocoding addresses',
      },
    });

    // ğŸ“Œ ìµœì í™”: DBì—ì„œ ê¸°ì¡´ ë‹¨ì§€ì˜ ë²•ì •ë™ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    const complexNos = complexesToUpsert.map(c => c.complexNo);
    const existingComplexes = await prisma.complex.findMany({
      where: { complexNo: { in: complexNos } },
      select: {
        complexNo: true,
        beopjungdong: true,
        haengjeongdong: true,
        sidoCode: true,
        sigunguCode: true,
        dongCode: true,
        lawdCd: true,
      },
    });

    // Mapìœ¼ë¡œ ë¹ ë¥¸ ì¡°íšŒ
    const existingDataMap = new Map(
      existingComplexes.map(c => [c.complexNo, c])
    );

    // ê¸°ì¡´ ë‹¨ì§€ì˜ ë²•ì •ë™ ì •ë³´ ë³‘í•©
    let skippedGeocoding = 0;
    for (const complex of complexesToUpsert) {
      const existing = existingDataMap.get(complex.complexNo);
      if (existing && existing.beopjungdong) {
        // DBì— ë²•ì •ë™ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        complex.beopjungdong = complex.beopjungdong || existing.beopjungdong;
        complex.haengjeongdong = complex.haengjeongdong || existing.haengjeongdong;
        complex.sidoCode = complex.sidoCode || existing.sidoCode;
        complex.sigunguCode = complex.sigunguCode || existing.sigunguCode;
        complex.dongCode = complex.dongCode || existing.dongCode;
        complex.lawdCd = complex.lawdCd || existing.lawdCd;
        skippedGeocoding++;
        console.log(`ğŸ’¾ ${complex.complexName} (${complex.complexNo}): ê¸°ì¡´ ë²•ì •ë™ ì •ë³´ ì¬ì‚¬ìš© â†’ ì§€ì˜¤ì½”ë”© ìŠ¤í‚µ`);
      }
    }

    if (skippedGeocoding > 0) {
      console.log(`âœ… ì´ ${skippedGeocoding}ê°œ ë‹¨ì§€ì˜ ì§€ì˜¤ì½”ë”© ìŠ¤í‚µ (ê¸°ì¡´ DB ë°ì´í„° ì‚¬ìš©)`);
    }

    for (const complex of complexesToUpsert) {
      if (complex.latitude && complex.longitude && !complex.beopjungdong) {
        try {
          // ë‚´ë¶€ geocode API í˜¸ì¶œ
          const baseUrl = process.env.NEXTAUTH_URL || 'http://localhost:3000';
          const geocodeUrl = `${baseUrl}/api/geocode?latitude=${complex.latitude}&longitude=${complex.longitude}`;

          const geocodeRes = await fetch(geocodeUrl);
          const geocodeData = await geocodeRes.json();

          if (geocodeData.success && geocodeData.data) {
            complex.beopjungdong = geocodeData.data.beopjungdong || null;
            complex.haengjeongdong = geocodeData.data.haengjeongdong || null;
            complex.sidoCode = geocodeData.data.sidoCode || null;
            complex.sigunguCode = geocodeData.data.sigunguCode || null;
            complex.dongCode = geocodeData.data.dongCode || null;

            // ì£¼ì†Œ ì •ë³´ë„ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ addressê°€ ì—†ëŠ” ê²½ìš°)
            if (!complex.address && geocodeData.data.fullAddress) {
              complex.address = geocodeData.data.fullAddress;
            }

            console.log(`âœ… Geocoded: ${complex.complexName} â†’ ${complex.beopjungdong} (ë²•ì •ë™ì½”ë“œ: ${geocodeData.data.lawdCd})`);
          }
        } catch (err: any) {
          console.error(`[Geocoding Error] Failed for complex ${complex.complexNo} (${complex.complexName}):`, err.message);
          // ì—­ì§€ì˜¤ì½”ë”© ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
        }

        // Rate limiting ë°©ì§€ (SGIS API í˜¸ì¶œ ì œí•œ)
        await new Promise(resolve => setTimeout(resolve, 300));
      }
    }

    // Batch upsert complexes
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Saving complex information',
      },
    });

    for (const complexData of complexesToUpsert) {
      // update ì‹œì—ëŠ” userId ì œì™¸ (ê¸°ì¡´ ì‚¬ìš©ì ìœ ì§€)
      const { userId, ...updateData } = complexData;

      const complex = await prisma.complex.upsert({
        where: { complexNo: complexData.complexNo },
        update: updateData, // userId ì œì™¸í•œ ë‚˜ë¨¸ì§€ë§Œ ì—…ë°ì´íŠ¸
        create: complexData, // ì‹ ê·œ ìƒì„± ì‹œì—ëŠ” userId í¬í•¨
      });
      complexNoToIdMap.set(complexData.complexNo, complex.id);
      totalComplexes++;
    }

    // ëª¨ë“  ë§¤ë¬¼ ë°ì´í„° ì¤€ë¹„ (complexId ë§¤í•‘)
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Preparing article data',
      },
    });

    for (const data of dataArray) {
      if (!data.overview || !data.articles) continue;

      const overview = data.overview;
      const articleList = data.articles.articleList || [];

      // complexNo ê°€ì ¸ì˜¤ê¸° (ë™ì¼í•œ fallback ë¡œì§)
      const complexNo = overview.complexNo || data.crawling_info?.complex_no;
      if (!complexNo) continue;

      const complexId = complexNoToIdMap.get(complexNo);

      if (!complexId) {
        logger.error(`Complex ID not found for ${complexNo}`);
        continue;
      }

      for (const article of articleList) {
        articlesToCreate.push({
          articleNo: article.articleNo,
          complexId: complexId,
          realEstateTypeName: article.realEstateTypeName || 'ì•„íŒŒíŠ¸',
          tradeTypeName: article.tradeTypeName,
          dealOrWarrantPrc: article.dealOrWarrantPrc,
          rentPrc: article.rentPrc,
          // âœ… ì¶”ê°€: ìˆ«ì ê°€ê²© ì»¬ëŸ¼ (ì„±ëŠ¥ ìµœì í™”ìš©)
          dealOrWarrantPrcWon: parsePriceToWonBigInt(article.dealOrWarrantPrc),
          rentPrcWon: article.rentPrc ? parsePriceToWonBigInt(article.rentPrc) : null,
          area1: parseFloat(article.area1) || 0,
          area2: article.area2 ? parseFloat(article.area2) : null,
          floorInfo: article.floorInfo,
          direction: article.direction,
          articleConfirmYmd: article.articleConfirmYmd,
          buildingName: article.buildingName,
          sameAddrCnt: article.sameAddrCnt ? parseInt(article.sameAddrCnt) : null,
          realtorName: article.realtorName,
          articleFeatureDesc: article.articleFeatureDesc,
          tagList: article.tagList || [],
        });
      }
    }

    // Batch insert articles (ë‹¨ì§€ë³„ë¡œ ê¸°ì¡´ ë§¤ë¬¼ ì „ì²´ ì‚­ì œ í›„ ì¬ìƒì„±)
    // 1. í¬ë¡¤ë§ëœ ë‹¨ì§€ë“¤ì˜ complexId ëª©ë¡ ì¶”ì¶œ
    const crawledComplexIds = Array.from(new Set(articlesToCreate.map(a => a.complexId)));

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Deleting old articles from ${crawledComplexIds.length} complexes`,
      },
    });

    // 2. í•´ë‹¹ ë‹¨ì§€ë“¤ì˜ ëª¨ë“  ê¸°ì¡´ ë§¤ë¬¼ ì‚­ì œ (ì‚­ì œëœ ë§¤ë¬¼ ë°˜ì˜)
    await prisma.article.deleteMany({
      where: {
        complexId: {
          in: crawledComplexIds,
        },
      },
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Saving articles (${articlesToCreate.length} items)`,
      },
    });

    // 3. Batch insert (skipDuplicatesë¡œ ì•ˆì „í•˜ê²Œ)
    const articlesBeforeInsert = articlesToCreate.length;
    const result = await prisma.article.createMany({
      data: articlesToCreate,
      skipDuplicates: true,
    });

    totalArticles = result.count;

    // ì¤‘ë³µ ì œê±°ëœ ë§¤ë¬¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
    if (articlesBeforeInsert > totalArticles) {
      console.log(`âš ï¸  Duplicates skipped: ${articlesBeforeInsert - totalArticles} articles (${articlesBeforeInsert} â†’ ${totalArticles})`);
    }

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Database save completed',
        processedArticles: totalArticles,
      },
    });

    console.log(`âœ… Batch save completed: ${totalComplexes} complexes, ${totalArticles} articles`);

  } catch (error: any) {
    logger.error('Failed to process crawl data', error);
    errors.push(`Database save error: ${error.message}`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'failed',
        errorMessage: errors.join(', '),
      },
    });
  }

  return { totalArticles, totalComplexes, errors };
}

export async function POST(request: NextRequest) {
  const startTime = Date.now();
  let crawlId: string | null = null;

  try {
    // ğŸ”’ ì¤‘ë³µ í¬ë¡¤ë§ ë°©ì§€: ì´ë¯¸ í¬ë¡¤ë§ ì§„í–‰ ì¤‘ì¸ ê²½ìš° ê±°ë¶€
    if (isCurrentlyCrawling) {
      logger.warn('Crawl request rejected: Another crawl is already in progress', {
        currentCrawlId,
      });
      return NextResponse.json(
        {
          error: 'ì´ë¯¸ í¬ë¡¤ë§ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì™„ë£Œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
          currentCrawlId,
        },
        { status: 409 } // 409 Conflict
      );
    }

    // ë‚´ë¶€ ìŠ¤ì¼€ì¤„ëŸ¬ í˜¸ì¶œ í™•ì¸ (íŠ¹ë³„í•œ í—¤ë”ë¡œ ì‹ë³„)
    const internalSecret = request.headers.get('x-internal-secret');
    const isInternalCall = internalSecret === process.env.INTERNAL_API_SECRET;

    // Rate Limiting (ë‚´ë¶€ í˜¸ì¶œì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
    if (!isInternalCall) {
      const rateLimitResponse = rateLimit(request, rateLimitPresets.crawl);
      if (rateLimitResponse) return rateLimitResponse;
    }

    const body = await request.json();
    const {
      complexNumbers,
      userId: requestUserId,
      initiator = 'manual',  // manual, schedule, api, complex-detail
      scheduleId,
      scheduleName,
    } = body;

    // ì‚¬ìš©ì ì¸ì¦ í™•ì¸ (ë‚´ë¶€ í˜¸ì¶œì´ ì•„ë‹Œ ê²½ìš°)
    let currentUser;
    if (isInternalCall) {
      // ë‚´ë¶€ í˜¸ì¶œ: bodyì—ì„œ ì „ë‹¬ëœ userId ì‚¬ìš©
      if (!requestUserId) {
        return NextResponse.json(
          { error: 'Internal call requires userId' },
          { status: 400 }
        );
      }
      const user = await prisma.user.findUnique({ where: { id: requestUserId } });
      if (!user) {
        return NextResponse.json(
          { error: 'User not found' },
          { status: 404 }
        );
      }
      currentUser = { id: user.id, email: user.email, name: user.name, role: user.role };
    } else {
      // ì™¸ë¶€ í˜¸ì¶œ: ì„¸ì…˜ ì¸ì¦ í•„ìš”
      currentUser = await requireAuth();
    }

    if (!complexNumbers || complexNumbers.length === 0) {
      return NextResponse.json(
        { error: 'ë‹¨ì§€ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.' },
        { status: 400 }
      );
    }

    const complexNosArray = Array.isArray(complexNumbers)
      ? complexNumbers
      : complexNumbers.split(',').map((n: string) => n.trim());

    const complexNos = complexNosArray.join(',');

    // Generate unique crawl ID
    crawlId = `crawl_${Date.now()}_${Math.random().toString(36).substring(7)}`;
    currentCrawlId = crawlId;
    isCurrentlyCrawling = true; // ğŸ”’ í¬ë¡¤ë§ ì‹œì‘ í”Œë˜ê·¸ ì„¤ì •

    // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì‹œì‘
    eventBroadcaster.notifyCrawlStart(crawlId, complexNosArray.length);

    logger.info('Starting crawler workflow', { crawlId, complexNos: complexNosArray.length });

    // ìŠ¤ì¼€ì¤„ ì‹¤í–‰ì¸ ê²½ìš°: crawlIdë§Œ ë°˜í™˜í•˜ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    if (initiator === 'schedule') {
      console.log(`ğŸ“¤ Returning crawlId immediately for schedule execution: ${crawlId}`);

      // Import workflow service
      const { executeCrawlWorkflow } = await import('@/services/crawl-workflow');

      // ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ (await ì—†ì´)
      executeCrawlWorkflow({
        crawlId,
        complexNos: complexNosArray,
        userId: currentUser.id,
        scheduleId: scheduleId || null,
      })
        .then(() => {
          currentCrawlId = null;
          isCurrentlyCrawling = false;
        })
        .catch((error) => {
          logger.error('Background crawl failed', { crawlId, error: error.message });
          currentCrawlId = null;
          isCurrentlyCrawling = false;
        });

      // ì¦‰ì‹œ crawlId ë°˜í™˜
      return NextResponse.json({
        success: true,
        crawlId,
        message: 'Crawl started in background',
      });
    }

    // Import workflow service
    const { executeCrawlWorkflow } = await import('@/services/crawl-workflow');

    // Execute complete crawl workflow
    const result = await executeCrawlWorkflow({
      crawlId,
      complexNos: complexNosArray,
      userId: currentUser.id,
      scheduleId: scheduleId || null,
    });

    currentCrawlId = null;
    isCurrentlyCrawling = false; // ğŸ”“ í¬ë¡¤ë§ ì™„ë£Œ í”Œë˜ê·¸ í•´ì œ

    return NextResponse.json({
      success: result.success,
      message: result.success
        ? 'í¬ë¡¤ë§ì´ ì™„ë£Œë˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.'
        : 'í¬ë¡¤ë§ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      crawlId: result.crawlId,
      data: {
        complexes: result.totalComplexes,
        articles: result.totalArticles,
        duration: Math.floor(result.duration / 1000),
        errors: result.errors,
        status: result.status,
      },
    });

  } catch (error: any) {
    logger.error('Crawling error', error);

    // ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if (crawlId) {
      try {
        const duration = Date.now() - startTime;
        await prisma.crawlHistory.update({
          where: { id: crawlId },
          data: {
            duration: Math.floor(duration / 1000),
            status: 'failed',
            errorMessage: error.message,
            currentStep: 'Failed',
          },
        });

        // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì‹¤íŒ¨
        eventBroadcaster.notifyCrawlFailed(crawlId, error.message);
      } catch (historyError: any) {
        logger.error('Failed to update error history', historyError);
      }
    }

    currentCrawlId = null;
    isCurrentlyCrawling = false; // ğŸ”“ ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ í”Œë˜ê·¸ í•´ì œ

    return NextResponse.json(
      {
        error: 'í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: error.message,
        crawlId,
      },
      { status: 500 }
    );
  }
}

// ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡
async function sendScheduleCrawlCompleteNotification(
  scheduleId: string,
  dbResult: { totalComplexes: number; totalArticles: number; errors: string[] },
  duration: number
) {
  try {
    // ìŠ¤ì¼€ì¤„ ì •ë³´ ì¡°íšŒ
    const schedule = await prisma.schedule.findUnique({
      where: { id: scheduleId },
      include: {
        user: true,
      },
    });

    if (!schedule) {
      logger.warn('Schedule not found for notification', { scheduleId });
      return;
    }

    // ì‚¬ìš©ìì˜ ì•Œë¦¼ ì„¤ì • í™•ì¸ (í™œì„±í™”ëœ ì•Œë¦¼ì´ ìˆê³  webhookUrlì´ ìˆëŠ” ê²½ìš°)
    const userAlerts = await prisma.alert.findMany({
      where: {
        userId: schedule.userId,
        isActive: true,
        webhookUrl: { not: null },
      },
      take: 1, // í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ë¨ (webhookUrl ê°€ì ¸ì˜¤ê¸° ìœ„í•´)
    });

    if (userAlerts.length === 0) {
      logger.info('No active alerts with webhook URL for schedule notification', { scheduleId });
      return;
    }

    const webhookUrl = userAlerts[0].webhookUrl!;

    // Discord ì„ë² ë“œ ìƒì„±
    const embed = {
      title: 'â° ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì™„ë£Œ',
      description: `**${schedule.name}** ìŠ¤ì¼€ì¤„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`,
      color: dbResult.errors.length > 0 ? 0xfbbf24 : 0x10b981, // ì—ëŸ¬ ìˆìœ¼ë©´ ë…¸ë€ìƒ‰, ì—†ìœ¼ë©´ ì´ˆë¡ìƒ‰
      fields: [
        {
          name: 'ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼',
          value: `â€¢ ë‹¨ì§€ ìˆ˜: ${dbResult.totalComplexes}ê°œ\nâ€¢ ë§¤ë¬¼ ìˆ˜: ${dbResult.totalArticles}ê°œ\nâ€¢ ì†Œìš” ì‹œê°„: ${Math.floor(duration / 1000)}ì´ˆ`,
          inline: false,
        },
      ],
      timestamp: new Date().toISOString(),
      footer: {
        text: `Schedule ID: ${scheduleId.substring(0, 8)}`,
      },
    };

    // ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if (dbResult.errors.length > 0) {
      embed.fields.push({
        name: 'âš ï¸ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ',
        value: dbResult.errors.slice(0, 3).join('\n') + (dbResult.errors.length > 3 ? `\n... ì™¸ ${dbResult.errors.length - 3}ê°œ` : ''),
        inline: false,
      });
    }

    // Discord ì›¹í›… ì „ì†¡
    const response = await fetch(webhookUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        embeds: [embed],
      }),
    });

    if (!response.ok) {
      logger.error('Failed to send schedule completion notification to Discord', {
        status: response.status,
        statusText: response.statusText,
      });
    } else {
      logger.info('Schedule completion notification sent successfully', { scheduleId });
    }
  } catch (error: any) {
    logger.error('Error sending schedule completion notification', error);
  }
}

export async function GET() {
  return NextResponse.json({
    message: 'POST ìš”ì²­ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.',
    example: {
      complexNumbers: ['22065', '12345']
    }
  });
}
