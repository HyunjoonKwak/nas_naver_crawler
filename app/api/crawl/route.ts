import { NextRequest, NextResponse } from 'next/server';
import { spawn } from 'child_process';
import { prisma } from '@/lib/prisma';
import { requireAuth } from '@/lib/auth-utils';
import { rateLimit, rateLimitPresets } from '@/lib/rate-limit';
import { createLogger } from '@/lib/logger';
import { deleteCache } from '@/lib/redis-cache';
import { parsePriceToWonBigInt } from '@/lib/price-utils';
import fs from 'fs/promises';
import path from 'path';
import {
  detectArticleChanges,
  filterChangesForAlerts,
  getComplexInfo,
  saveNotificationLog,
} from '@/lib/article-tracker';
import {
  sendDiscordNotification,
  createNewArticleEmbed,
  createDeletedArticleEmbed,
  createPriceChangedEmbed,
  createCrawlSummaryEmbed,
} from '@/lib/discord';
import { eventBroadcaster } from '@/lib/eventBroadcaster';
import { calculateDynamicTimeout } from '@/lib/timeoutCalculator';

const logger = createLogger('CRAWL');

export const dynamic = 'force-dynamic';

// Global crawl state for progress tracking
let currentCrawlId: string | null = null;
let isCurrentlyCrawling: boolean = false;

// ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ (ìŠ¤ì¼€ì¤„ ì‹¤í–‰ìš©)
async function executeCrawlInBackground(
  crawlId: string,
  complexNosArray: string[],
  complexNos: string,
  baseDir: string,
  dynamicTimeout: number,
  userId: string,
  scheduleId?: string | null
) {
  const startTime = Date.now();

  try {
    // Python í¬ë¡¤ëŸ¬ ì‹¤í–‰
    await new Promise<void>((resolve, reject) => {
      const pythonProcess = spawn('python3', [
        '-u',  // unbuffered output
        `${baseDir}/logic/nas_playwright_crawler.py`,
        complexNos,
        crawlId
      ], {
        cwd: baseDir,
        env: process.env,
      });

      let hasOutput = false;

      // stdout ì‹¤ì‹œê°„ ì¶œë ¥
      pythonProcess.stdout.on('data', (data) => {
        const output = data.toString();
        if (!hasOutput) {
          console.log('=== Python Crawler Output ===');
          hasOutput = true;
        }
        process.stdout.write(output);
      });

      // stderr ì‹¤ì‹œê°„ ì¶œë ¥
      pythonProcess.stderr.on('data', (data) => {
        process.stderr.write(data.toString());
      });

      // í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì²˜ë¦¬
      pythonProcess.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Python crawler exited with code ${code}`));
        }
      });

      // í”„ë¡œì„¸ìŠ¤ ì—ëŸ¬ ì²˜ë¦¬
      pythonProcess.on('error', (error) => {
        reject(error);
      });

      // íƒ€ì„ì•„ì›ƒ ì„¤ì •
      const timeoutId = setTimeout(() => {
        pythonProcess.kill('SIGTERM');
        reject(new Error('Crawler timeout'));
      }, dynamicTimeout);

      // í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ íƒ€ì„ì•„ì›ƒ í´ë¦¬ì–´
      pythonProcess.on('close', () => {
        clearTimeout(timeoutId);
      });
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Crawling completed',
      },
    });

    // í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥
    logger.info('Saving results to database');
    const dbResult = await saveCrawlResultsToDB(crawlId, complexNosArray, userId);

    const duration = Date.now() - startTime;
    const status = dbResult.errors.length > 0 ? 'partial' : 'success';

    // ìµœì¢… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        successCount: dbResult.totalComplexes,
        errorCount: complexNosArray.length - dbResult.totalComplexes,
        totalArticles: dbResult.totalArticles,
        duration: Math.floor(duration / 1000),
        status,
        errorMessage: dbResult.errors.length > 0 ? dbResult.errors.join(', ') : null,
        currentStep: 'Completed',
      },
    });

    // ìŠ¤ì¼€ì¤„ ì •ë³´ ì—…ë°ì´íŠ¸ ë° ë¡œê·¸ ì €ì¥
    if (scheduleId) {
      await prisma.schedule.update({
        where: { id: scheduleId },
        data: {
          lastRun: new Date(),
        },
      }).catch((error) => {
        logger.error('Failed to update schedule info', { scheduleId, error: error.message });
      });

      // ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ë¡œê·¸ ì €ì¥
      await prisma.scheduleLog.create({
        data: {
          scheduleId,
          status: status === 'success' ? 'success' : 'failed',
          duration: Math.floor(duration / 1000), // ì´ˆ ë‹¨ìœ„
          articlesCount: dbResult.totalArticles,
          errorMessage: dbResult.errors.length > 0 ? dbResult.errors.slice(0, 3).join(', ') : null,
        },
      }).catch((error) => {
        logger.error('Failed to save schedule log', { scheduleId, error: error.message });
      });
    }

    // í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼
    eventBroadcaster.notifyCrawlComplete(crawlId, dbResult.totalArticles);

    // âœ… ìºì‹œ ë¬´íš¨í™” (í¬ë¡¤ë§ëœ ë‹¨ì§€ ê´€ë ¨ ìºì‹œ ì‚­ì œ)
    await deleteCache('complex:*');
    await deleteCache('analytics:*');
    await deleteCache('article:*');
    console.log('[Cache] Invalidated all complex-related caches');

    logger.info('Background crawl completed', {
      crawlId,
      complexes: dbResult.totalComplexes,
      articles: dbResult.totalArticles,
      status
    });

    // ì•Œë¦¼ ë°œì†¡
    await sendAlertsForChanges(complexNosArray).catch((error) => {
      logger.error('Failed to send alerts', error);
    });

    // ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ (ìŠ¤ì¼€ì¤„ì—ì„œ ì‹¤í–‰ëœ ê²½ìš°ì—ë§Œ)
    if (scheduleId) {
      await sendScheduleCrawlCompleteNotification(scheduleId, dbResult, duration).catch((error) => {
        logger.error('Failed to send schedule completion notification', error);
      });
    }

    // ğŸ”“ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì™„ë£Œ - í”Œë˜ê·¸ í•´ì œ
    isCurrentlyCrawling = false;
    logger.info('Background crawl lock released', { crawlId });

  } catch (error: any) {
    logger.error('Background crawl error', { crawlId, error: error.message });

    const duration = Date.now() - startTime;

    // ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        duration: Math.floor(duration / 1000),
        status: 'failed',
        errorMessage: error.message,
        currentStep: 'Failed',
      },
    }).catch((historyError) => {
      logger.error('Failed to update error history', historyError);
    });

    // ìŠ¤ì¼€ì¤„ ì •ë³´ ì—…ë°ì´íŠ¸ ë° ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
    if (scheduleId) {
      await prisma.schedule.update({
        where: { id: scheduleId },
        data: {
          lastRun: new Date(),
        },
      }).catch((error) => {
        logger.error('Failed to update schedule info on error', { scheduleId, error: error.message });
      });

      // ìŠ¤ì¼€ì¤„ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
      await prisma.scheduleLog.create({
        data: {
          scheduleId,
          status: 'failed',
          duration: Math.floor(duration / 1000), // ì´ˆ ë‹¨ìœ„
          articlesCount: 0,
          errorMessage: error.message,
        },
      }).catch((logError) => {
        logger.error('Failed to save schedule error log', { scheduleId, error: logError.message });
      });
    }

    // ğŸ”“ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨ - í”Œë˜ê·¸ í•´ì œ
    isCurrentlyCrawling = false;
    logger.warn('Background crawl lock released due to error', { crawlId });

    // í¬ë¡¤ë§ ì‹¤íŒ¨ ì•Œë¦¼
    eventBroadcaster.notifyCrawlFailed(crawlId, error.message);
  } finally {
    currentCrawlId = null;
  }
}

// í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (Batch Insert ë°©ì‹)
async function saveCrawlResultsToDB(crawlId: string, complexNos: string[], userId: string) {
  const baseDir = process.env.NODE_ENV === 'production' ? '/app' : process.cwd();
  const crawledDataDir = path.join(baseDir, 'crawled_data');

  let totalArticles = 0;
  let totalComplexes = 0;
  let errors: string[] = [];

  try {
    // Update status: saving to DB
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'saving',
        currentStep: 'Reading crawl result files',
      },
    });

    // ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ ì°¾ê¸° (complexes_N_ë‚ ì§œ.json í˜•ì‹)
    const files = await fs.readdir(crawledDataDir);
    const jsonFiles = files
      .filter(f => f.startsWith('complexes_') && f.endsWith('.json'))
      .map(f => {
        const fullPath = path.join(crawledDataDir, f);
        const stats = require('fs').statSync(fullPath);
        return { name: f, mtime: stats.mtime };
      })
      .sort((a, b) => b.mtime.getTime() - a.mtime.getTime())  // Sort by modification time (newest first)
      .map(f => f.name);

    if (jsonFiles.length === 0) {
      console.log('No crawl result files found');
      return { totalArticles: 0, totalComplexes: 0, errors: ['No data files found'] };
    }

    // ê°€ì¥ ìµœì‹  íŒŒì¼ ì‚¬ìš© (ìˆ˜ì • ì‹œê°„ ê¸°ì¤€)
    const latestFile = path.join(crawledDataDir, jsonFiles[0]);
    console.log(`Processing file: ${jsonFiles[0]} (latest by mtime)`);

    const rawData = await fs.readFile(latestFile, 'utf-8');
    const crawlData = JSON.parse(rawData);

    // ë°ì´í„°ê°€ ë°°ì—´ì¸ì§€ í™•ì¸
    const dataArray = Array.isArray(crawlData) ? crawlData : [crawlData];
    console.log(`Found ${dataArray.length} complexes in file`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Processing ${dataArray.length} complexes`,
      },
    });

    // ëª¨ë“  ë‹¨ì§€ ì •ë³´ë¥¼ ë¨¼ì € ìˆ˜ì§‘
    const complexesToUpsert: any[] = [];
    const articlesToCreate: any[] = [];
    const complexNoToIdMap = new Map<string, string>();

    for (let i = 0; i < dataArray.length; i++) {
      const data = dataArray[i];

      // Overviewì™€ Articles ì¤‘ ìµœì†Œ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
      if (!data.overview && !data.articles) {
        console.log('Skipping item without overview and articles');
        continue;
      }

      const overview = data.overview;
      const articleList = data.articles?.articleList || [];

      // complexNo ìš°ì„ ìˆœìœ„: overview > crawling_info
      const complexNo = overview?.complexNo || data.crawling_info?.complex_no;

      if (!complexNo) {
        console.log(`âš ï¸  Skipping item ${i}: complexNo not found in overview or crawling_info`);
        errors.push(`Complex ${i}: Missing complexNo`);
        continue;
      }

      // ìœ„ì¹˜ ì •ë³´: overview ë˜ëŠ” ì²« ë²ˆì§¸ ë§¤ë¬¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
      const firstArticle = articleList[0];
      const latitude = overview?.location?.latitude || overview?.latitude ||
                      (firstArticle?.latitude ? parseFloat(firstArticle.latitude) : null);
      const longitude = overview?.location?.longitude || overview?.longitude ||
                       (firstArticle?.longitude ? parseFloat(firstArticle.longitude) : null);

      // ë‹¨ì§€ ì •ë³´ ì¤€ë¹„ (Overview ì—†ì–´ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì €ì¥)
      complexesToUpsert.push({
        complexNo: complexNo,
        complexName: overview?.complexName || `ë‹¨ì§€ ${complexNo}`,
        totalHousehold: overview?.totalHouseHoldCount || overview?.totalHousehold || null,
        totalDong: overview?.totalDongCount || overview?.totalDong || null,
        latitude: latitude,
        longitude: longitude,
        address: overview?.address || null,
        roadAddress: overview?.roadAddress || null,
        jibunAddress: overview?.jibunAddress || null,
        beopjungdong: overview?.beopjungdong || null,
        haengjeongdong: overview?.haengjeongdong || null,
        pyeongs: overview?.pyeongs || [],
        userId: userId, // í¬ë¡¤ë§ ì‹¤í–‰í•œ ì‚¬ìš©ì ID
      });

      // Note: processedComplexes is updated by Python crawler in real-time
      // No need to update here to avoid progress bar confusion
    }

    // âœ… ì—­ì§€ì˜¤ì½”ë”©: ì¢Œí‘œê°€ ìˆì§€ë§Œ ë²•ì •ë™ ì •ë³´ê°€ ì—†ëŠ” ë‹¨ì§€ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ì£¼ì†Œ ì •ë³´ ì¶”ê°€
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Reverse geocoding addresses',
      },
    });

    // ğŸ“Œ ìµœì í™”: DBì—ì„œ ê¸°ì¡´ ë‹¨ì§€ì˜ ë²•ì •ë™ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    const complexNos = complexesToUpsert.map(c => c.complexNo);
    const existingComplexes = await prisma.complex.findMany({
      where: { complexNo: { in: complexNos } },
      select: {
        complexNo: true,
        beopjungdong: true,
        haengjeongdong: true,
        sidoCode: true,
        sigunguCode: true,
        dongCode: true,
        lawdCd: true,
      },
    });

    // Mapìœ¼ë¡œ ë¹ ë¥¸ ì¡°íšŒ
    const existingDataMap = new Map(
      existingComplexes.map(c => [c.complexNo, c])
    );

    // ê¸°ì¡´ ë‹¨ì§€ì˜ ë²•ì •ë™ ì •ë³´ ë³‘í•©
    let skippedGeocoding = 0;
    for (const complex of complexesToUpsert) {
      const existing = existingDataMap.get(complex.complexNo);
      if (existing && existing.beopjungdong) {
        // DBì— ë²•ì •ë™ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        complex.beopjungdong = complex.beopjungdong || existing.beopjungdong;
        complex.haengjeongdong = complex.haengjeongdong || existing.haengjeongdong;
        complex.sidoCode = complex.sidoCode || existing.sidoCode;
        complex.sigunguCode = complex.sigunguCode || existing.sigunguCode;
        complex.dongCode = complex.dongCode || existing.dongCode;
        complex.lawdCd = complex.lawdCd || existing.lawdCd;
        skippedGeocoding++;
        console.log(`ğŸ’¾ ${complex.complexName} (${complex.complexNo}): ê¸°ì¡´ ë²•ì •ë™ ì •ë³´ ì¬ì‚¬ìš© â†’ ì§€ì˜¤ì½”ë”© ìŠ¤í‚µ`);
      }
    }

    if (skippedGeocoding > 0) {
      console.log(`âœ… ì´ ${skippedGeocoding}ê°œ ë‹¨ì§€ì˜ ì§€ì˜¤ì½”ë”© ìŠ¤í‚µ (ê¸°ì¡´ DB ë°ì´í„° ì‚¬ìš©)`);
    }

    for (const complex of complexesToUpsert) {
      if (complex.latitude && complex.longitude && !complex.beopjungdong) {
        try {
          // ë‚´ë¶€ geocode API í˜¸ì¶œ
          const baseUrl = process.env.NEXTAUTH_URL || 'http://localhost:3000';
          const geocodeUrl = `${baseUrl}/api/geocode?latitude=${complex.latitude}&longitude=${complex.longitude}`;

          const geocodeRes = await fetch(geocodeUrl);
          const geocodeData = await geocodeRes.json();

          if (geocodeData.success && geocodeData.data) {
            complex.beopjungdong = geocodeData.data.beopjungdong || null;
            complex.haengjeongdong = geocodeData.data.haengjeongdong || null;
            complex.sidoCode = geocodeData.data.sidoCode || null;
            complex.sigunguCode = geocodeData.data.sigunguCode || null;
            complex.dongCode = geocodeData.data.dongCode || null;

            // ì£¼ì†Œ ì •ë³´ë„ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ addressê°€ ì—†ëŠ” ê²½ìš°)
            if (!complex.address && geocodeData.data.fullAddress) {
              complex.address = geocodeData.data.fullAddress;
            }

            console.log(`âœ… Geocoded: ${complex.complexName} â†’ ${complex.beopjungdong} (ë²•ì •ë™ì½”ë“œ: ${geocodeData.data.lawdCd})`);
          }
        } catch (err: any) {
          console.error(`[Geocoding Error] Failed for complex ${complex.complexNo} (${complex.complexName}):`, err.message);
          // ì—­ì§€ì˜¤ì½”ë”© ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
        }

        // Rate limiting ë°©ì§€ (SGIS API í˜¸ì¶œ ì œí•œ)
        await new Promise(resolve => setTimeout(resolve, 300));
      }
    }

    // Batch upsert complexes
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Saving complex information',
      },
    });

    for (const complexData of complexesToUpsert) {
      // update ì‹œì—ëŠ” userId ì œì™¸ (ê¸°ì¡´ ì‚¬ìš©ì ìœ ì§€)
      const { userId, ...updateData } = complexData;

      const complex = await prisma.complex.upsert({
        where: { complexNo: complexData.complexNo },
        update: updateData, // userId ì œì™¸í•œ ë‚˜ë¨¸ì§€ë§Œ ì—…ë°ì´íŠ¸
        create: complexData, // ì‹ ê·œ ìƒì„± ì‹œì—ëŠ” userId í¬í•¨
      });
      complexNoToIdMap.set(complexData.complexNo, complex.id);
      totalComplexes++;
    }

    // ëª¨ë“  ë§¤ë¬¼ ë°ì´í„° ì¤€ë¹„ (complexId ë§¤í•‘)
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Preparing article data',
      },
    });

    for (const data of dataArray) {
      if (!data.overview || !data.articles) continue;

      const overview = data.overview;
      const articleList = data.articles.articleList || [];

      // complexNo ê°€ì ¸ì˜¤ê¸° (ë™ì¼í•œ fallback ë¡œì§)
      const complexNo = overview.complexNo || data.crawling_info?.complex_no;
      if (!complexNo) continue;

      const complexId = complexNoToIdMap.get(complexNo);

      if (!complexId) {
        logger.error(`Complex ID not found for ${complexNo}`);
        continue;
      }

      for (const article of articleList) {
        articlesToCreate.push({
          articleNo: article.articleNo,
          complexId: complexId,
          realEstateTypeName: article.realEstateTypeName || 'ì•„íŒŒíŠ¸',
          tradeTypeName: article.tradeTypeName,
          dealOrWarrantPrc: article.dealOrWarrantPrc,
          rentPrc: article.rentPrc,
          // âœ… ì¶”ê°€: ìˆ«ì ê°€ê²© ì»¬ëŸ¼ (ì„±ëŠ¥ ìµœì í™”ìš©)
          dealOrWarrantPrcWon: parsePriceToWonBigInt(article.dealOrWarrantPrc),
          rentPrcWon: article.rentPrc ? parsePriceToWonBigInt(article.rentPrc) : null,
          area1: parseFloat(article.area1) || 0,
          area2: article.area2 ? parseFloat(article.area2) : null,
          floorInfo: article.floorInfo,
          direction: article.direction,
          articleConfirmYmd: article.articleConfirmYmd,
          buildingName: article.buildingName,
          sameAddrCnt: article.sameAddrCnt ? parseInt(article.sameAddrCnt) : null,
          realtorName: article.realtorName,
          articleFeatureDesc: article.articleFeatureDesc,
          tagList: article.tagList || [],
        });
      }
    }

    // Batch insert articles (ë‹¨ì§€ë³„ë¡œ ê¸°ì¡´ ë§¤ë¬¼ ì „ì²´ ì‚­ì œ í›„ ì¬ìƒì„±)
    // 1. í¬ë¡¤ë§ëœ ë‹¨ì§€ë“¤ì˜ complexId ëª©ë¡ ì¶”ì¶œ
    const crawledComplexIds = Array.from(new Set(articlesToCreate.map(a => a.complexId)));

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Deleting old articles from ${crawledComplexIds.length} complexes`,
      },
    });

    // 2. í•´ë‹¹ ë‹¨ì§€ë“¤ì˜ ëª¨ë“  ê¸°ì¡´ ë§¤ë¬¼ ì‚­ì œ (ì‚­ì œëœ ë§¤ë¬¼ ë°˜ì˜)
    await prisma.article.deleteMany({
      where: {
        complexId: {
          in: crawledComplexIds,
        },
      },
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Saving articles (${articlesToCreate.length} items)`,
      },
    });

    // 3. Batch insert (skipDuplicatesë¡œ ì•ˆì „í•˜ê²Œ)
    const articlesBeforeInsert = articlesToCreate.length;
    const result = await prisma.article.createMany({
      data: articlesToCreate,
      skipDuplicates: true,
    });

    totalArticles = result.count;

    // ì¤‘ë³µ ì œê±°ëœ ë§¤ë¬¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
    if (articlesBeforeInsert > totalArticles) {
      console.log(`âš ï¸  Duplicates skipped: ${articlesBeforeInsert - totalArticles} articles (${articlesBeforeInsert} â†’ ${totalArticles})`);
    }

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Database save completed',
        processedArticles: totalArticles,
      },
    });

    console.log(`âœ… Batch save completed: ${totalComplexes} complexes, ${totalArticles} articles`);

  } catch (error: any) {
    logger.error('Failed to process crawl data', error);
    errors.push(`Database save error: ${error.message}`);

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        status: 'failed',
        errorMessage: errors.join(', '),
      },
    });
  }

  return { totalArticles, totalComplexes, errors };
}

// ì•Œë¦¼ ë°œì†¡ í•¨ìˆ˜ (ë¹„ë™ê¸° ìµœì í™”)
async function sendAlertsForChanges(complexNos: string[]) {
  console.log('ğŸ”” Checking for alerts...');

  // ğŸš€ ì„±ëŠ¥ ìµœì í™”: ë°°ì¹˜ë¡œ ë‹¨ì§€ ì •ë³´ ì¡°íšŒ
  const complexInfos = await prisma.complex.findMany({
    where: { complexNo: { in: complexNos } },
    include: {
      articles: true, // ë§¤ë¬¼ë„ í•¨ê»˜ ì¡°íšŒ (N+1 ì¿¼ë¦¬ ë°©ì§€)
    },
  });

  const complexMap = new Map(complexInfos.map(c => [c.complexNo, c]));

  for (const complexNo of complexNos) {
    try {
      // 1. í˜„ì¬ ë§¤ë¬¼ ë°ì´í„° ì¡°íšŒ (ì´ë¯¸ ë¡œë“œë¨)
      const complexInfo = complexMap.get(complexNo);
      if (!complexInfo) {
        console.log(`Complex not found: ${complexNo}`);
        continue;
      }

      const currentArticles = complexInfo.articles;

      // 2. ë³€ê²½ì‚¬í•­ ê°ì§€
      const changes = await detectArticleChanges(complexNo, currentArticles);

      console.log(`ğŸ“Š Changes for ${complexInfo.complexName}:`, {
        new: changes.newArticles.length,
        deleted: changes.deletedArticles.length,
        priceChanged: changes.priceChangedArticles.length,
      });

      // ë³€ê²½ì‚¬í•­ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
      if (
        changes.newArticles.length === 0 &&
        changes.deletedArticles.length === 0 &&
        changes.priceChangedArticles.length === 0
      ) {
        console.log(`No changes for ${complexInfo.complexName}, skipping alerts`);
        continue;
      }

      // 3. ì•Œë¦¼ ì¡°ê±´ì— ë§ëŠ” ë³€ê²½ì‚¬í•­ í•„í„°ë§
      const alertTargets = await filterChangesForAlerts(complexNo, changes);

      if (alertTargets.length === 0) {
        console.log(`No active alerts for ${complexInfo.complexName}`);
        continue;
      }

      console.log(`ğŸ“¬ Sending alerts to ${alertTargets.length} alert(s)`);

      // 4. ê° ì•Œë¦¼ì— ëŒ€í•´ Discord ì›¹í›… ì „ì†¡
      for (const target of alertTargets) {
        if (!target.alert.webhookUrl) {
          console.log(`No webhook URL for alert: ${target.alert.name}`);
          continue;
        }

        try {
          const embeds: any[] = [];

          // ì‹ ê·œ ë§¤ë¬¼ ì•Œë¦¼
          for (const article of target.newArticles) {
            embeds.push(
              createNewArticleEmbed(article, complexInfo.complexName, complexNo)
            );
          }

          // ì‚­ì œëœ ë§¤ë¬¼ ì•Œë¦¼
          for (const article of target.deletedArticles) {
            embeds.push(
              createDeletedArticleEmbed(article, complexInfo.complexName, complexNo)
            );
          }

          // ê°€ê²© ë³€ë™ ì•Œë¦¼
          for (const { old: oldArticle, new: newArticle } of target.priceChangedArticles) {
            embeds.push(
              createPriceChangedEmbed(
                oldArticle,
                newArticle,
                complexInfo.complexName,
                complexNo
              )
            );
          }

          // ìš”ì•½ ì„ë² ë“œ ì¶”ê°€
          embeds.push(
            createCrawlSummaryEmbed({
              complexName: complexInfo.complexName,
              complexNo,
              newCount: target.newArticles.length,
              deletedCount: target.deletedArticles.length,
              priceChangedCount: target.priceChangedArticles.length,
              totalArticles: currentArticles.length,
              duration: 0, // í¬ë¡¤ë§ ì‹œê°„ì€ ì—¬ê¸°ì„  ë¶ˆí•„ìš”
            })
          );

          // Discordë¡œ ì „ì†¡ (í•œ ë²ˆì— ìµœëŒ€ 10ê°œ embed)
          for (let i = 0; i < embeds.length; i += 10) {
            const batch = embeds.slice(i, i + 10);

            const result = await sendDiscordNotification(target.alert.webhookUrl, {
              username: 'ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬',
              content:
                i === 0
                  ? `ğŸ”” **${target.alert.name}** ì•Œë¦¼\n${complexInfo.complexName}ì— ë³€ê²½ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤!`
                  : undefined,
              embeds: batch,
            });

            // ì•Œë¦¼ ë¡œê·¸ ì €ì¥
            await saveNotificationLog(
              target.alertId,
              'webhook',
              result.success ? 'sent' : 'failed',
              result.success
                ? `Sent ${batch.length} notifications`
                : result.error || 'Unknown error'
            );

            if (!result.success) {
              console.error(`Failed to send alert: ${result.error}`);
            } else {
              console.log(`âœ… Sent ${batch.length} notification(s) for alert: ${target.alert.name}`);
            }

            // Discord API ì†ë„ ì œí•œ ë°©ì§€
            if (i + 10 < embeds.length) {
              await new Promise((resolve) => setTimeout(resolve, 1000));
            }
          }
        } catch (error: any) {
          console.error(`Failed to send alert for ${target.alert.name}:`, error);
          await saveNotificationLog(
            target.alertId,
            'webhook',
            'failed',
            error.message || 'Unknown error'
          );
        }
      }
    } catch (error: any) {
      console.error(`Failed to process alerts for ${complexNo}:`, error);
    }
  }

  console.log('âœ… Alert processing completed');
}

export async function POST(request: NextRequest) {
  const startTime = Date.now();
  let crawlId: string | null = null;

  try {
    // ğŸ”’ ì¤‘ë³µ í¬ë¡¤ë§ ë°©ì§€: ì´ë¯¸ í¬ë¡¤ë§ ì§„í–‰ ì¤‘ì¸ ê²½ìš° ê±°ë¶€
    if (isCurrentlyCrawling) {
      logger.warn('Crawl request rejected: Another crawl is already in progress', {
        currentCrawlId,
      });
      return NextResponse.json(
        {
          error: 'ì´ë¯¸ í¬ë¡¤ë§ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì™„ë£Œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
          currentCrawlId,
        },
        { status: 409 } // 409 Conflict
      );
    }

    // ë‚´ë¶€ ìŠ¤ì¼€ì¤„ëŸ¬ í˜¸ì¶œ í™•ì¸ (íŠ¹ë³„í•œ í—¤ë”ë¡œ ì‹ë³„)
    const internalSecret = request.headers.get('x-internal-secret');
    const isInternalCall = internalSecret === process.env.INTERNAL_API_SECRET;

    // Rate Limiting (ë‚´ë¶€ í˜¸ì¶œì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
    if (!isInternalCall) {
      const rateLimitResponse = rateLimit(request, rateLimitPresets.crawl);
      if (rateLimitResponse) return rateLimitResponse;
    }

    const body = await request.json();
    const {
      complexNumbers,
      userId: requestUserId,
      initiator = 'manual',  // manual, schedule, api, complex-detail
      scheduleId,
      scheduleName,
    } = body;

    // ì‚¬ìš©ì ì¸ì¦ í™•ì¸ (ë‚´ë¶€ í˜¸ì¶œì´ ì•„ë‹Œ ê²½ìš°)
    let currentUser;
    if (isInternalCall) {
      // ë‚´ë¶€ í˜¸ì¶œ: bodyì—ì„œ ì „ë‹¬ëœ userId ì‚¬ìš©
      if (!requestUserId) {
        return NextResponse.json(
          { error: 'Internal call requires userId' },
          { status: 400 }
        );
      }
      const user = await prisma.user.findUnique({ where: { id: requestUserId } });
      if (!user) {
        return NextResponse.json(
          { error: 'User not found' },
          { status: 404 }
        );
      }
      currentUser = { id: user.id, email: user.email, name: user.name, role: user.role };
    } else {
      // ì™¸ë¶€ í˜¸ì¶œ: ì„¸ì…˜ ì¸ì¦ í•„ìš”
      currentUser = await requireAuth();
    }

    if (!complexNumbers || complexNumbers.length === 0) {
      return NextResponse.json(
        { error: 'ë‹¨ì§€ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.' },
        { status: 400 }
      );
    }

    const complexNosArray = Array.isArray(complexNumbers)
      ? complexNumbers
      : complexNumbers.split(',').map((n: string) => n.trim());

    const complexNos = complexNosArray.join(',');

    // 1. í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ìƒì„± (ì§„í–‰ ì¤‘ ìƒíƒœ)
    const crawlHistory = await prisma.crawlHistory.create({
      data: {
        complexNos: complexNosArray,
        totalComplexes: complexNosArray.length,
        successCount: 0,
        errorCount: 0,
        totalArticles: 0,
        duration: 0,
        initiator,
        scheduleId,
        scheduleName,
        status: 'crawling',
        currentStep: 'Starting crawler',
        processedArticles: 0,
        processedComplexes: 0,
        userId: currentUser.id, // í¬ë¡¤ë§ ì‹¤í–‰í•œ ì‚¬ìš©ì ID
      },
    });

    crawlId = crawlHistory.id;
    currentCrawlId = crawlId;
    isCurrentlyCrawling = true; // ğŸ”’ í¬ë¡¤ë§ ì‹œì‘ í”Œë˜ê·¸ ì„¤ì •

    // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì‹œì‘
    eventBroadcaster.notifyCrawlStart(crawlId, complexNosArray.length);

    // 2. Python í¬ë¡¤ëŸ¬ ì‹¤í–‰ (crawl_id ì „ë‹¬, -u í”Œë˜ê·¸ë¡œ unbuffered ì¶œë ¥)
    const baseDir = process.env.NODE_ENV === 'production' ? '/app' : process.cwd();
    const command = `python3 -u ${baseDir}/logic/nas_playwright_crawler.py "${complexNos}" "${crawlId}"`;

    logger.info('Starting crawler', { crawlId, complexNos: complexNosArray.length });

    // ë™ì  íƒ€ì„ì•„ì›ƒ ê³„ì‚°
    const dynamicTimeout = await calculateDynamicTimeout(complexNosArray.length);
    logger.info('Dynamic timeout calculated', {
      seconds: Math.floor(dynamicTimeout / 1000),
      minutes: Math.floor(dynamicTimeout / 60000)
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: `Crawling ${complexNosArray.length} complexes`,
      },
    });

    // ìŠ¤ì¼€ì¤„ ì‹¤í–‰ì¸ ê²½ìš°: crawlIdë§Œ ë°˜í™˜í•˜ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    if (initiator === 'schedule') {
      console.log(`ğŸ“¤ Returning crawlId immediately for schedule execution: ${crawlId}`);

      // ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ (await ì—†ì´)
      executeCrawlInBackground(crawlId, complexNosArray, complexNos, baseDir, dynamicTimeout, currentUser.id, scheduleId)
        .catch((error) => {
          logger.error('Background crawl failed', { crawlId, error: error.message });
        });

      // ì¦‰ì‹œ crawlId ë°˜í™˜
      return NextResponse.json({
        success: true,
        crawlId,
        message: 'Crawl started in background',
      });
    }

    // Python í¬ë¡¤ëŸ¬ë¥¼ spawnìœ¼ë¡œ ì‹¤í–‰ (ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥)
    await new Promise<void>((resolve, reject) => {
      const pythonProcess = spawn('python3', [
        '-u',  // unbuffered output
        `${baseDir}/logic/nas_playwright_crawler.py`,
        complexNos,
        crawlId
      ], {
        cwd: baseDir,
        env: process.env,
      });

      let hasOutput = false;

      // stdout ì‹¤ì‹œê°„ ì¶œë ¥
      pythonProcess.stdout.on('data', (data) => {
        const output = data.toString();
        if (!hasOutput) {
          console.log('=== Python Crawler Output ===');
          hasOutput = true;
        }
        process.stdout.write(output);  // ì‹¤ì‹œê°„ ì¶œë ¥
      });

      // stderr ì‹¤ì‹œê°„ ì¶œë ¥
      pythonProcess.stderr.on('data', (data) => {
        const output = data.toString();
        process.stderr.write(output);  // ì‹¤ì‹œê°„ ì—ëŸ¬ ì¶œë ¥
      });

      // í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì²˜ë¦¬
      pythonProcess.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Python crawler exited with code ${code}`));
        }
      });

      // í”„ë¡œì„¸ìŠ¤ ì—ëŸ¬ ì²˜ë¦¬
      pythonProcess.on('error', (error) => {
        reject(error);
      });

      // íƒ€ì„ì•„ì›ƒ ì„¤ì •
      const timeoutId = setTimeout(() => {
        pythonProcess.kill('SIGTERM');
        reject(new Error('Crawler timeout'));
      }, dynamicTimeout);

      // í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ íƒ€ì„ì•„ì›ƒ í´ë¦¬ì–´
      pythonProcess.on('close', () => {
        clearTimeout(timeoutId);
      });
    });

    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        currentStep: 'Crawling completed',
      },
    });

    // 3. í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (Batch Insert)
    logger.info('Saving results to database');
    const dbResult = await saveCrawlResultsToDB(crawlId, complexNosArray, currentUser.id);

    const duration = Date.now() - startTime;
    const status = dbResult.errors.length > 0 ? 'partial' : 'success';

    // 4. ìµœì¢… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    await prisma.crawlHistory.update({
      where: { id: crawlId },
      data: {
        successCount: dbResult.totalComplexes,
        errorCount: complexNosArray.length - dbResult.totalComplexes,
        totalArticles: dbResult.totalArticles,
        duration: Math.floor(duration / 1000), // ì´ˆ ë‹¨ìœ„
        status,
        errorMessage: dbResult.errors.length > 0 ? dbResult.errors.join(', ') : null,
        currentStep: 'Completed',
      },
    });

    currentCrawlId = null;
    isCurrentlyCrawling = false; // ğŸ”“ í¬ë¡¤ë§ ì™„ë£Œ í”Œë˜ê·¸ í•´ì œ

    // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì™„ë£Œ
    eventBroadcaster.notifyCrawlComplete(crawlId, dbResult.totalArticles);

    logger.info('Crawl completed and saved to DB', {
      complexes: dbResult.totalComplexes,
      articles: dbResult.totalArticles,
      durationSeconds: Math.floor(duration / 1000),
      status
    });

    // 5. ì•Œë¦¼ ë°œì†¡ (ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ì§€ì—° ë°©ì§€)
    sendAlertsForChanges(complexNosArray).catch((error) => {
      logger.error('Failed to send alerts', error);
    });

    return NextResponse.json({
      success: true,
      message: 'í¬ë¡¤ë§ì´ ì™„ë£Œë˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
      crawlId,
      data: {
        complexes: dbResult.totalComplexes,
        articles: dbResult.totalArticles,
        duration: Math.floor(duration / 1000),
        errors: dbResult.errors,
      },
    });

  } catch (error: any) {
    logger.error('Crawling error', error);

    // ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    if (crawlId) {
      try {
        const duration = Date.now() - startTime;
        await prisma.crawlHistory.update({
          where: { id: crawlId },
          data: {
            duration: Math.floor(duration / 1000),
            status: 'failed',
            errorMessage: error.message,
            currentStep: 'Failed',
          },
        });

        // ğŸ”” ì‹¤ì‹œê°„ ì•Œë¦¼: í¬ë¡¤ë§ ì‹¤íŒ¨
        eventBroadcaster.notifyCrawlFailed(crawlId, error.message);
      } catch (historyError: any) {
        logger.error('Failed to update error history', historyError);
      }
    }

    currentCrawlId = null;
    isCurrentlyCrawling = false; // ğŸ”“ ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ í”Œë˜ê·¸ í•´ì œ

    return NextResponse.json(
      {
        error: 'í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: error.message,
        crawlId,
      },
      { status: 500 }
    );
  }
}

// ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡
async function sendScheduleCrawlCompleteNotification(
  scheduleId: string,
  dbResult: { totalComplexes: number; totalArticles: number; errors: string[] },
  duration: number
) {
  try {
    // ìŠ¤ì¼€ì¤„ ì •ë³´ ì¡°íšŒ
    const schedule = await prisma.schedule.findUnique({
      where: { id: scheduleId },
      include: {
        user: true,
      },
    });

    if (!schedule) {
      logger.warn('Schedule not found for notification', { scheduleId });
      return;
    }

    // ì‚¬ìš©ìì˜ ì•Œë¦¼ ì„¤ì • í™•ì¸ (í™œì„±í™”ëœ ì•Œë¦¼ì´ ìˆê³  webhookUrlì´ ìˆëŠ” ê²½ìš°)
    const userAlerts = await prisma.alert.findMany({
      where: {
        userId: schedule.userId,
        isActive: true,
        webhookUrl: { not: null },
      },
      take: 1, // í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ë¨ (webhookUrl ê°€ì ¸ì˜¤ê¸° ìœ„í•´)
    });

    if (userAlerts.length === 0) {
      logger.info('No active alerts with webhook URL for schedule notification', { scheduleId });
      return;
    }

    const webhookUrl = userAlerts[0].webhookUrl!;

    // Discord ì„ë² ë“œ ìƒì„±
    const embed = {
      title: 'â° ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì™„ë£Œ',
      description: `**${schedule.name}** ìŠ¤ì¼€ì¤„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`,
      color: dbResult.errors.length > 0 ? 0xfbbf24 : 0x10b981, // ì—ëŸ¬ ìˆìœ¼ë©´ ë…¸ë€ìƒ‰, ì—†ìœ¼ë©´ ì´ˆë¡ìƒ‰
      fields: [
        {
          name: 'ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼',
          value: `â€¢ ë‹¨ì§€ ìˆ˜: ${dbResult.totalComplexes}ê°œ\nâ€¢ ë§¤ë¬¼ ìˆ˜: ${dbResult.totalArticles}ê°œ\nâ€¢ ì†Œìš” ì‹œê°„: ${Math.floor(duration / 1000)}ì´ˆ`,
          inline: false,
        },
      ],
      timestamp: new Date().toISOString(),
      footer: {
        text: `Schedule ID: ${scheduleId.substring(0, 8)}`,
      },
    };

    // ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if (dbResult.errors.length > 0) {
      embed.fields.push({
        name: 'âš ï¸ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ',
        value: dbResult.errors.slice(0, 3).join('\n') + (dbResult.errors.length > 3 ? `\n... ì™¸ ${dbResult.errors.length - 3}ê°œ` : ''),
        inline: false,
      });
    }

    // Discord ì›¹í›… ì „ì†¡
    const response = await fetch(webhookUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        embeds: [embed],
      }),
    });

    if (!response.ok) {
      logger.error('Failed to send schedule completion notification to Discord', {
        status: response.status,
        statusText: response.statusText,
      });
    } else {
      logger.info('Schedule completion notification sent successfully', { scheduleId });
    }
  } catch (error: any) {
    logger.error('Error sending schedule completion notification', error);
  }
}

export async function GET() {
  return NextResponse.json({
    message: 'POST ìš”ì²­ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.',
    example: {
      complexNumbers: ['22065', '12345']
    }
  });
}
