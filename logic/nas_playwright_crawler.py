#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NAS í™˜ê²½ìš© ë„¤ì´ë²„ ë¶€ë™ì‚° Playwright í¬ë¡¤ëŸ¬
í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ë™ì‘í•˜ì—¬ ìŠ¤í¬ë¦°ì´ ì—†ëŠ” NASì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥
"""

import asyncio
import json
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

from playwright.async_api import async_playwright, Browser, Page, BrowserContext
import pandas as pd
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class NASNaverRealEstateCrawler:
    """NAS í™˜ê²½ìš© ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.status_file = None  # ì§„í–‰ ìƒíƒœ íŒŒì¼
        self.start_time = None  # í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„
        self.results = []
        self.output_dir = Path(os.getenv('OUTPUT_DIR', './crawled_data'))
        self.output_dir.mkdir(exist_ok=True)
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.request_delay = float(os.getenv('REQUEST_DELAY', '2.0'))  # ìš”ì²­ ê°„ê²© (ì´ˆ)
        self.timeout = int(os.getenv('TIMEOUT', '30000'))  # íƒ€ì„ì•„ì›ƒ (ë°€ë¦¬ì´ˆ)
        self.headless = os.getenv('HEADLESS', 'true').lower() == 'true'
        
        print(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"- ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"- ìš”ì²­ ê°„ê²©: {self.request_delay}ì´ˆ")
        print(f"- í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ: {self.headless}")
        print(f"- íƒ€ì„ì•„ì›ƒ: {self.timeout}ms")
    
    def update_status(self, status: str, progress: int, total: int, current_complex: str = "", message: str = "", items_collected: int = 0):
        """ì§„í–‰ ìƒíƒœë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not self.status_file:
            return
        
        # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
        elapsed_seconds = 0
        speed = 0.0
        estimated_total_seconds = 0
        
        if self.start_time:
            elapsed_seconds = int((datetime.now() - self.start_time).total_seconds())
            
            # ì†ë„ ê³„ì‚° (ë§¤ë¬¼/ì´ˆ)
            if elapsed_seconds > 0 and items_collected > 0:
                speed = round(items_collected / elapsed_seconds, 2)
            
            # ì˜ˆìƒ ì´ ì†Œìš” ì‹œê°„ ê³„ì‚° (ë‹¨ì§€ ê¸°ì¤€)
            if progress > 0 and total > 0:
                avg_time_per_complex = elapsed_seconds / progress
                estimated_total_seconds = int(avg_time_per_complex * total)
        
        status_data = {
            "status": status,  # "running", "completed", "error"
            "progress": progress,
            "total": total,
            "percent": round((progress / total * 100) if total > 0 else 0, 1),
            "current_complex": current_complex,
            "message": message,
            "timestamp": datetime.now().isoformat(),
            # ì‹œê°„ ì •ë³´
            "elapsed_seconds": elapsed_seconds,
            "estimated_total_seconds": estimated_total_seconds,
            # ì†ë„ ì •ë³´
            "items_collected": items_collected,
            "speed": speed  # ë§¤ë¬¼/ì´ˆ
        }
        
        try:
            with open(self.status_file, 'w', encoding='utf-8') as f:
                json.dump(status_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[WARNING] ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™”"""
        try:
            playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (NAS í™˜ê²½ì— ìµœì í™”)
            browser_options = {
                'headless': self.headless,
                'args': [
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--memory-pressure-off',
                    '--max_old_space_size=4096'
                ]
            }
            
            # Chrome ë¸Œë¼ìš°ì € ì‹¤í–‰
            self.browser = await playwright.chromium.launch(**browser_options)
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¿ í‚¤, ì„¸ì…˜ ê´€ë¦¬)
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                extra_http_headers={
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                }
            )
            
            # í˜ì´ì§€ ìƒì„±
            self.page = await self.context.new_page()
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.page.set_default_timeout(self.timeout)
            
            print("ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            print("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    async def crawl_complex_overview(self, complex_no: str) -> Optional[Dict]:
        """ë‹¨ì§€ ê°œìš” ì •ë³´ í¬ë¡¤ë§"""
        try:
            print(f"ë‹¨ì§€ ê°œìš” ì •ë³´ í¬ë¡¤ë§ ì‹œì‘: {complex_no}")
            
            # ë„¤ì´ë²„ ë¶€ë™ì‚° ë‹¨ì§€ í˜ì´ì§€ ì ‘ì†
            url = f"https://new.land.naver.com/complexes/{complex_no}"
            await self.page.goto(url, wait_until='networkidle')
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            await asyncio.sleep(3)
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§í•˜ì—¬ API ì‘ë‹µ ìºì¹˜
            overview_data = None
            
            async def handle_response(response):
                nonlocal overview_data
                if f'/api/complexes/overview/{complex_no}' in response.url:
                    try:
                        data = await response.json()
                        overview_data = data
                        print(f"ë‹¨ì§€ ê°œìš” API ì‘ë‹µ ìºì¹˜ë¨")
                    except Exception as e:
                        print(f"API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ë“±ë¡
            self.page.on('response', handle_response)
            
            # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ API í˜¸ì¶œ íŠ¸ë¦¬ê±°
            await self.page.reload(wait_until='networkidle')
            await asyncio.sleep(2)
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ì œê±°
            self.page.remove_listener('response', handle_response)
            
            return overview_data
            
        except Exception as e:
            print(f"ë‹¨ì§€ ê°œìš” í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    async def crawl_complex_articles_with_scroll(self, complex_no: str) -> Optional[Dict]:
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§"""
        try:
            print(f"ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§ ì‹œì‘ (ë¬´í•œ ìŠ¤í¬ë¡¤): {complex_no}")
            
            # ëª¨ë“  ë§¤ë¬¼ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            all_articles = []
            collected_article_ids = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            # API ì‘ë‹µ ìˆ˜ì§‘
            async def handle_articles_response(response):
                # ë§¤ë¬¼ ëª©ë¡ API ì‘ë‹µ ê°ì§€
                if f'/api/articles/complex/{complex_no}' in response.url:
                    # ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì ìš© ì—¬ë¶€ í™•ì¸
                    same_group = 'sameAddressGroup=true' in response.url or 'sameAddressGroup=Y' in response.url
                    group_status = "âœ… ON" if same_group else "âŒ OFF"
                    
                    print(f"[API] í˜¸ì¶œ ê°ì§€ #{len(all_articles)//20 + 1} (ë™ì¼ë§¤ë¬¼ë¬¶ê¸°: {group_status})")
                    if len(all_articles) == 0:  # ì²« API í˜¸ì¶œë§Œ ì „ì²´ URL ë¡œê·¸
                        print(f"[API] URL: {response.url[:120]}...")
                    
                    try:
                        data = await response.json()
                        article_list = data.get('articleList', [])
                        total_count = data.get('totalCount', 0)
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        new_count = 0
                        for article in article_list:
                            article_id = article.get('articleNo') or article.get('id')
                            if article_id and article_id not in collected_article_ids:
                                collected_article_ids.add(article_id)
                                all_articles.append(article)
                                new_count += 1
                        
                        if new_count > 0:
                            total_info = f", ì „ì²´: {total_count}ê±´" if total_count > 0 else ""
                            print(f"  â†’ {new_count}ê°œ ìƒˆ ë§¤ë¬¼ ì¶”ê°€ (ì´ {len(all_articles)}ê°œ{total_info})")
                    except Exception as e:
                        print(f"ë§¤ë¬¼ API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ë“±ë¡
            self.page.on('response', handle_articles_response)
            
            try:
                # 1. ë©”ì¸ í˜ì´ì§€ì—ì„œ localStorage ì„¤ì • (ì¤‘ìš”!)
                print("ğŸ”§ ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì„¤ì • ì¤€ë¹„ ì¤‘...")
                await self.page.goto("https://new.land.naver.com", wait_until='domcontentloaded')
                
                await self.page.evaluate('''
                    () => {
                        localStorage.setItem('sameAddrYn', 'true');
                        localStorage.setItem('sameAddressGroup', 'true');
                        console.log('[LocalStorage] ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì„¤ì • ì™„ë£Œ');
                    }
                ''')
                print("âœ… localStorage ì„¤ì • ì™„ë£Œ")
                await asyncio.sleep(1)
                
                # 2. ë‹¨ì§€ í˜ì´ì§€ë¡œ ì´ë™ (localStorage ê°’ì´ ìë™ ì ìš©ë¨)
                url = f"https://new.land.naver.com/complexes/{complex_no}"
                print(f"URL ì ‘ì†: {url}")
                await self.page.goto(url, wait_until='networkidle')
                await asyncio.sleep(3)
                
                # 2. ë§¤ë¬¼ íƒ­ í´ë¦­
                print("ë§¤ë¬¼ íƒ­ ì°¾ëŠ” ì¤‘...")
                try:
                    selectors = [
                        'a[href*="article"]',
                        'button:has-text("ë§¤ë¬¼")',
                        '[class*="article"]',
                    ]
                    
                    for selector in selectors:
                        try:
                            element = await self.page.wait_for_selector(selector, timeout=5000)
                            if element:
                                await element.click()
                                print(f"ë§¤ë¬¼ íƒ­ í´ë¦­ ì„±ê³µ")
                                await asyncio.sleep(3)
                                break
                        except:
                            continue
                except Exception as e:
                    print(f"ë§¤ë¬¼ íƒ­ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # 3. localStorage ë° ì²´í¬ë°•ìŠ¤ ìƒíƒœ ê²€ì¦
                print("ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ìƒíƒœ ê²€ì¦ ì¤‘...")
                storage_check = await self.page.evaluate('''
                    () => {
                        const sameAddrYn = localStorage.getItem('sameAddrYn');
                        const sameAddressGroup = localStorage.getItem('sameAddressGroup');
                        
                        // ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸
                        const checkboxes = document.querySelectorAll('input[type="checkbox"]');
                        let checkboxState = null;
                        
                        for (const checkbox of checkboxes) {
                            const label = checkbox.closest('label') || checkbox.nextElementSibling;
                            const text = label ? (label.textContent || label.innerText || '') : '';
                            if (text.includes('ë™ì¼ë§¤ë¬¼')) {
                                checkboxState = {
                                    checked: checkbox.checked,
                                    labelText: text
                                };
                                break;
                            }
                        }
                        
                        return {
                            sameAddrYn,
                            sameAddressGroup,
                            checkboxState
                        };
                    }
                ''')
                print(f"[DEBUG] localStorage ìƒíƒœ: sameAddrYn={storage_check.get('sameAddrYn')}, sameAddressGroup={storage_check.get('sameAddressGroup')}")
                if storage_check.get('checkboxState'):
                    print(f"[DEBUG] ì²´í¬ë°•ìŠ¤ ìƒíƒœ: checked={storage_check['checkboxState'].get('checked')}")
                
                # 4. ì²´í¬ë°•ìŠ¤ê°€ ì²´í¬ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í´ë¦­
                if storage_check.get('checkboxState') and not storage_check['checkboxState'].get('checked'):
                    print("ğŸ”˜ ì²´í¬ë°•ìŠ¤ í´ë¦­ ì¤‘...")
                    clicked = await self.page.evaluate('''
                        () => {
                            const checkboxes = document.querySelectorAll('input[type="checkbox"]');
                            for (const checkbox of checkboxes) {
                                const label = checkbox.closest('label') || checkbox.nextElementSibling;
                                const text = label ? (label.textContent || label.innerText || '') : '';
                                if (text.includes('ë™ì¼ë§¤ë¬¼')) {
                                    checkbox.click();
                                    console.log('[Checkbox] í´ë¦­ ì™„ë£Œ');
                                    return true;
                                }
                            }
                            return false;
                        }
                    ''')
                    
                    if clicked:
                        print("[DEBUG] ì²´í¬ë°•ìŠ¤ í´ë¦­ ì™„ë£Œ, ë°ì´í„° ì¬ë¡œë”© ëŒ€ê¸°...")
                        await asyncio.sleep(3)
                        print("âœ… ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° í™œì„±í™” ì™„ë£Œ")
                    else:
                        print("[DEBUG] ì²´í¬ë°•ìŠ¤ë¥¼ ì°¾ì§€ ëª»í•¨")
                else:
                    print("âœ… ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì´ë¯¸ í™œì„±í™”ë¨")
                
                # 5. ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                print("ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì°¾ëŠ” ì¤‘...")
                list_container = None
                container_selectors = [
                    '[class*="list"]',
                    '[class*="article"]',
                    '[class*="item"]',
                ]
                
                for selector in container_selectors:
                    try:
                        list_container = await self.page.wait_for_selector(selector, timeout=5000)
                        if list_container:
                            print(f"ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                            break
                    except:
                        continue
                
                # 4. ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸°
                await asyncio.sleep(3)
                initial_count = len(all_articles)
                print(f"ì´ˆê¸° ë§¤ë¬¼ ìˆ˜: {initial_count}ê°œ")
                
                # 5. ì ì§„ì  ìŠ¤í¬ë¡¤ë¡œ ë°ì´í„° ìˆ˜ì§‘ (crawler_service.py ë°©ì‹)
                print("ì¶”ê°€ ë§¤ë¬¼ ìˆ˜ì§‘ ì‹œì‘ (ì ì§„ì  ìŠ¤í¬ë¡¤)...")
                print(f"[ì„¤ì •] ìµœëŒ€ ì‹œë„: 100íšŒ, API ëŒ€ê¸°: 2.5ì´ˆ, ì¢…ë£Œ ì¡°ê±´: 8íšŒ ì—°ì† ë³€í™” ì—†ìŒ")
                scroll_attempts = 0
                max_scroll_attempts = 100  # ìµœëŒ€ 100íšŒ
                scroll_end_count = 0  # ìŠ¤í¬ë¡¤ì´ ì•ˆ ì›€ì§ì´ëŠ” íšŸìˆ˜
                max_scroll_end = 8  # 8íšŒ ì—°ì† ìŠ¤í¬ë¡¤ ì•ˆ ë˜ë©´ ì¢…ë£Œ (5â†’8 ì™„í™”)
                
                while scroll_attempts < max_scroll_attempts:
                    prev_count = len(all_articles)
                    
                    # ë§¤ë¬¼ ìŠ¤í¬ë¡¤ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if scroll_attempts % 3 == 0:  # 3íšŒë§ˆë‹¤ ì—…ë°ì´íŠ¸ (ë„ˆë¬´ ìì£¼ ì—…ë°ì´íŠ¸í•˜ë©´ ë¶€í•˜)
                        self.update_status(
                            status="running",
                            progress=len(all_articles),
                            total=100,  # ì˜ˆìƒ ì´ ë§¤ë¬¼ ìˆ˜ (ì‹¤ì œëŠ” ì•Œ ìˆ˜ ì—†ìŒ)
                            current_complex=complex_no,
                            message=f"ğŸ”„ ë§¤ë¬¼ ìŠ¤í¬ë¡¤ ì¤‘... (ì‹œë„ {scroll_attempts}íšŒ, ìˆ˜ì§‘ {len(all_articles)}ê°œ)",
                            items_collected=len(all_articles)
                        )
                    
                    # ë„¤ì´ë²„ ì‹¤ì œ ì»¨í…Œì´ë„ˆë¡œ ì ì§„ì  ìŠ¤í¬ë¡¤ (500pxì”©)
                    scroll_result = await self.page.evaluate('''
                        () => {
                            // ë„¤ì´ë²„ê°€ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ì…€ë ‰í„°ë“¤
                            const selectors = [
                                '.item_list',  // âœ… crawler_service.pyì—ì„œ ì‚¬ìš©
                                'div[class*="list_contents"]',
                                'div[class*="item_list"]',
                                'div[class*="article_list"]'
                            ];
                            
                            let container = null;
                            for (const selector of selectors) {
                                container = document.querySelector(selector);
                                if (container && container.scrollHeight > container.clientHeight) {
                                    break;
                                }
                            }
                            
                            if (!container) {
                                return { found: false, reason: 'container not found' };
                            }
                            
                            // ì ì§„ì  ìŠ¤í¬ë¡¤ (500pxì”©)
                            const before = container.scrollTop;
                            container.scrollTop += 500;  // âœ… í•œ ë²ˆì— ëê¹Œì§€ ê°€ì§€ ì•ŠìŒ
                            const after = container.scrollTop;
                            
                            const items = container.querySelectorAll('.item_link, .item_inner, [class*="item"]');
                            
                            return {
                                found: true,
                                moved: after > before,  // âœ… ì‹¤ì œë¡œ ìŠ¤í¬ë¡¤ë˜ì—ˆëŠ”ì§€
                                scrollBefore: before,
                                scrollAfter: after,
                                scrollDelta: after - before,
                                scrollHeight: container.scrollHeight,
                                clientHeight: container.clientHeight,
                                itemCount: items.length,
                                containerClass: container.className
                            };
                        }
                    ''')
                        
                    if scroll_attempts == 0:
                        if scroll_result.get('found'):
                            print(f"[DEBUG] ì»¨í…Œì´ë„ˆ ë°œê²¬: .{scroll_result.get('containerClass', 'unknown')}")
                            print(f"  DOM ì•„ì´í…œ: {scroll_result.get('itemCount', 0)}ê°œ (ë™ì¼ë§¤ë¬¼ë¬¶ê¸° ì´ì „, ì°¸ê³ ìš©)")
                            print(f"  ìŠ¤í¬ë¡¤ ë†’ì´: {scroll_result.get('scrollHeight')} / {scroll_result.get('clientHeight')}")
                            print(f"  ğŸ’¡ ì‹¤ì œ ìˆ˜ì§‘ ê°œìˆ˜ëŠ” API ì‘ë‹µ ê¸°ì¤€ (ë™ì¼ë§¤ë¬¼ë¬¶ê¸° ì´í›„)")
                        else:
                            print(f"[DEBUG] ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í•¨: {scroll_result.get('reason', 'unknown')}")
                    
                    # API í˜¸ì¶œ ëŒ€ê¸° (1.5ì´ˆ â†’ 2.5ì´ˆë¡œ ì¦ê°€)
                    await asyncio.sleep(2.5)  # âœ… API ì‘ë‹µ ì¶©ë¶„íˆ ëŒ€ê¸°
                    
                    current_count = len(all_articles)
                    new_items = current_count - prev_count
                    
                    scroll_attempts += 1
                    
                    # ì¢…ë£Œ ì¡°ê±´: ìŠ¤í¬ë¡¤ ë + ë°ì´í„° ì¦ê°€ ì—†ìŒ (ë‘˜ ë‹¤ ì¶©ì¡±í•´ì•¼ í•¨)
                    scroll_ended = scroll_result.get('found') and not scroll_result.get('moved')
                    no_new_data = new_items == 0
                    
                    if scroll_ended and no_new_data:
                        scroll_end_count += 1
                        print(f"ì‹œë„ {scroll_attempts}íšŒ: ìŠ¤í¬ë¡¤ ë & ë°ì´í„° ì—†ìŒ ({scroll_end_count}/{max_scroll_end}) - ì´ {current_count}ê°œ")
                        print(f"  â†’ ìŠ¤í¬ë¡¤: {scroll_result.get('scrollAfter')} / {scroll_result.get('scrollHeight')}")
                        
                        if scroll_end_count >= max_scroll_end:
                            print(f"â¹ï¸  ìˆ˜ì§‘ ì¢…ë£Œ ({max_scroll_end}íšŒ ì—°ì† ë³€í™” ì—†ìŒ)")
                            print(f"ğŸ“Š ìµœì¢…: {current_count}ê°œ ìˆ˜ì§‘ (DOM: {scroll_result.get('itemCount', 0)}ê°œëŠ” ë™ì¼ë§¤ë¬¼ë¬¶ê¸° ì´ì „)")
                            break
                    else:
                        # ìŠ¤í¬ë¡¤ì´ ëì´ì–´ë„ ë°ì´í„°ê°€ ì¦ê°€í•˜ë©´ ê³„ì† ì‹œë„
                        if new_items > 0:
                            scroll_end_count = 0  # ë°ì´í„° ì¦ê°€í•˜ë©´ ë¦¬ì…‹
                            print(f"ì‹œë„ {scroll_attempts}íšŒ: +{scroll_result.get('scrollDelta', 0)}px ìŠ¤í¬ë¡¤ â†’ ğŸ‰ {new_items}ê°œ ì¶”ê°€ (ì´ {current_count}ê°œ)")
                        elif scroll_ended:
                            # ìŠ¤í¬ë¡¤ ëì´ì§€ë§Œ ë°ì´í„° ì¦ê°€ ëŒ€ê¸° ì¤‘
                            print(f"ì‹œë„ {scroll_attempts}íšŒ: ìŠ¤í¬ë¡¤ ë ë„ë‹¬, API ì‘ë‹µ ëŒ€ê¸° ì¤‘... (ì´ {current_count}ê°œ)")
                        else:
                            scroll_end_count = 0  # ìŠ¤í¬ë¡¤ ì¤‘ì´ë©´ ë¦¬ì…‹
                            print(f"ì‹œë„ {scroll_attempts}íšŒ: +{scroll_result.get('scrollDelta', 0)}px ìŠ¤í¬ë¡¤ ì¤‘... (ì´ {current_count}ê°œ, ëŒ€ê¸° ì¤‘)")
                
                if len(all_articles) > initial_count:
                    print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ: ì´ˆê¸° {initial_count}ê°œ â†’ ìµœì¢… {len(all_articles)}ê°œ (ì´ {scroll_attempts}íšŒ ì‹œë„)")
                else:
                    print(f"âš ï¸  ì¶”ê°€ ìˆ˜ì§‘ ì‹¤íŒ¨: {initial_count}ê°œì—ì„œ ë³€í™” ì—†ìŒ (ì´ {scroll_attempts}íšŒ ì‹œë„)")
                    print(f"   â†’ ì‹¤ì œë¡œ {initial_count}ê°œë§Œ ìˆê±°ë‚˜, ìŠ¤í¬ë¡¤/ë²„íŠ¼ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ")
                
            except Exception as e:
                print(f"ìŠ¤í¬ë¡¤ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ì œê±°
            self.page.remove_listener('response', handle_articles_response)
            
            if all_articles:
                return {
                    'articleList': all_articles,
                    'totalCount': len(all_articles),
                    'isMoreData': False
                }
            else:
                print("ë§¤ë¬¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            print(f"ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    async def crawl_complex_articles(self, complex_no: str, page_num: int = 1) -> Optional[Dict]:
        """ë‹¨ì§€ ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§"""
        # ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë§¤ë¬¼ ìˆ˜ì§‘
        if page_num == 1:
            return await self.crawl_complex_articles_with_scroll(complex_no)
        else:
            return None

    async def crawl_complex_data(self, complex_no: str) -> Dict:
        """ë‹¨ì§€ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§"""
        print(f"\n{'='*60}")
        print(f"ë‹¨ì§€ ë²ˆí˜¸ {complex_no} í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*60}")
        
        complex_data = {
            'crawling_info': {
                'complex_no': complex_no,
                'crawling_date': datetime.now().isoformat(),
                'crawler_version': '1.0.0'
            }
        }
        
        try:
            # 1. ë‹¨ì§€ ê°œìš” ì •ë³´
            overview = await self.crawl_complex_overview(complex_no)
            if overview:
                complex_data['overview'] = overview
                complex_name = overview.get('complexName', 'Unknown')
                print(f"ë‹¨ì§€ëª…: {complex_name}")
                print(f"ì„¸ëŒ€ìˆ˜: {overview.get('totalHouseHoldCount', 'Unknown')}")
                print(f"ë™ìˆ˜: {overview.get('totalDongCount', 'Unknown')}")
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            await asyncio.sleep(self.request_delay)
            
            # 2. ë§¤ë¬¼ ëª©ë¡ (ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹)
            articles = await self.crawl_complex_articles(complex_no, 1)
            if articles:
                complex_data['articles'] = articles
                article_count = len(articles.get('articleList', []))
                print(f"ë§¤ë¬¼ ìˆ˜: {article_count}ê°œ")
            
            print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            complex_data['error'] = str(e)
        
        return complex_data

    async def crawl_multiple_complexes(self, complex_numbers: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ ë‹¨ì§€ í¬ë¡¤ë§"""
        results = []
        total = len(complex_numbers)
        
        for i, complex_no in enumerate(complex_numbers, 1):
            print(f"\nì§„í–‰ë¥ : {i}/{total}")
            
            # í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì „ì²´ ë§¤ë¬¼ ìˆ˜ ê³„ì‚°
            total_items_so_far = 0
            for r in results:
                if 'articles' in r and 'articleList' in r['articles']:
                    total_items_so_far += len(r['articles']['articleList'])
            
            # ë‹¨ì§€ ê°œìš” ìˆ˜ì§‘ ì „ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="running",
                progress=i - 1,
                total=total,
                current_complex=complex_no,
                message=f"ğŸ“‹ ë‹¨ì§€ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i}/{total})",
                items_collected=total_items_so_far
            )
            
            try:
                complex_data = await self.crawl_complex_data(complex_no)
                results.append(complex_data)
                
                # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
                article_count = 0
                if 'articles' in complex_data and 'articleList' in complex_data['articles']:
                    article_count = len(complex_data['articles']['articleList'])
                
                # ì „ì²´ ë§¤ë¬¼ ìˆ˜ ì¬ê³„ì‚°
                total_items_so_far = 0
                for r in results:
                    if 'articles' in r and 'articleList' in r['articles']:
                        total_items_so_far += len(r['articles']['articleList'])
                
                self.update_status(
                    status="running",
                    progress=i,
                    total=total,
                    current_complex=complex_no,
                    message=f"âœ… ì™„ë£Œ: {complex_data.get('overview', {}).get('complexName', complex_no)} ({article_count}ê°œ ë§¤ë¬¼)",
                    items_collected=total_items_so_far
                )
                
                # ë‹¨ì§€ ê°„ ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                if i < total:
                    await asyncio.sleep(self.request_delay * 2)
                    
            except Exception as e:
                print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results.append({
                    'complex_no': complex_no,
                    'error': str(e),
                    'crawling_date': datetime.now().isoformat()
                })
                
                # ì‹¤íŒ¨ ì‹œì—ë„ ì „ì²´ ë§¤ë¬¼ ìˆ˜ ê³„ì‚°
                total_items_so_far = 0
                for r in results:
                    if 'articles' in r and 'articleList' in r['articles']:
                        total_items_so_far += len(r['articles']['articleList'])
                
                # ì‹¤íŒ¨ ì‹œì—ë„ ìƒíƒœ ì—…ë°ì´íŠ¸
                self.update_status(
                    status="running",
                    progress=i,
                    total=total,
                    current_complex=complex_no,
                    message=f"âŒ ì‹¤íŒ¨: {complex_no} - {str(e)[:50]}",
                    items_collected=total_items_so_far
                )
        
        return results

    def save_data(self, data: Any, filename_prefix: str = "naver_complex"):
        """ë°ì´í„° ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # JSON ì €ì¥
            json_filename = self.output_dir / f"{filename_prefix}_{timestamp}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"JSON ë°ì´í„° ì €ì¥: {json_filename}")
            
            # CSV ì €ì¥ (ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ì¸ ê²½ìš°)
            if isinstance(data, list) and data and isinstance(data[0], dict):
                # ë‹¨ì§€ ê°œìš” ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ì €ì¥
                csv_data = []
                for item in data:
                    if 'overview' in item:
                        overview = item['overview']
                        csv_data.append({
                            'ë‹¨ì§€ë²ˆí˜¸': overview.get('complexNo', ''),
                            'ë‹¨ì§€ëª…': overview.get('complexName', ''),
                            'ì„¸ëŒ€ìˆ˜': overview.get('totalHouseHoldCount', ''),
                            'ë™ìˆ˜': overview.get('totalDongCount', ''),
                            'ì‚¬ìš©ìŠ¹ì¸ì¼': overview.get('useApproveYmd', ''),
                            'ìµœì†Œë©´ì ': overview.get('minArea', ''),
                            'ìµœëŒ€ë©´ì ': overview.get('maxArea', ''),
                            'ìµœì†Œê°€ê²©': overview.get('minPrice', ''),
                            'ìµœëŒ€ê°€ê²©': overview.get('maxPrice', ''),
                            'ìœ„ë„': overview.get('latitude', ''),
                            'ê²½ë„': overview.get('longitude', ''),
                            'í¬ë¡¤ë§ì¼ì‹œ': item.get('crawling_info', {}).get('crawling_date', '')
                        })
                
                if csv_data:
                    df = pd.DataFrame(csv_data)
                    csv_filename = self.output_dir / f"{filename_prefix}_{timestamp}.csv"
                    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                    print(f"CSV ë°ì´í„° ì €ì¥: {csv_filename}")
            
        except Exception as e:
            print(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    async def run_crawling(self, complex_numbers: List[str]):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        # ìƒíƒœ íŒŒì¼ ë° ì‹œì‘ ì‹œê°„ ì„¤ì •
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.status_file = self.output_dir / f"crawl_status_{timestamp}.json"
        self.start_time = datetime.now()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        
        try:
            # ë¸Œë¼ìš°ì € ì„¤ì •
            await self.setup_browser()
            
            # í¬ë¡¤ë§ ì‹œì‘ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="running",
                progress=0,
                total=len(complex_numbers),
                message="ğŸš€ í¬ë¡¤ë§ ì‹œì‘ ì¤‘...",
                items_collected=0
            )
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            if len(complex_numbers) == 1:
                data = await self.crawl_complex_data(complex_numbers[0])
                results = [data]
            else:
                results = await self.crawl_multiple_complexes(complex_numbers)
            
            # ë°ì´í„° ì €ì¥
            self.save_data(results, f"complexes_{len(complex_numbers)}")
            
            # ê²°ê³¼ ìš”ì•½
            print(f"\n{'='*60}")
            print("í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"{'='*60}")
            print(f"ì´ {len(results)}ê°œ ë‹¨ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
            
            success_count = len([r for r in results if 'overview' in r])
            error_count = len([r for r in results if 'error' in r])
            print(f"ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {error_count}ê°œ")
            
            # ì „ì²´ ìˆ˜ì§‘ëœ ë§¤ë¬¼ ìˆ˜ ê³„ì‚°
            total_items = 0
            for r in results:
                if 'articles' in r and 'articleList' in r['articles']:
                    total_items += len(r['articles']['articleList'])
            
            # í¬ë¡¤ë§ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="completed",
                progress=len(complex_numbers),
                total=len(complex_numbers),
                message=f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count}",
                items_collected=total_items
            )
            
            return results
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="error",
                progress=0,
                total=len(complex_numbers),
                message=f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}",
                items_collected=0
            )
            
            raise
        finally:
            await self.close_browser()


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = NASNaverRealEstateCrawler()
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        complex_numbers = sys.argv[1].split(',')
        complex_numbers = [num.strip() for num in complex_numbers if num.strip()]
    else:
        # ê¸°ë³¸ê°’: ë™íƒ„ì‹œë²”ë‹¤ì€ë§ˆì„ì›”ë“œë©”ë¥´ë””ì•™ë°˜ë„ìœ ë³´ë¼
        complex_numbers = ['22065']
    
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ë‹¨ì§€: {complex_numbers}")
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    await crawler.run_crawling(complex_numbers)


if __name__ == "__main__":
    asyncio.run(main())
