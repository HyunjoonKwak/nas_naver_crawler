#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NAS í™˜ê²½ìš© ë„¤ì´ë²„ ë¶€ë™ì‚° Playwright í¬ë¡¤ëŸ¬
í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ë™ì‘í•˜ì—¬ ìŠ¤í¬ë¦°ì´ ì—†ëŠ” NASì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥
"""

import asyncio
import json
import os
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any

from playwright.async_api import async_playwright, Browser, Page, BrowserContext
import pandas as pd
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í•œêµ­ ì‹œê°„ëŒ€ (UTC+9)
KST = timezone(timedelta(hours=9))

def get_kst_now():
    """í•œêµ­ ì‹œê°„ìœ¼ë¡œ í˜„ì¬ ì‹œê° ë°˜í™˜"""
    return datetime.now(KST)


class NASNaverRealEstateCrawler:
    """NAS í™˜ê²½ìš© ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬"""

    def __init__(self, crawl_id: Optional[str] = None):
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.status_file = None  # ì§„í–‰ ìƒíƒœ íŒŒì¼ (ë°±ì—…ìš©)
        self.start_time = None  # í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„
        self.results = []
        self.output_dir = Path(os.getenv('OUTPUT_DIR', './crawled_data'))
        self.output_dir.mkdir(exist_ok=True)

        # í¬ë¡¤ë§ ì„¤ì •
        self.request_delay = float(os.getenv('REQUEST_DELAY', '2.0'))  # ìš”ì²­ ê°„ê²© (ì´ˆ)
        self.timeout = int(os.getenv('TIMEOUT', '30000'))  # íƒ€ì„ì•„ì›ƒ (ë°€ë¦¬ì´ˆ)
        self.headless = os.getenv('HEADLESS', 'true').lower() == 'true'

        # Retry ì„¤ì •
        self.max_retries = int(os.getenv('MAX_RETRIES', '3'))  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        self.retry_delay = float(os.getenv('RETRY_DELAY', '5.0'))  # ì¬ì‹œë„ ê°„ê²© (ì´ˆ)

        # DB ì—°ê²° ì„¤ì •
        self.crawl_id = crawl_id  # APIì—ì„œ ì „ë‹¬ë°›ì€ crawl ID
        self.db_conn = None
        self.db_enabled = self._init_db_connection()

        print(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"- ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"- ìš”ì²­ ê°„ê²©: {self.request_delay}ì´ˆ")
        print(f"- í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ: {self.headless}")
        print(f"- íƒ€ì„ì•„ì›ƒ: {self.timeout}ms")
        print(f"- DB ì—°ê²°: {'âœ… í™œì„±í™”' if self.db_enabled else 'âŒ ë¹„í™œì„±í™” (íŒŒì¼ ëª¨ë“œ)'}")
        if self.crawl_id:
            print(f"- Crawl ID: {self.crawl_id}")

    def _init_db_connection(self) -> bool:
        """DB ì—°ê²° ì´ˆê¸°í™”"""
        try:
            database_url = os.getenv('DATABASE_URL')
            if not database_url:
                print("[WARNING] DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì¼ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
                return False

            # Docker ë‚´ë¶€ì—ì„œëŠ” 'db' í˜¸ìŠ¤íŠ¸ ì‚¬ìš©
            # DATABASE_URLì´ localhostë¡œ ì‹œì‘í•˜ë©´ docker ë‚´ë¶€ì´ë¯€ë¡œ dbë¡œ ë³€ê²½
            if 'localhost' in database_url or '127.0.0.1' in database_url:
                # Docker ë‚´ë¶€ í™˜ê²½ ê°ì§€
                if os.path.exists('/.dockerenv'):
                    database_url = database_url.replace('localhost', 'db').replace('127.0.0.1', 'db')
                    print(f"[DB] Docker í™˜ê²½ ê°ì§€ - í˜¸ìŠ¤íŠ¸ë¥¼ 'db'ë¡œ ë³€ê²½")

            self.db_conn = psycopg2.connect(database_url)
            print(f"[DB] PostgreSQL ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            print(f"[WARNING] DB ì—°ê²° ì‹¤íŒ¨: {e}")
            print("[WARNING] íŒŒì¼ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
            return False

    def _close_db_connection(self):
        """DB ì—°ê²° ì¢…ë£Œ"""
        if self.db_conn:
            try:
                self.db_conn.close()
                print("[DB] ì—°ê²° ì¢…ë£Œ")
            except Exception as e:
                print(f"[WARNING] DB ì—°ê²° ì¢…ë£Œ ì‹¤íŒ¨: {e}")
    
    def update_status(self, status: str, progress: int, total: int, current_complex: str = "", message: str = "", items_collected: int = 0):
        """ì§„í–‰ ìƒíƒœë¥¼ DB ë° íŒŒì¼ì— ì €ì¥"""
        # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
        elapsed_seconds = 0
        speed = 0.0
        estimated_total_seconds = 0

        if self.start_time:
            elapsed_seconds = int((get_kst_now() - self.start_time).total_seconds())

            # ì†ë„ ê³„ì‚° (ë§¤ë¬¼/ì´ˆ)
            if elapsed_seconds > 0 and items_collected > 0:
                speed = round(items_collected / elapsed_seconds, 2)

            # ì˜ˆìƒ ì´ ì†Œìš” ì‹œê°„ ê³„ì‚° (ë‹¨ì§€ ê¸°ì¤€)
            if progress > 0 and total > 0:
                avg_time_per_complex = elapsed_seconds / progress
                estimated_total_seconds = int(avg_time_per_complex * total)

        status_data = {
            "status": status,  # "running", "completed", "error"
            "progress": progress,
            "total": total,
            "percent": round((progress / total * 100) if total > 0 else 0, 1),
            "current_complex": current_complex,
            "message": message,
            "timestamp": get_kst_now().isoformat(),
            # ì‹œê°„ ì •ë³´
            "elapsed_seconds": elapsed_seconds,
            "estimated_total_seconds": estimated_total_seconds,
            # ì†ë„ ì •ë³´
            "items_collected": items_collected,
            "speed": speed  # ë§¤ë¬¼/ì´ˆ
        }

        # 1. íŒŒì¼ì— ì €ì¥ (ë°±ì—…ìš©, ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        if self.status_file:
            try:
                with open(self.status_file, 'w', encoding='utf-8') as f:
                    json.dump(status_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"[WARNING] ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

        # 2. DBì— ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ ë°©ì‹)
        if self.db_enabled and self.crawl_id:
            try:
                cursor = self.db_conn.cursor()

                # CrawlHistory í…Œì´ë¸” ì—…ë°ì´íŠ¸
                # status: 'crawling' | 'saving' | 'success' | 'partial' | 'failed'
                db_status = 'crawling' if status == 'running' else status
                if status == 'completed':
                    db_status = 'success'
                elif status == 'error':
                    db_status = 'failed'

                cursor.execute("""
                    UPDATE crawl_history
                    SET current_step = %s,
                        processed_complexes = %s,
                        processed_articles = %s,
                        duration = %s,
                        status = %s,
                        updated_at = NOW()
                    WHERE id = %s
                """, (
                    message,
                    progress,
                    items_collected,
                    elapsed_seconds,
                    db_status,
                    self.crawl_id
                ))

                self.db_conn.commit()
                cursor.close()

                # ë””ë²„ê·¸ ë¡œê·¸ (ë„ˆë¬´ ìì£¼ ì¶œë ¥í•˜ì§€ ì•Šë„ë¡ 10% ë‹¨ìœ„ë¡œë§Œ)
                if progress % max(1, total // 10) == 0 or status in ['completed', 'error']:
                    print(f"[DB] ìƒíƒœ ì—…ë°ì´íŠ¸: {message} ({progress}/{total}, {items_collected}ê°œ ë§¤ë¬¼)")

            except Exception as e:
                print(f"[WARNING] DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                # DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ëŠ” í¬ë¡¤ë§ ì¤‘ë‹¨ ì‚¬ìœ ê°€ ì•„ë‹ˆë¯€ë¡œ ê³„ì† ì§„í–‰

    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™”"""
        try:
            import time

            # 1. Playwright ì‹œì‘
            start = time.time()
            playwright = await async_playwright().start()
            print(f"â±ï¸  Playwright ì‹œì‘: {time.time() - start:.2f}ì´ˆ")

            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (NAS í™˜ê²½ì— ìµœì í™”)
            browser_options = {
                'headless': self.headless,
                'args': [
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',  # /dev/shm ì‚¬ìš© ì•ˆ í•¨ (NAS ë©”ëª¨ë¦¬ ì ˆì•½)
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-software-rasterizer',  # GPU ì†Œí”„íŠ¸ì›¨ì–´ ë Œë”ë§ ë¹„í™œì„±í™”
                    '--disable-extensions',  # í™•ì¥ í”„ë¡œê·¸ë¨ ë¹„í™œì„±í™”
                    '--disable-background-networking',  # ë°±ê·¸ë¼ìš´ë“œ ë„¤íŠ¸ì›Œí‚¹ ë¹„í™œì„±í™”
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-breakpad',  # í¬ë˜ì‹œ ë¦¬í¬íŠ¸ ë¹„í™œì„±í™”
                    '--disable-component-extensions-with-background-pages',
                    '--disable-ipc-flooding-protection',
                    '--disable-renderer-backgrounding',
                    '--memory-pressure-off',
                    '--js-flags=--max-old-space-size=512'  # V8 í™ í¬ê¸° 512MBë¡œ ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)
                ]
            }

            # 2. Chrome ë¸Œë¼ìš°ì € ì‹¤í–‰
            start = time.time()
            self.browser = await playwright.chromium.launch(**browser_options)
            print(f"â±ï¸  Chromium ì‹¤í–‰: {time.time() - start:.2f}ì´ˆ")

            # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¿ í‚¤, ì„¸ì…˜ ê´€ë¦¬)
            start = time.time()
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                extra_http_headers={
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                }
            )
            print(f"â±ï¸  ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {time.time() - start:.2f}ì´ˆ")

            # 4. í˜ì´ì§€ ìƒì„±
            start = time.time()
            self.page = await self.context.new_page()
            print(f"â±ï¸  í˜ì´ì§€ ìƒì„±: {time.time() - start:.2f}ì´ˆ")

            # 5. íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.page.set_default_timeout(self.timeout)

            print("âœ… ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ë° DB ì—°ê²° ì¢…ë£Œ"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            print("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

        # DB ì—°ê²° ì¢…ë£Œ
        self._close_db_connection()

    async def fetch_complex_info_only(self, complex_no: str) -> Optional[Dict]:
        """ë‹¨ì§€ ê¸°ë³¸ ì •ë³´ë§Œ ê°€ì ¸ì˜¤ê¸° (ë§¤ë¬¼ í¬ë¡¤ë§ ì—†ì´)"""
        try:
            print(f"[INFO-ONLY] ë‹¨ì§€ ì •ë³´ ì¡°íšŒ ì‹œì‘: {complex_no}", flush=True)

            # ë„¤ì´ë²„ ë¶€ë™ì‚° ë‹¨ì§€ í˜ì´ì§€ ì ‘ì†
            url = f"https://new.land.naver.com/complexes/{complex_no}"
            print(f"[INFO-ONLY] í˜ì´ì§€ ì´ë™ ì¤‘: {url}", flush=True)

            # ë„¤íŠ¸ì›Œí¬ê°€ ì•ˆì •ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            await self.page.goto(url, wait_until='networkidle', timeout=30000)
            print(f"[INFO-ONLY] í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ", flush=True)

            # ì¶”ê°€ ëŒ€ê¸° (JavaScript ì‹¤í–‰ ì™„ë£Œ)
            await asyncio.sleep(2)

            # API ì‘ë‹µ ë°ì´í„°ë¥¼ ì§ì ‘ fetchë¡œ ê°€ì ¸ì˜¤ê¸°
            overview_data = None
            try:
                print(f"[INFO-ONLY] API ì§ì ‘ í˜¸ì¶œ ì‹œë„...", flush=True)
                api_url = f"https://new.land.naver.com/api/complexes/overview/{complex_no}?complexNo={complex_no}"

                # í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ fetch ì‹¤í–‰
                response = await self.page.evaluate(f'''
                    async () => {{
                        const response = await fetch('{api_url}');
                        return await response.json();
                    }}
                ''')

                if response:
                    overview_data = response
                    print(f"[INFO-ONLY] API ì§ì ‘ í˜¸ì¶œ ì„±ê³µ: {response.get('complexName', 'Unknown')}", flush=True)
            except Exception as e:
                print(f"[INFO-ONLY] API ì§ì ‘ í˜¸ì¶œ ì‹¤íŒ¨: {e}", flush=True)

            if overview_data:
                print(f"[INFO-ONLY] âœ… ë‹¨ì§€ ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ: {overview_data.get('complexName', 'Unknown')}", flush=True)
                return {
                    'complexNo': complex_no,
                    'complexName': overview_data.get('complexName'),
                    'totalHousehold': overview_data.get('totalHouseholdCount'),
                    'totalDong': overview_data.get('totalDongCount'),
                    'address': overview_data.get('address'),
                    'roadAddress': overview_data.get('roadAddress'),
                }
            else:
                print(f"[INFO-ONLY] âš ï¸ ë‹¨ì§€ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨", flush=True)
                return None

        except Exception as e:
            print(f"[INFO-ONLY] ë‹¨ì§€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}", flush=True)
            import traceback
            traceback.print_exc()
            return None

    async def recreate_page(self):
        """í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì¬ìƒì„± (ì—ëŸ¬ ë³µêµ¬ìš©)"""
        try:
            if self.page:
                await self.page.close()
        except:
            pass

        # ìƒˆ í˜ì´ì§€ ìƒì„±
        self.page = await self.context.new_page()
        print("ğŸ”„ í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì¬ìƒì„± ì™„ë£Œ")

    async def validate_complex_exists(self, complex_no: str) -> bool:
        """ë‹¨ì§€ ë²ˆí˜¸ê°€ ìœ íš¨í•œì§€ ê°„ë‹¨íˆ í™•ì¸ (ì»¨í…ìŠ¤íŠ¸ ì—ëŸ¬ ë³µêµ¬ í¬í•¨)"""
        max_attempts = 2

        for attempt in range(1, max_attempts + 1):
            try:
                url = f"https://new.land.naver.com/complexes/{complex_no}"
                # wait_until='commit'ìœ¼ë¡œ ë³€ê²½: ë„¤íŠ¸ì›Œí¬ ì‘ë‹µë§Œ ê¸°ë‹¤ë¦¼ (ë” ë¹ ë¦„)
                # domcontentloadedëŠ” SPAì—ì„œ íƒ€ì„ì•„ì›ƒ ë°œìƒ ê°€ëŠ¥
                response = await self.page.goto(url, wait_until='commit', timeout=self.timeout)

                # ì ì‹œ ëŒ€ê¸° (í˜ì´ì§€ ì´ˆê¸° ë Œë”ë§)
                await asyncio.sleep(1)

                # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
                if response and response.status >= 400:
                    print(f"âŒ ë‹¨ì§€ {complex_no}: HTTP {response.status} - ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€")
                    return False

                # í˜ì´ì§€ íƒ€ì´í‹€ í™•ì¸ (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)
                try:
                    title = await self.page.title()
                    if 'ì˜¤ë¥˜' in title or 'error' in title.lower() or 'not found' in title.lower():
                        print(f"âŒ ë‹¨ì§€ {complex_no}: í˜ì´ì§€ ì˜¤ë¥˜ - {title}")
                        return False
                except Exception as title_error:
                    print(f"âš ï¸ íƒ€ì´í‹€ í™•ì¸ ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ê³„ì†: {title_error}")

                print(f"âœ… ë‹¨ì§€ {complex_no} ìœ íš¨ì„± í™•ì¸ ì™„ë£Œ")
                return True

            except Exception as e:
                error_msg = str(e)
                if "Execution context was destroyed" in error_msg or "Target page" in error_msg:
                    if attempt < max_attempts:
                        print(f"âš ï¸ í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì—ëŸ¬ ë°œìƒ, ì¬ìƒì„± í›„ ì¬ì‹œë„ ({attempt}/{max_attempts})")
                        await self.recreate_page()
                        await asyncio.sleep(2)
                        continue

                print(f"âŒ ë‹¨ì§€ {complex_no} ê²€ì¦ ì‹¤íŒ¨: {e}")
                return False

        return False

    async def crawl_complex_overview_with_retry(self, complex_no: str) -> Optional[Dict]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë‹¨ì§€ ê°œìš” í¬ë¡¤ë§"""
        for attempt in range(1, self.max_retries + 1):
            try:
                if attempt > 1:
                    print(f"[ì¬ì‹œë„ {attempt}/{self.max_retries}] ë‹¨ì§€ ê°œìš” í¬ë¡¤ë§ ì‹œì‘: {complex_no}")

                overview_data = await self.crawl_complex_overview(complex_no)

                if overview_data:
                    return overview_data

                # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¬ì‹œë„
                if attempt < self.max_retries:
                    wait_time = self.retry_delay * (2 ** (attempt - 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„: 5s, 10s, 20s
                    print(f"â³ Overview ë°ì´í„° ì—†ìŒ. {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(wait_time)

            except Exception as e:
                print(f"âŒ ì‹œë„ {attempt}/{self.max_retries} ì‹¤íŒ¨: {e}")

                if attempt < self.max_retries:
                    wait_time = self.retry_delay * (2 ** (attempt - 1))
                    print(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"ğŸš« ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ {self.max_retries}íšŒ ë„ë‹¬, í¬ê¸°")
                    import traceback
                    traceback.print_exc()

        return None

    async def crawl_complex_overview(self, complex_no: str) -> Optional[Dict]:
        """ë‹¨ì§€ ê°œìš” ì •ë³´ í¬ë¡¤ë§ (ëª…ì‹œì  API ëŒ€ê¸° ë°©ì‹)"""
        try:
            print(f"ë‹¨ì§€ ê°œìš” ì •ë³´ í¬ë¡¤ë§ ì‹œì‘: {complex_no}")

            # ë„¤ì´ë²„ ë¶€ë™ì‚° ë‹¨ì§€ í˜ì´ì§€ ì ‘ì†
            url = f"https://new.land.naver.com/complexes/{complex_no}"
            overview_data = None

            try:
                try:
                    # ëª…ì‹œì ìœ¼ë¡œ Overview API ì‘ë‹µì„ ê¸°ë‹¤ë¦¼ (ìµœëŒ€ 30ì´ˆ)
                    print(f"[ëŒ€ê¸°] Overview API ì‘ë‹µ ëŒ€ê¸° ì¤‘...")
                    async with self.page.expect_response(
                        lambda response: f'/api/complexes/overview/{complex_no}' in response.url,
                        timeout=self.timeout
                    ) as response_info:
                        # í˜ì´ì§€ ì ‘ì†ê³¼ ë™ì‹œì— API ì‘ë‹µ ëŒ€ê¸°
                        await self.page.goto(url, wait_until='commit', timeout=self.timeout)

                    # API ì‘ë‹µ ë°›ê¸°
                    response = await response_info.value

                    # Protocol Error ì¬ì‹œë„ ë¡œì§
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            overview_data = await response.json()
                            print(f"âœ… ë‹¨ì§€ ê°œìš” API ì‘ë‹µ ìˆ˜ì‹  ì„±ê³µ: {overview_data.get('complexName', 'Unknown')}")
                            break
                        except Exception as e:
                            if attempt < max_retries - 1:
                                print(f"API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                                await asyncio.sleep(0.5)
                            else:
                                print(f"API ì‘ë‹µ íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {e}")
                                raise

                except Exception as api_wait_error:
                    # API ì‘ë‹µ ëŒ€ê¸° ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ ë“±)
                    print(f"âš ï¸ Overview API ì‘ë‹µ ëŒ€ê¸° ì‹¤íŒ¨: {api_wait_error}")

                    # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹œë„
                    screenshot_path = self.output_dir / f"api_timeout_{complex_no}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                    try:
                        await self.page.screenshot(path=str(screenshot_path), full_page=True, timeout=5000)
                        print(f"ğŸ“¸ íƒ€ì„ì•„ì›ƒ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
                    except Exception as ss_error:
                        print(f"[WARNING] ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {ss_error}")

                    # URL í™•ì¸
                    current_url = self.page.url
                    print(f"í˜„ì¬ URL: {current_url}")

                    # ë´‡ íƒì§€ íŒ¨í„´ ë¶„ì„
                    if '/404' in current_url:
                        print(f"âš ï¸ 404 í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€! {url} â†’ {current_url}")
                    elif f'/complexes/{complex_no}' not in current_url:
                        print(f"âš ï¸ ë´‡ íƒì§€ë¡œ ì¸í•œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€! {url} â†’ {current_url}")
                        print(f"   ë‹¨ì§€ IDê°€ URLì—ì„œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    elif current_url.startswith(f'https://new.land.naver.com/complexes/{complex_no}'):
                        if '?' in current_url:
                            print(f"âœ… URLì€ ì •ìƒì´ë‚˜ API ì‘ë‹µ ì—†ìŒ: {current_url}")
                        else:
                            print(f"âœ… URLì€ ì •ìƒì´ë‚˜ API ì‘ë‹µ ì—†ìŒ: {current_url}")

            except Exception as e:
                print(f"ë‹¨ì§€ ê°œìš” í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                raise

            if overview_data:
                print(f"âœ… Overview ìˆ˜ì§‘ ì„±ê³µ: {overview_data.get('complexName', 'Unknown')}")
                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                result = {
                    # ê¸°ë³¸ ì •ë³´
                    'complexName': overview_data.get('complexName', ''),
                    'complexType': overview_data.get('complexTypeName', ''),
                    'complexNo': overview_data.get('complexNo', ''),
                    'totalHousehold': overview_data.get('totalHouseHoldCount'),
                    'totalDong': overview_data.get('totalDongCount'),
                    'useApproveYmd': overview_data.get('useApproveYmd', ''),

                    # ì¢Œí‘œ ì •ë³´
                    'latitude': overview_data.get('latitude'),
                    'longitude': overview_data.get('longitude'),

                    # ë©´ì  ì •ë³´
                    'minArea': overview_data.get('minArea'),
                    'maxArea': overview_data.get('maxArea'),

                    # ê°€ê²© ì •ë³´
                    'minPrice': overview_data.get('minPrice'),
                    'maxPrice': overview_data.get('maxPrice'),
                    'minPriceByLetter': overview_data.get('minPriceByLetter', ''),
                    'maxPriceByLetter': overview_data.get('maxPriceByLetter', ''),
                    'minLeasePrice': overview_data.get('minLeasePrice'),
                    'maxLeasePrice': overview_data.get('maxLeasePrice'),
                    'minLeasePriceByLetter': overview_data.get('minLeasePriceByLetter', ''),
                    'maxLeasePriceByLetter': overview_data.get('maxLeasePriceByLetter', ''),

                    # ìµœê·¼ ì‹¤ê±°ë˜ê°€
                    'realPrice': overview_data.get('realPrice'),

                    # í‰í˜• ì •ë³´
                    'pyeongs': overview_data.get('pyeongs', []),

                    # ë™ ì •ë³´
                    'dongs': overview_data.get('dongs', []),
                }
                return result
            else:
                print(f"âš ï¸ Overview ìˆ˜ì§‘ ì‹¤íŒ¨")
                return None

        except Exception as e:
            print(f"ë‹¨ì§€ ê°œìš” í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def crawl_complex_articles_with_scroll(self, complex_no: str) -> Optional[Dict]:
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§"""
        try:
            print(f"ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§ ì‹œì‘ (ë¬´í•œ ìŠ¤í¬ë¡¤): {complex_no}")
            
            # ëª¨ë“  ë§¤ë¬¼ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            all_articles = []
            collected_article_ids = set()  # ì¤‘ë³µ ì œê±°ìš©
            last_api_time = [0]  # API ë§ˆì§€ë§‰ ê°ì§€ ì‹œê°„ (ë¦¬ìŠ¤íŠ¸ë¡œ í´ë¡œì € íšŒí”¼)

            # API ì‘ë‹µ ìˆ˜ì§‘
            async def handle_articles_response(response):
                # ë§¤ë¬¼ ëª©ë¡ API ì‘ë‹µ ê°ì§€
                if f'/api/articles/complex/{complex_no}' in response.url:
                    # API ê°ì§€ ì‹œê°„ ê¸°ë¡
                    last_api_time[0] = time.time()

                    # ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì ìš© ì—¬ë¶€ í™•ì¸
                    same_group = 'sameAddressGroup=true' in response.url or 'sameAddressGroup=Y' in response.url
                    group_status = "âœ… ON" if same_group else "âŒ OFF"

                    print(f"[API] í˜¸ì¶œ ê°ì§€ #{len(all_articles)//20 + 1} (ë™ì¼ë§¤ë¬¼ë¬¶ê¸°: {group_status})")
                    if len(all_articles) == 0:  # ì²« API í˜¸ì¶œë§Œ ì „ì²´ URL ë¡œê·¸
                        print(f"[API] URL: {response.url[:120]}...")

                    # Protocol Error ì¬ì‹œë„ ë¡œì§
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            data = await response.json()
                            article_list = data.get('articleList', [])
                            total_count = data.get('totalCount', 0)

                            # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                            new_count = 0
                            for article in article_list:
                                article_id = article.get('articleNo') or article.get('id')
                                if article_id and article_id not in collected_article_ids:
                                    collected_article_ids.add(article_id)
                                    all_articles.append(article)
                                    new_count += 1

                            if new_count > 0:
                                total_info = f", ì „ì²´: {total_count}ê±´" if total_count > 0 else ""
                                print(f"  â†’ {new_count}ê°œ ìƒˆ ë§¤ë¬¼ ì¶”ê°€ (ì´ {len(all_articles)}ê°œ{total_info})")
                            break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                        except Exception as e:
                            if attempt < max_retries - 1:
                                print(f"ë§¤ë¬¼ API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                                await asyncio.sleep(0.5)  # 0.5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                            else:
                                print(f"ë§¤ë¬¼ API ì‘ë‹µ íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {e}")
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ë“±ë¡
            self.page.on('response', handle_articles_response)

            try:
                # 1. ë©”ì¸ í˜ì´ì§€ì—ì„œ localStorage ì„¤ì • (ì¤‘ìš”!)
                print("ğŸ”§ ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì„¤ì • ì¤€ë¹„ ì¤‘...")
                await self.page.goto("https://new.land.naver.com", wait_until='domcontentloaded')
                
                await self.page.evaluate('''
                    () => {
                        localStorage.setItem('sameAddrYn', 'true');
                        localStorage.setItem('sameAddressGroup', 'true');
                        console.log('[LocalStorage] ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì„¤ì • ì™„ë£Œ');
                    }
                ''')
                print("âœ… localStorage ì„¤ì • ì™„ë£Œ")
                await asyncio.sleep(1)
                
                # 2. ë‹¨ì§€ í˜ì´ì§€ë¡œ ì´ë™ (localStorage ê°’ì´ ìë™ ì ìš©ë¨)
                url = f"https://new.land.naver.com/complexes/{complex_no}"
                print(f"URL ì ‘ì†: {url}")
                await self.page.goto(url, wait_until='domcontentloaded', timeout=30000)  # Increased to 30s
                await asyncio.sleep(2)
                
                # 2. ë§¤ë¬¼ íƒ­ í´ë¦­
                print("ë§¤ë¬¼ íƒ­ ì°¾ëŠ” ì¤‘...")
                try:
                    selectors = [
                        'a[href*="article"]',
                        'button:has-text("ë§¤ë¬¼")',
                        '[class*="article"]',
                    ]
                    
                    for selector in selectors:
                        try:
                            element = await self.page.wait_for_selector(selector, timeout=5000)
                            if element:
                                await element.click()
                                print(f"ë§¤ë¬¼ íƒ­ í´ë¦­ ì„±ê³µ")
                                await asyncio.sleep(3)
                                break
                        except:
                            continue
                except Exception as e:
                    print(f"ë§¤ë¬¼ íƒ­ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # 3. localStorage ë° ì²´í¬ë°•ìŠ¤ ìƒíƒœ ê²€ì¦
                print("ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ìƒíƒœ ê²€ì¦ ì¤‘...")
                storage_check = await self.page.evaluate('''
                    () => {
                        const sameAddrYn = localStorage.getItem('sameAddrYn');
                        const sameAddressGroup = localStorage.getItem('sameAddressGroup');
                        
                        // ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸
                        const checkboxes = document.querySelectorAll('input[type="checkbox"]');
                        let checkboxState = null;
                        
                        for (const checkbox of checkboxes) {
                            const label = checkbox.closest('label') || checkbox.nextElementSibling;
                            const text = label ? (label.textContent || label.innerText || '') : '';
                            if (text.includes('ë™ì¼ë§¤ë¬¼')) {
                                checkboxState = {
                                    checked: checkbox.checked,
                                    labelText: text
                                };
                                break;
                            }
                        }
                        
                        return {
                            sameAddrYn,
                            sameAddressGroup,
                            checkboxState
                        };
                    }
                ''')
                print(f"[DEBUG] localStorage ìƒíƒœ: sameAddrYn={storage_check.get('sameAddrYn')}, sameAddressGroup={storage_check.get('sameAddressGroup')}")
                if storage_check.get('checkboxState'):
                    print(f"[DEBUG] ì²´í¬ë°•ìŠ¤ ìƒíƒœ: checked={storage_check['checkboxState'].get('checked')}")
                
                # 4. ì²´í¬ë°•ìŠ¤ê°€ ì²´í¬ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í´ë¦­
                if storage_check.get('checkboxState') and not storage_check['checkboxState'].get('checked'):
                    print("ğŸ”˜ ì²´í¬ë°•ìŠ¤ í´ë¦­ ì¤‘...")
                    clicked = await self.page.evaluate('''
                        () => {
                            const checkboxes = document.querySelectorAll('input[type="checkbox"]');
                            for (const checkbox of checkboxes) {
                                const label = checkbox.closest('label') || checkbox.nextElementSibling;
                                const text = label ? (label.textContent || label.innerText || '') : '';
                                if (text.includes('ë™ì¼ë§¤ë¬¼')) {
                                    checkbox.click();
                                    console.log('[Checkbox] í´ë¦­ ì™„ë£Œ');
                                    return true;
                                }
                            }
                            return false;
                        }
                    ''')
                    
                    if clicked:
                        print("[DEBUG] ì²´í¬ë°•ìŠ¤ í´ë¦­ ì™„ë£Œ, ë°ì´í„° ì¬ë¡œë”© ëŒ€ê¸°...")
                        await asyncio.sleep(3)
                        print("âœ… ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° í™œì„±í™” ì™„ë£Œ")
                    else:
                        print("[DEBUG] ì²´í¬ë°•ìŠ¤ë¥¼ ì°¾ì§€ ëª»í•¨")
                else:
                    print("âœ… ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° ì´ë¯¸ í™œì„±í™”ë¨")
                
                # 5. ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                print("ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì°¾ëŠ” ì¤‘...")
                list_container = None
                # ë” êµ¬ì²´ì ì¸ ì…€ë ‰í„°ë¶€í„° ì‹œë„ (ì¼ë°˜ì ì¸ ê²ƒì€ ë‚˜ì¤‘ì—)
                container_selectors = [
                    '.item_list--article',  # ê°€ì¥ êµ¬ì²´ì 
                    '[class*="article"]',   # ë§¤ë¬¼ ê´€ë ¨
                    '.item_list',           # ì¼ë°˜ ë¦¬ìŠ¤íŠ¸
                    '[class*="list"]',      # ê°€ì¥ ì¼ë°˜ì  (ë§ˆì§€ë§‰)
                ]

                # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
                container_retry_count = 0
                max_container_retries = 3

                while not list_container and container_retry_count < max_container_retries:
                    if container_retry_count > 0:
                        print(f"âš ï¸  ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„ ({container_retry_count}/{max_container_retries})...")
                        await self.page.reload(wait_until='domcontentloaded', timeout=30000)
                        await asyncio.sleep(3)

                        # ë§¤ë¬¼ íƒ­ ë‹¤ì‹œ í´ë¦­
                        try:
                            tab_selectors = [
                                'a[href*="article"]',
                                'button:has-text("ë§¤ë¬¼")',
                                '[class*="article"]',
                            ]

                            for tab_selector in tab_selectors:
                                try:
                                    element = await self.page.wait_for_selector(tab_selector, timeout=5000)
                                    if element:
                                        await element.click()
                                        print(f"ë§¤ë¬¼ íƒ­ ë‹¤ì‹œ í´ë¦­ ì„±ê³µ")
                                        await asyncio.sleep(2)
                                        break
                                except:
                                    continue
                        except Exception as e:
                            print(f"ë§¤ë¬¼ íƒ­ ì¬í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")

                    for selector in container_selectors:
                        try:
                            list_container = await self.page.wait_for_selector(selector, timeout=5000)
                            if list_container:
                                print(f"âœ… ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                                break
                        except:
                            continue

                    if not list_container:
                        container_retry_count += 1

                if not list_container:
                    print(f"âŒ {max_container_retries}íšŒ ì¬ì‹œë„ í›„ì—ë„ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    print(f"   â†’ ì´ ë‹¨ì§€ëŠ” ë§¤ë¬¼ì´ ì—†ê±°ë‚˜, ë„¤ì´ë²„ í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    return None
                
                # 4. ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸°
                await asyncio.sleep(3)
                initial_count = len(all_articles)
                print(f"ì´ˆê¸° ë§¤ë¬¼ ìˆ˜: {initial_count}ê°œ")

                # ì´ˆê¸° ë§¤ë¬¼ì´ 0ê°œì¸ ê²½ìš° ì¶”ê°€ ëŒ€ê¸° (API ì‘ë‹µ ëŒ€ê¸°)
                if initial_count == 0:
                    print("âš ï¸ ì´ˆê¸° ë§¤ë¬¼ì´ 0ê°œì…ë‹ˆë‹¤. API ì‘ë‹µ ëŒ€ê¸° ì¤‘... (5ì´ˆ)")
                    await asyncio.sleep(5)
                    initial_count = len(all_articles)
                    print(f"ì¬í™•ì¸ ë§¤ë¬¼ ìˆ˜: {initial_count}ê°œ")

                    if initial_count == 0:
                        print("âŒ ì—¬ì „íˆ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. ì´ ë‹¨ì§€ëŠ” ë§¤ë¬¼ì´ ì—†ê±°ë‚˜ í˜ì´ì§€ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        return None
                
                # 5. ì ì§„ì  ìŠ¤í¬ë¡¤ë¡œ ë°ì´í„° ìˆ˜ì§‘ (crawler_service.py ë°©ì‹)
                print("ì¶”ê°€ ë§¤ë¬¼ ìˆ˜ì§‘ ì‹œì‘ (ì ì§„ì  ìŠ¤í¬ë¡¤)...")
                print(f"[ì„¤ì •] ìµœëŒ€ ì‹œë„: 100íšŒ, ìŠ¤í¬ë¡¤: 800px, ë™ì  ëŒ€ê¸°(APIê°ì§€:0.3ì´ˆ/ë¯¸ê°ì§€:1.0ì´ˆ), ì¢…ë£Œ: 3íšŒ ì—°ì† ë³€í™” ì—†ìŒ")
                scroll_attempts = 0
                max_scroll_attempts = 100  # ìµœëŒ€ 100íšŒ
                scroll_end_count = 0  # ìŠ¤í¬ë¡¤ì´ ì•ˆ ì›€ì§ì´ëŠ” íšŸìˆ˜
                max_scroll_end = 3  # 3íšŒ ì—°ì† ìŠ¤í¬ë¡¤ ì•ˆ ë˜ë©´ ì¢…ë£Œ (ì†ë„ ê°œì„ )
                
                while scroll_attempts < max_scroll_attempts:
                    prev_count = len(all_articles)
                    
                    # ë§¤ë¬¼ ìŠ¤í¬ë¡¤ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if scroll_attempts % 3 == 0:  # 3íšŒë§ˆë‹¤ ì—…ë°ì´íŠ¸ (ë„ˆë¬´ ìì£¼ ì—…ë°ì´íŠ¸í•˜ë©´ ë¶€í•˜)
                        self.update_status(
                            status="running",
                            progress=len(all_articles),
                            total=100,  # ì˜ˆìƒ ì´ ë§¤ë¬¼ ìˆ˜ (ì‹¤ì œëŠ” ì•Œ ìˆ˜ ì—†ìŒ)
                            current_complex=complex_no,
                            message=f"ğŸ”„ ë§¤ë¬¼ ìŠ¤í¬ë¡¤ ì¤‘... (ì‹œë„ {scroll_attempts}íšŒ, ìˆ˜ì§‘ {len(all_articles)}ê°œ)",
                            items_collected=len(all_articles)
                        )
                    
                    # ë„¤ì´ë²„ ì‹¤ì œ ì»¨í…Œì´ë„ˆë¡œ ìŠ¤í¬ë¡¤ (800px ê³ ì •)
                    scroll_result = await self.page.evaluate('''
                        () => {
                            // ë„¤ì´ë²„ê°€ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ì…€ë ‰í„°ë“¤
                            const selectors = [
                                '.item_list',  // âœ… crawler_service.pyì—ì„œ ì‚¬ìš©
                                'div[class*="list_contents"]',
                                'div[class*="item_list"]',
                                'div[class*="article_list"]'
                            ];

                            let container = null;
                            for (const selector of selectors) {
                                container = document.querySelector(selector);
                                if (container && container.scrollHeight > container.clientHeight) {
                                    break;
                                }
                            }

                            if (!container) {
                                return { found: false, reason: 'container not found' };
                            }

                            // ì ì§„ì  ìŠ¤í¬ë¡¤ (800pxì”© - ì´ì „ 500pxì—ì„œ ì¦ê°€)
                            const before = container.scrollTop;
                            container.scrollTop += 800;  // âœ… í•œ ë²ˆì— ëê¹Œì§€ ê°€ì§€ ì•ŠìŒ
                            const after = container.scrollTop;

                            const items = container.querySelectorAll('.item_link, .item_inner, [class*="item"]');

                            return {
                                found: true,
                                moved: after > before,  // âœ… ì‹¤ì œë¡œ ìŠ¤í¬ë¡¤ë˜ì—ˆëŠ”ì§€
                                scrollBefore: before,
                                scrollAfter: after,
                                scrollDelta: after - before,
                                scrollHeight: container.scrollHeight,
                                clientHeight: container.clientHeight,
                                itemCount: items.length,
                                containerClass: container.className
                            };
                        }
                    ''')
                        
                    if scroll_attempts == 0:
                        if scroll_result.get('found'):
                            print(f"[DEBUG] ì»¨í…Œì´ë„ˆ ë°œê²¬: .{scroll_result.get('containerClass', 'unknown')}")
                            print(f"  DOM ì•„ì´í…œ: {scroll_result.get('itemCount', 0)}ê°œ (ë™ì¼ë§¤ë¬¼ë¬¶ê¸° ì´ì „, ì°¸ê³ ìš©)")
                            print(f"  ìŠ¤í¬ë¡¤ ë†’ì´: {scroll_result.get('scrollHeight')} / {scroll_result.get('clientHeight')}")
                            print(f"  ğŸ’¡ ì‹¤ì œ ìˆ˜ì§‘ ê°œìˆ˜ëŠ” API ì‘ë‹µ ê¸°ì¤€ (ë™ì¼ë§¤ë¬¼ë¬¶ê¸° ì´í›„)")
                        else:
                            print(f"[DEBUG] ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í•¨: {scroll_result.get('reason', 'unknown')}")

                    # ë™ì  ëŒ€ê¸° ì‹œê°„ (API ê°ì§€ ì—¬ë¶€ì— ë”°ë¼)
                    time_since_last_api = time.time() - last_api_time[0]

                    if time_since_last_api < 0.5:  # ìµœê·¼ 0.5ì´ˆ ì´ë‚´ì— API ê°ì§€ë¨
                        wait_time = 0.3
                        # print(f"  [ëŒ€ê¸°] API ìµœê·¼ ê°ì§€ â†’ {wait_time}ì´ˆ ëŒ€ê¸°")
                    else:  # API ê°ì§€ ì•ˆë¨
                        wait_time = 1.0
                        # print(f"  [ëŒ€ê¸°] API ë¯¸ê°ì§€ â†’ {wait_time}ì´ˆ ëŒ€ê¸°")

                    await asyncio.sleep(wait_time)
                    
                    current_count = len(all_articles)
                    new_items = current_count - prev_count
                    
                    scroll_attempts += 1
                    
                    # ì¢…ë£Œ ì¡°ê±´: ìŠ¤í¬ë¡¤ ë + ë°ì´í„° ì¦ê°€ ì—†ìŒ (ë‘˜ ë‹¤ ì¶©ì¡±í•´ì•¼ í•¨)
                    scroll_ended = scroll_result.get('found') and not scroll_result.get('moved')
                    no_new_data = new_items == 0
                    
                    if scroll_ended and no_new_data:
                        scroll_end_count += 1
                        print(f"ì‹œë„ {scroll_attempts}íšŒ: ìŠ¤í¬ë¡¤ ë & ë°ì´í„° ì—†ìŒ ({scroll_end_count}/{max_scroll_end}) - ì´ {current_count}ê°œ")
                        print(f"  â†’ ìŠ¤í¬ë¡¤: {scroll_result.get('scrollAfter')} / {scroll_result.get('scrollHeight')}")
                        
                        if scroll_end_count >= max_scroll_end:
                            print(f"â¹ï¸  ìˆ˜ì§‘ ì¢…ë£Œ ({max_scroll_end}íšŒ ì—°ì† ë³€í™” ì—†ìŒ)")
                            print(f"ğŸ“Š ìµœì¢…: {current_count}ê°œ ìˆ˜ì§‘ (DOM: {scroll_result.get('itemCount', 0)}ê°œëŠ” ë™ì¼ë§¤ë¬¼ë¬¶ê¸° ì´ì „)")
                            break
                    else:
                        # ìŠ¤í¬ë¡¤ì´ ëì´ì–´ë„ ë°ì´í„°ê°€ ì¦ê°€í•˜ë©´ ê³„ì† ì‹œë„
                        if new_items > 0:
                            scroll_end_count = 0  # ë°ì´í„° ì¦ê°€í•˜ë©´ ë¦¬ì…‹
                            print(f"ì‹œë„ {scroll_attempts}íšŒ: +{scroll_result.get('scrollDelta', 0)}px ìŠ¤í¬ë¡¤ â†’ ğŸ‰ {new_items}ê°œ ì¶”ê°€ (ì´ {current_count}ê°œ)")
                        elif scroll_ended:
                            # ìŠ¤í¬ë¡¤ ëì´ì§€ë§Œ ë°ì´í„° ì¦ê°€ ëŒ€ê¸° ì¤‘
                            print(f"ì‹œë„ {scroll_attempts}íšŒ: ìŠ¤í¬ë¡¤ ë ë„ë‹¬, API ì‘ë‹µ ëŒ€ê¸° ì¤‘... (ì´ {current_count}ê°œ)")
                        else:
                            scroll_end_count = 0  # ìŠ¤í¬ë¡¤ ì¤‘ì´ë©´ ë¦¬ì…‹
                            print(f"ì‹œë„ {scroll_attempts}íšŒ: +{scroll_result.get('scrollDelta', 0)}px ìŠ¤í¬ë¡¤ ì¤‘... (ì´ {current_count}ê°œ, ëŒ€ê¸° ì¤‘)")
                
                if len(all_articles) > initial_count:
                    print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ: ì´ˆê¸° {initial_count}ê°œ â†’ ìµœì¢… {len(all_articles)}ê°œ (ì´ {scroll_attempts}íšŒ ì‹œë„)")
                else:
                    print(f"âš ï¸  ì¶”ê°€ ìˆ˜ì§‘ ì‹¤íŒ¨: {initial_count}ê°œì—ì„œ ë³€í™” ì—†ìŒ (ì´ {scroll_attempts}íšŒ ì‹œë„)")
                    print(f"   â†’ ì‹¤ì œë¡œ {initial_count}ê°œë§Œ ìˆê±°ë‚˜, ìŠ¤í¬ë¡¤/ë²„íŠ¼ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ")
                
            except Exception as e:
                error_msg = str(e)
                print(f"ìŠ¤í¬ë¡¤ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")

                # ì—ëŸ¬ ë°œìƒ ì‹œ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (ë³„ë„ íƒ€ì„ì•„ì›ƒ 5ì´ˆ)
                screenshot_path = self.output_dir / f"scroll_error_screenshot_{complex_no}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                try:
                    await self.page.screenshot(path=str(screenshot_path), full_page=True, timeout=5000)
                    print(f"ğŸ–¼ï¸  ì—ëŸ¬ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
                except Exception as ss_error:
                    print(f"[WARNING] ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {ss_error}")

                # ì»¨í…ìŠ¤íŠ¸ íŒŒê´´ ì—ëŸ¬ì¸ ê²½ìš° í˜ì´ì§€ ì¬ìƒì„±
                if "Execution context was destroyed" in error_msg or "Target page" in error_msg:
                    print("âš ï¸ í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì—ëŸ¬ - í˜ì´ì§€ ì¬ìƒì„±")
                    await self.recreate_page()

                # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì´ë¯¸ ìˆ˜ì§‘í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                if all_articles:
                    print(f"âš ï¸  ì—ëŸ¬ ë°œìƒí–ˆì§€ë§Œ {len(all_articles)}ê°œ ë§¤ë¬¼ì€ ìˆ˜ì§‘ ì™„ë£Œ")
            finally:
                # ì‘ë‹µ í•¸ë“¤ëŸ¬ ì œê±° (ì—ëŸ¬ ë°œìƒí•´ë„ ë°˜ë“œì‹œ ì‹¤í–‰)
                try:
                    self.page.remove_listener('response', handle_articles_response)
                    print(f"[DEBUG] Articles í•¸ë“¤ëŸ¬ ì œê±° ì™„ë£Œ")
                except Exception as e:
                    print(f"[WARNING] í•¸ë“¤ëŸ¬ ì œê±° ì‹¤íŒ¨: {e}")

            if all_articles:
                return {
                    'articleList': all_articles,
                    'totalCount': len(all_articles),
                    'isMoreData': False
                }
            else:
                print("âš ï¸  ë§¤ë¬¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            print(f"ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    async def crawl_complex_articles(self, complex_no: str, page_num: int = 1) -> Optional[Dict]:
        """ë‹¨ì§€ ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§"""
        # ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë§¤ë¬¼ ìˆ˜ì§‘
        if page_num == 1:
            return await self.crawl_complex_articles_with_scroll(complex_no)
        else:
            return None

    async def crawl_complex_data(self, complex_no: str) -> Dict:
        """ë‹¨ì§€ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ (ì¬ì‹œë„ ë¡œì§ ë° ì—ëŸ¬ ë³µêµ¬ í¬í•¨)"""
        print(f"\n{'='*60}")
        print(f"ë‹¨ì§€ ë²ˆí˜¸ {complex_no} í¬ë¡¤ë§ ì‹œì‘")

        complex_data = {
            'crawling_info': {
                'complex_no': complex_no,
                'crawling_date': get_kst_now().isoformat(),
                'crawler_version': '1.0.2'  # ì»¨í…ìŠ¤íŠ¸ ë³µêµ¬ ë¡œì§ ì¶”ê°€
            }
        }

        max_attempts = 2  # ì „ì²´ í¬ë¡¤ë§ ì¬ì‹œë„ íšŸìˆ˜

        for attempt in range(1, max_attempts + 1):
            try:
                if attempt > 1:
                    print(f"\nğŸ”„ [{attempt}/{max_attempts}] ë‹¨ì§€ {complex_no} ì¬ì‹œë„")
                    await self.recreate_page()
                    await asyncio.sleep(3)

                # 0. ë‹¨ì§€ ë²ˆí˜¸ ìœ íš¨ì„± ê²€ì‚¬ (ì„ íƒ ì‚¬í•­)
                is_valid = await self.validate_complex_exists(complex_no)
                if not is_valid:
                    print(f"âš ï¸ ë‹¨ì§€ {complex_no}ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"   â†’ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    complex_data['error'] = f'ë‹¨ì§€ {complex_no} ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€'
                    complex_data['skipped'] = True
                    return complex_data

                # 0.5. DBì—ì„œ ê¸°ì¡´ ë‹¨ì§€ í™•ì¸ (Overview ìŠ¤í‚µ íŒë‹¨)
                skip_overview = False
                if self.db_enabled and self.db_conn:
                    try:
                        cursor = self.db_conn.cursor()
                        cursor.execute("""
                            SELECT "complexNo", "complexName", "totalHousehold", "totalDong",
                                   latitude, longitude
                            FROM complexes
                            WHERE "complexNo" = %s
                            LIMIT 1
                        """, (complex_no,))

                        existing_complex = cursor.fetchone()
                        cursor.close()

                        if existing_complex:
                            complexNo, complexName, totalHousehold, totalDong, latitude, longitude = existing_complex
                            print(f"ğŸ’¾ ë‹¨ì§€ {complex_no} ì´ë¯¸ DBì— ì¡´ì¬")
                            print(f"   ë‹¨ì§€ëª…: {complexName}")
                            print(f"   â†’ Overview í¬ë¡¤ë§ ìŠ¤í‚µ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)")
                            skip_overview = True
                            # ê¸°ì¡´ ë°ì´í„°ë¥¼ overviewë¡œ ì‚¬ìš©
                            complex_data['overview'] = {
                                'complexNo': complexNo,
                                'complexName': complexName,
                                'totalHousehold': totalHousehold,
                                'totalDong': totalDong,
                                'latitude': float(latitude) if latitude else None,
                                'longitude': float(longitude) if longitude else None,
                            }
                        else:
                            print(f"ğŸ†• ì‹ ê·œ ë‹¨ì§€ {complex_no} â†’ Overview ìˆ˜ì§‘ í•„ìš”")
                    except Exception as e:
                        print(f"[WARNING] DB ì²´í¬ ì‹¤íŒ¨, Overview ìˆ˜ì§‘ ì§„í–‰: {e}")
                        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì»¤ì„œ ë‹«ê¸° ì‹œë„
                        try:
                            cursor.close()
                        except:
                            pass

                # 1. ë‹¨ì§€ ê°œìš” ì •ë³´ (ì¬ì‹œë„ ë¡œì§ í¬í•¨) - ì‹ ê·œ ë‹¨ì§€ë§Œ
                if not skip_overview:
                    overview = await self.crawl_complex_overview_with_retry(complex_no)
                    if overview:
                        complex_data['overview'] = overview
                else:
                    overview = complex_data.get('overview')  # ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©

                if overview:
                    # ê¸°ë³¸ ì •ë³´
                    complex_name = overview.get('complexName', 'Unknown')
                    complex_type = overview.get('complexType', 'Unknown')
                    total_household = overview.get('totalHousehold', 'Unknown')
                    total_dong = overview.get('totalDong', 'Unknown')
                    use_approve_ymd = overview.get('useApproveYmd', '')

                    print(f"\nğŸ“‹ ë‹¨ì§€ ê¸°ë³¸ ì •ë³´")
                    print(f"  ë‹¨ì§€ëª…: {complex_name}")
                    print(f"  ìœ í˜•: {complex_type}")
                    print(f"  ì„¸ëŒ€ìˆ˜: {total_household}ì„¸ëŒ€")
                    print(f"  ë™ìˆ˜: {total_dong}ê°œë™")
                    if use_approve_ymd:
                        formatted_date = f"{use_approve_ymd[:4]}.{use_approve_ymd[4:6]}.{use_approve_ymd[6:]}"
                        print(f"  ì‚¬ìš©ìŠ¹ì¸ì¼: {formatted_date}")

                    # ì¢Œí‘œ ì •ë³´
                    latitude = overview.get('latitude')
                    longitude = overview.get('longitude')
                    if latitude and longitude:
                        print(f"\nğŸ“ ìœ„ì¹˜ ì •ë³´")
                        print(f"  ì¢Œí‘œ: {latitude}, {longitude}")

                    # ë©´ì  ì •ë³´
                    min_area = overview.get('minArea')
                    max_area = overview.get('maxArea')
                    if min_area and max_area:
                        print(f"\nğŸ“ ë©´ì  ë²”ìœ„")
                        print(f"  {min_area}ã¡ ~ {max_area}ã¡")

                    # ê°€ê²© ì •ë³´
                    min_price_letter = overview.get('minPriceByLetter')
                    max_price_letter = overview.get('maxPriceByLetter')
                    if min_price_letter and max_price_letter:
                        print(f"\nğŸ’° ë§¤ë§¤ê°€ ë²”ìœ„")
                        print(f"  {min_price_letter} ~ {max_price_letter}")

                    min_lease_letter = overview.get('minLeasePriceByLetter')
                    max_lease_letter = overview.get('maxLeasePriceByLetter')
                    if min_lease_letter and max_lease_letter:
                        print(f"  ì „ì„¸: {min_lease_letter} ~ {max_lease_letter}")

                    # ìµœê·¼ ì‹¤ê±°ë˜ê°€
                    real_price = overview.get('realPrice')
                    if real_price:
                        # íƒ€ì… ì•ˆì „í•˜ê²Œ ë³€í™˜ (ë¬¸ìì—´/ì •ìˆ˜ ëª¨ë‘ ì²˜ë¦¬)
                        trade_year = str(real_price.get('tradeYear', ''))
                        trade_month = int(real_price.get('tradeMonth', 0)) if real_price.get('tradeMonth') else 0
                        trade_day = int(real_price.get('tradeDate', 0)) if real_price.get('tradeDate') else 0

                        trade_date = f"{trade_year}.{trade_month:02d}.{trade_day:02d}"
                        price = real_price.get('formattedPrice', '')
                        floor = real_price.get('floor', '')
                        area = real_price.get('representativeArea', '')
                        print(f"\nğŸ·ï¸  ìµœê·¼ ì‹¤ê±°ë˜ê°€")
                        print(f"  {trade_date} | {price} | {floor}ì¸µ | {area}ã¡")

                    # í‰í˜• ì •ë³´
                    pyeongs = overview.get('pyeongs', [])
                    if pyeongs:
                        print(f"\nğŸ  í‰í˜• ì¢…ë¥˜ ({len(pyeongs)}ê°œ)")
                        for pyeong in pyeongs:
                            supply_area = pyeong.get('supplyArea', '')
                            exclusive_area = pyeong.get('exclusiveArea', '')
                            pyeong_name = pyeong.get('pyeongName', '')
                            print(f"  {pyeong_name}ã¡ (ê³µê¸‰ {supply_area}ã¡ / ì „ìš© {exclusive_area}ã¡)")

                    # ë™ ì •ë³´
                    dongs = overview.get('dongs', [])
                    if dongs:
                        dong_names = [d.get('bildName', '') for d in dongs[:10]]  # ìµœëŒ€ 10ê°œë§Œ
                        dong_display = ', '.join(dong_names)
                        if len(dongs) > 10:
                            dong_display += f" ì™¸ {len(dongs) - 10}ê°œë™"
                        print(f"\nğŸ¢ ë™ ì •ë³´")
                        print(f"  {dong_display}")
                else:
                    print(f"âš ï¸ ë‹¨ì§€ ê°œìš” ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    # ê°œìš” ì—†ì´ë„ ë§¤ë¬¼ì€ ì‹œë„

                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                await asyncio.sleep(self.request_delay)

                # 2. ë§¤ë¬¼ ëª©ë¡ (ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹)
                articles = await self.crawl_complex_articles(complex_no, 1)
                if articles:
                    complex_data['articles'] = articles
                    article_count = len(articles.get('articleList', []))
                    print(f"ë§¤ë¬¼ ìˆ˜: {article_count}ê°œ")
                else:
                    print(f"âš ï¸ ë§¤ë¬¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì™„ë£Œ")
                return complex_data  # ì„±ê³µ ì‹œ ì¦‰ì‹œ ë°˜í™˜

            except Exception as e:
                error_msg = str(e)
                print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")

                # ì»¨í…ìŠ¤íŠ¸ íŒŒê´´ ì—ëŸ¬ì´ê³  ì¬ì‹œë„ ê°€ëŠ¥í•œ ê²½ìš°
                if "Execution context was destroyed" in error_msg or "Target page" in error_msg:
                    if attempt < max_attempts:
                        print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì—ëŸ¬ ë°œìƒ, ì¬ì‹œë„ ì˜ˆì •...")
                        continue
                    else:
                        print(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")

                # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬ê±°ë‚˜ ë§ˆì§€ë§‰ ì‹œë„
                import traceback
                traceback.print_exc()
                complex_data['error'] = str(e)
                return complex_data

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        complex_data['error'] = 'ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨'
        return complex_data

    async def crawl_multiple_complexes(self, complex_numbers: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ ë‹¨ì§€ í¬ë¡¤ë§"""
        results = []
        total = len(complex_numbers)
        
        for i, complex_no in enumerate(complex_numbers, 1):
            print(f"\nì§„í–‰ë¥ : {i}/{total}")
            
            # í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì „ì²´ ë§¤ë¬¼ ìˆ˜ ê³„ì‚°
            total_items_so_far = 0
            for r in results:
                if 'articles' in r and 'articleList' in r['articles']:
                    total_items_so_far += len(r['articles']['articleList'])
            
            # ë‹¨ì§€ ê°œìš” ìˆ˜ì§‘ ì „ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="running",
                progress=i - 1,
                total=total,
                current_complex=complex_no,
                message=f"ğŸ“‹ ë‹¨ì§€ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i}/{total})",
                items_collected=total_items_so_far
            )
            
            try:
                complex_data = await self.crawl_complex_data(complex_no)
                results.append(complex_data)
                
                # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
                article_count = 0
                if 'articles' in complex_data and 'articleList' in complex_data['articles']:
                    article_count = len(complex_data['articles']['articleList'])
                
                # ì „ì²´ ë§¤ë¬¼ ìˆ˜ ì¬ê³„ì‚°
                total_items_so_far = 0
                for r in results:
                    if 'articles' in r and 'articleList' in r['articles']:
                        total_items_so_far += len(r['articles']['articleList'])
                
                self.update_status(
                    status="running",
                    progress=i,
                    total=total,
                    current_complex=complex_no,
                    message=f"âœ… ì™„ë£Œ: {complex_data.get('overview', {}).get('complexName', complex_no)} ({article_count}ê°œ ë§¤ë¬¼)",
                    items_collected=total_items_so_far
                )
                
                # ë‹¨ì§€ ê°„ ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                if i < total:
                    await asyncio.sleep(self.request_delay * 2)
                    
            except Exception as e:
                print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results.append({
                    'complex_no': complex_no,
                    'error': str(e),
                    'crawling_date': get_kst_now().isoformat()
                })
                
                # ì‹¤íŒ¨ ì‹œì—ë„ ì „ì²´ ë§¤ë¬¼ ìˆ˜ ê³„ì‚°
                total_items_so_far = 0
                for r in results:
                    if 'articles' in r and 'articleList' in r['articles']:
                        total_items_so_far += len(r['articles']['articleList'])
                
                # ì‹¤íŒ¨ ì‹œì—ë„ ìƒíƒœ ì—…ë°ì´íŠ¸
                self.update_status(
                    status="running",
                    progress=i,
                    total=total,
                    current_complex=complex_no,
                    message=f"âŒ ì‹¤íŒ¨: {complex_no} - {str(e)[:50]}",
                    items_collected=total_items_so_far
                )
        
        return results

    def save_data(self, data: Any, filename_prefix: str = "naver_complex"):
        """ë°ì´í„° ì €ì¥"""
        timestamp = get_kst_now().strftime("%Y%m%d_%H%M%S")

        try:
            # ë‹¨ì§€ë²ˆí˜¸ ì¶”ì¶œ (íŒŒì¼ëª…ì— í¬í•¨í•˜ê¸° ìœ„í•´)
            complex_nos = []
            if isinstance(data, list):
                for item in data:
                    if 'crawling_info' in item and 'complex_no' in item['crawling_info']:
                        complex_nos.append(item['crawling_info']['complex_no'])
                    elif 'overview' in item and 'complexNo' in item['overview']:
                        complex_nos.append(item['overview']['complexNo'])

            # íŒŒì¼ëª…ì— ë‹¨ì§€ë²ˆí˜¸ í¬í•¨ (ì˜ˆ: complexes_3_22065-12345-67890_20251014_120000)
            complex_nos_str = '-'.join(complex_nos[:10]) if complex_nos else ''  # ìµœëŒ€ 10ê°œê¹Œì§€
            if complex_nos_str:
                filename = f"{filename_prefix}_{complex_nos_str}_{timestamp}"
            else:
                filename = f"{filename_prefix}_{timestamp}"

            # JSON ì €ì¥
            json_filename = self.output_dir / f"{filename}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"JSON ë°ì´í„° ì €ì¥: {json_filename}")

            # CSV ì €ì¥ (ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ì¸ ê²½ìš°)
            if isinstance(data, list) and data and isinstance(data[0], dict):
                # ë‹¨ì§€ ê°œìš” ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ì €ì¥
                csv_data = []
                for item in data:
                    if 'overview' in item:
                        overview = item['overview']
                        csv_data.append({
                            'ë‹¨ì§€ë²ˆí˜¸': overview.get('complexNo', ''),
                            'ë‹¨ì§€ëª…': overview.get('complexName', ''),
                            'ì„¸ëŒ€ìˆ˜': overview.get('totalHouseHoldCount', ''),
                            'ë™ìˆ˜': overview.get('totalDongCount', ''),
                            'ì‚¬ìš©ìŠ¹ì¸ì¼': overview.get('useApproveYmd', ''),
                            'ìµœì†Œë©´ì ': overview.get('minArea', ''),
                            'ìµœëŒ€ë©´ì ': overview.get('maxArea', ''),
                            'ìµœì†Œê°€ê²©': overview.get('minPrice', ''),
                            'ìµœëŒ€ê°€ê²©': overview.get('maxPrice', ''),
                            'ìœ„ë„': overview.get('latitude', ''),
                            'ê²½ë„': overview.get('longitude', ''),
                            'í¬ë¡¤ë§ì¼ì‹œ': item.get('crawling_info', {}).get('crawling_date', '')
                        })

                if csv_data:
                    df = pd.DataFrame(csv_data)
                    csv_filename = self.output_dir / f"{filename}.csv"
                    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                    print(f"CSV ë°ì´í„° ì €ì¥: {csv_filename}")

        except Exception as e:
            print(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    async def run_crawling(self, complex_numbers: List[str]):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        import time

        # ìƒíƒœ íŒŒì¼ ë° ì‹œì‘ ì‹œê°„ ì„¤ì •
        timestamp = get_kst_now().strftime("%Y%m%d_%H%M%S")
        self.status_file = self.output_dir / f"crawl_status_{timestamp}.json"
        self.start_time = get_kst_now()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡

        try:
            # ë¸Œë¼ìš°ì € ì„¤ì •
            setup_start = time.time()
            print("â±ï¸  ë¸Œë¼ìš°ì € ì„¤ì • ì‹œì‘...")
            await self.setup_browser()
            setup_duration = time.time() - setup_start
            print(f"â±ï¸  ë¸Œë¼ìš°ì € ì„¤ì • ì´ ì†Œìš”ì‹œê°„: {setup_duration:.2f}ì´ˆ")

            # í¬ë¡¤ë§ ì‹œì‘ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="running",
                progress=0,
                total=len(complex_numbers),
                message="ğŸš€ í¬ë¡¤ë§ ì‹œì‘ ì¤‘...",
                items_collected=0
            )
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            if len(complex_numbers) == 1:
                data = await self.crawl_complex_data(complex_numbers[0])
                results = [data]
            else:
                results = await self.crawl_multiple_complexes(complex_numbers)
            
            # ë°ì´í„° ì €ì¥
            self.save_data(results, f"complexes_{len(complex_numbers)}")
            
            # ê²°ê³¼ ìš”ì•½
            print(f"\n{'='*60}")
            print("í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"{'='*60}")
            print(f"ì´ {len(results)}ê°œ ë‹¨ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
            
            # ì„±ê³µ: articlesê°€ ìˆê³  articleListì— ë§¤ë¬¼ì´ ìˆëŠ” ê²½ìš°
            success_count = len([r for r in results if 'articles' in r and r.get('articles', {}).get('articleList')])
            # ì‹¤íŒ¨: errorê°€ ìˆê±°ë‚˜ ë§¤ë¬¼ì´ ì—†ëŠ” ê²½ìš°
            error_count = len(results) - success_count
            print(f"ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {error_count}ê°œ")
            
            # ì „ì²´ ìˆ˜ì§‘ëœ ë§¤ë¬¼ ìˆ˜ ê³„ì‚°
            total_items = 0
            for r in results:
                if 'articles' in r and 'articleList' in r['articles']:
                    total_items += len(r['articles']['articleList'])
            
            # í¬ë¡¤ë§ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="completed",
                progress=len(complex_numbers),
                total=len(complex_numbers),
                message=f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count}",
                items_collected=total_items
            )
            
            return results
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_status(
                status="error",
                progress=0,
                total=len(complex_numbers),
                message=f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}",
                items_collected=0
            )
            
            raise
        finally:
            await self.close_browser()


async def fetch_info_only(complex_no: str) -> Optional[Dict]:
    """ë‹¨ì§€ ì •ë³´ë§Œ ê°€ì ¸ì˜¤ëŠ” ë…ë¦½ í•¨ìˆ˜ (ë§¤ë¬¼ í¬ë¡¤ë§ ì—†ì´)"""
    print(f"[fetch_info_only] í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...", flush=True)
    crawler = NASNaverRealEstateCrawler()
    try:
        print(f"[fetch_info_only] ë¸Œë¼ìš°ì € ì„¤ì • ì‹œì‘...", flush=True)
        await crawler.setup_browser()
        print(f"[fetch_info_only] ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ, ë‹¨ì§€ ì •ë³´ ì¡°íšŒ ì‹œì‘...", flush=True)
        info = await crawler.fetch_complex_info_only(complex_no)
        print(f"[fetch_info_only] ë‹¨ì§€ ì •ë³´ ì¡°íšŒ ì™„ë£Œ", flush=True)
        return info
    except Exception as e:
        print(f"[fetch_info_only] ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
        import traceback
        traceback.print_exc()
        return None
    finally:
        print(f"[fetch_info_only] ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘...", flush=True)
        await crawler.close_browser()
        print(f"[fetch_info_only] ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ", flush=True)


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys

    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    # Usage:
    #   - Full crawl: python nas_playwright_crawler.py "22065,12345" [crawl_id]
    #   - Info only: python nas_playwright_crawler.py --info-only 22065

    if len(sys.argv) > 1 and sys.argv[1] == '--info-only':
        # ì •ë³´ë§Œ ê°€ì ¸ì˜¤ê¸° ëª¨ë“œ
        if len(sys.argv) < 3:
            print("Usage: python nas_playwright_crawler.py --info-only <complex_no>")
            sys.exit(1)

        complex_no = sys.argv[2].strip()
        print(f"ğŸ“‹ ë‹¨ì§€ ì •ë³´ë§Œ ì¡°íšŒ: {complex_no}")

        info = await fetch_info_only(complex_no)
        if info:
            # JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (Node.jsì—ì„œ íŒŒì‹± ê°€ëŠ¥)
            print("===INFO_START===")
            print(json.dumps(info, ensure_ascii=False))
            print("===INFO_END===")
        else:
            print("ERROR: Failed to fetch complex info")
            sys.exit(1)
        return

    # ì¼ë°˜ í¬ë¡¤ë§ ëª¨ë“œ
    crawl_id = None
    complex_numbers = ['22065']  # ê¸°ë³¸ê°’

    if len(sys.argv) > 1:
        complex_numbers = sys.argv[1].split(',')
        complex_numbers = [num.strip() for num in complex_numbers if num.strip()]

    if len(sys.argv) > 2:
        crawl_id = sys.argv[2].strip()
        print(f"ğŸ”— Crawl ID: {crawl_id}")

    print(f"ğŸ“‹ í¬ë¡¤ë§ ëŒ€ìƒ ë‹¨ì§€: {complex_numbers}")

    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (crawl_id ì „ë‹¬)
    crawler = NASNaverRealEstateCrawler(crawl_id=crawl_id)

    # í¬ë¡¤ë§ ì‹¤í–‰
    await crawler.run_crawling(complex_numbers)


if __name__ == "__main__":
    asyncio.run(main())
