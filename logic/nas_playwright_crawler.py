#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NAS í™˜ê²½ìš© ë„¤ì´ë²„ ë¶€ë™ì‚° Playwright í¬ë¡¤ëŸ¬
í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ë™ì‘í•˜ì—¬ ìŠ¤í¬ë¦°ì´ ì—†ëŠ” NASì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥
"""

import asyncio
import json
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

from playwright.async_api import async_playwright, Browser, Page, BrowserContext
import pandas as pd
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class NASNaverRealEstateCrawler:
    """NAS í™˜ê²½ìš© ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.results = []
        self.output_dir = Path(os.getenv('OUTPUT_DIR', './crawled_data'))
        self.output_dir.mkdir(exist_ok=True)
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.request_delay = float(os.getenv('REQUEST_DELAY', '2.0'))  # ìš”ì²­ ê°„ê²© (ì´ˆ)
        self.timeout = int(os.getenv('TIMEOUT', '30000'))  # íƒ€ì„ì•„ì›ƒ (ë°€ë¦¬ì´ˆ)
        self.headless = os.getenv('HEADLESS', 'true').lower() == 'true'
        
        print(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"- ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"- ìš”ì²­ ê°„ê²©: {self.request_delay}ì´ˆ")
        print(f"- í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ: {self.headless}")
        print(f"- íƒ€ì„ì•„ì›ƒ: {self.timeout}ms")

    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™”"""
        try:
            playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (NAS í™˜ê²½ì— ìµœì í™”)
            browser_options = {
                'headless': self.headless,
                'args': [
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--memory-pressure-off',
                    '--max_old_space_size=4096'
                ]
            }
            
            # Chrome ë¸Œë¼ìš°ì € ì‹¤í–‰
            self.browser = await playwright.chromium.launch(**browser_options)
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¿ í‚¤, ì„¸ì…˜ ê´€ë¦¬)
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                extra_http_headers={
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                }
            )
            
            # í˜ì´ì§€ ìƒì„±
            self.page = await self.context.new_page()
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.page.set_default_timeout(self.timeout)
            
            print("ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            print("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    async def crawl_complex_overview(self, complex_no: str) -> Optional[Dict]:
        """ë‹¨ì§€ ê°œìš” ì •ë³´ í¬ë¡¤ë§"""
        try:
            print(f"ë‹¨ì§€ ê°œìš” ì •ë³´ í¬ë¡¤ë§ ì‹œì‘: {complex_no}")
            
            # ë„¤ì´ë²„ ë¶€ë™ì‚° ë‹¨ì§€ í˜ì´ì§€ ì ‘ì†
            url = f"https://new.land.naver.com/complexes/{complex_no}"
            await self.page.goto(url, wait_until='networkidle')
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            await asyncio.sleep(3)
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§í•˜ì—¬ API ì‘ë‹µ ìºì¹˜
            overview_data = None
            
            async def handle_response(response):
                nonlocal overview_data
                if f'/api/complexes/overview/{complex_no}' in response.url:
                    try:
                        data = await response.json()
                        overview_data = data
                        print(f"ë‹¨ì§€ ê°œìš” API ì‘ë‹µ ìºì¹˜ë¨")
                    except Exception as e:
                        print(f"API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ë“±ë¡
            self.page.on('response', handle_response)
            
            # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ API í˜¸ì¶œ íŠ¸ë¦¬ê±°
            await self.page.reload(wait_until='networkidle')
            await asyncio.sleep(2)
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ì œê±°
            self.page.remove_listener('response', handle_response)
            
            return overview_data
            
        except Exception as e:
            print(f"ë‹¨ì§€ ê°œìš” í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    async def crawl_complex_articles_with_scroll(self, complex_no: str) -> Optional[Dict]:
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§"""
        try:
            print(f"ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§ ì‹œì‘ (ë¬´í•œ ìŠ¤í¬ë¡¤): {complex_no}")
            
            # ëª¨ë“  ë§¤ë¬¼ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            all_articles = []
            collected_article_ids = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            # API ì‘ë‹µ ìˆ˜ì§‘
            async def handle_articles_response(response):
                # ë§¤ë¬¼ ëª©ë¡ API ì‘ë‹µ ê°ì§€
                if f'/api/articles/complex/{complex_no}' in response.url:
                    print(f"[DEBUG] API í˜¸ì¶œ ê°ì§€")
                    try:
                        data = await response.json()
                        article_list = data.get('articleList', [])
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        new_count = 0
                        for article in article_list:
                            article_id = article.get('articleNo') or article.get('id')
                            if article_id and article_id not in collected_article_ids:
                                collected_article_ids.add(article_id)
                                all_articles.append(article)
                                new_count += 1
                        
                        if new_count > 0:
                            print(f"  â†’ {new_count}ê°œ ìƒˆ ë§¤ë¬¼ ì¶”ê°€ (ì´ {len(all_articles)}ê°œ)")
                    except Exception as e:
                        print(f"ë§¤ë¬¼ API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ë“±ë¡
            self.page.on('response', handle_articles_response)
            
            try:
                # 1. ë‹¨ì§€ í˜ì´ì§€ë¡œ ì´ë™
                url = f"https://new.land.naver.com/complexes/{complex_no}"
                print(f"URL ì ‘ì†: {url}")
                await self.page.goto(url, wait_until='networkidle')
                await asyncio.sleep(3)
                
                # 2. ë§¤ë¬¼ íƒ­ í´ë¦­
                print("ë§¤ë¬¼ íƒ­ ì°¾ëŠ” ì¤‘...")
                try:
                    selectors = [
                        'a[href*="article"]',
                        'button:has-text("ë§¤ë¬¼")',
                        '[class*="article"]',
                    ]
                    
                    for selector in selectors:
                        try:
                            element = await self.page.wait_for_selector(selector, timeout=5000)
                            if element:
                                await element.click()
                                print(f"ë§¤ë¬¼ íƒ­ í´ë¦­ ì„±ê³µ")
                                await asyncio.sleep(3)
                                break
                        except:
                            continue
                except Exception as e:
                    print(f"ë§¤ë¬¼ íƒ­ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # 3. ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                print("ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì°¾ëŠ” ì¤‘...")
                list_container = None
                container_selectors = [
                    '[class*="list"]',
                    '[class*="article"]',
                    '[class*="item"]',
                ]
                
                for selector in container_selectors:
                    try:
                        list_container = await self.page.wait_for_selector(selector, timeout=5000)
                        if list_container:
                            print(f"ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                            break
                    except:
                        continue
                
                # 4. ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸°
                await asyncio.sleep(3)
                initial_count = len(all_articles)
                print(f"ì´ˆê¸° ë§¤ë¬¼ ìˆ˜: {initial_count}ê°œ")
                
                # 5. "ë”ë³´ê¸°" ë²„íŠ¼ ë˜ëŠ” ìŠ¤í¬ë¡¤ë¡œ ë°ì´í„° ìˆ˜ì§‘
                print("ì¶”ê°€ ë§¤ë¬¼ ìˆ˜ì§‘ ì‹œì‘...")
                scroll_attempts = 0
                max_scroll_attempts = 50  # ìµœëŒ€ ì‹œë„ íšŸìˆ˜
                no_new_data_count = 0
                max_no_new_data = 3  # 3ë²ˆ ì—°ì† ìƒˆ ë°ì´í„° ì—†ìœ¼ë©´ ì¤‘ë‹¨
                
                while scroll_attempts < max_scroll_attempts:
                    prev_count = len(all_articles)
                    
                    # ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ íŒ¨ë„ ìŠ¤í¬ë¡¤ - ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê¸°ë°˜
                    scroll_result = await self.page.evaluate('''
                        () => {
                            // 1. ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì°¾ê¸°
                            const itemSelectors = [
                                'div[class*="list_contents"] > div',
                                'div[class*="article_list"] > div',
                                'div[class*="item"]',
                                '[class*="list"] > div[class*="article"]'
                            ];
                            
                            let items = [];
                            for (const selector of itemSelectors) {
                                items = Array.from(document.querySelectorAll(selector));
                                if (items.length > 5) break;  // ì¶©ë¶„í•œ ì•„ì´í…œì´ ìˆìœ¼ë©´ ì‚¬ìš©
                            }
                            
                            if (items.length === 0) {
                                return { scrolled: false, reason: 'no items found' };
                            }
                            
                            // 2. ë§ˆì§€ë§‰ ì•„ì´í…œìœ¼ë¡œ ìŠ¤í¬ë¡¤
                            const lastItem = items[items.length - 1];
                            const beforeScroll = lastItem.getBoundingClientRect().top;
                            
                            // scrollIntoView ì‚¬ìš© (ë” í™•ì‹¤í•¨)
                            lastItem.scrollIntoView({ behavior: 'auto', block: 'end' });
                            
                            const afterScroll = lastItem.getBoundingClientRect().top;
                            
                            // 3. ë¶€ëª¨ ì»¨í…Œì´ë„ˆ ì •ë³´
                            let container = lastItem.parentElement;
                            while (container && container.scrollHeight <= container.clientHeight) {
                                container = container.parentElement;
                            }
                            
                            return {
                                scrolled: true,
                                itemCount: items.length,
                                lastItemMoved: Math.abs(beforeScroll - afterScroll) > 10,
                                beforeTop: beforeScroll,
                                afterTop: afterScroll,
                                container: container ? {
                                    className: container.className.substring(0, 50),
                                    scrollHeight: container.scrollHeight,
                                    clientHeight: container.clientHeight,
                                    scrollTop: container.scrollTop
                                } : null
                            };
                        }
                    ''')
                        
                    if scroll_attempts == 0:
                        if scroll_result.get('scrolled'):
                            print(f"[DEBUG] ë§¤ë¬¼ ì•„ì´í…œ {scroll_result.get('itemCount', 0)}ê°œ ë°œê²¬")
                            print(f"  ë§ˆì§€ë§‰ ì•„ì´í…œ ìŠ¤í¬ë¡¤: {scroll_result.get('lastItemMoved', False)}")
                            if scroll_result.get('container'):
                                print(f"  ì»¨í…Œì´ë„ˆ: {scroll_result['container']}")
                        else:
                            print(f"[DEBUG] ë§¤ë¬¼ ì•„ì´í…œì„ ì°¾ì§€ ëª»í•¨: {scroll_result.get('reason', 'unknown')}")
                    
                    await asyncio.sleep(5)  # API í˜¸ì¶œ ëŒ€ê¸° (ì¦ê°€)
                    
                    current_count = len(all_articles)
                    new_items = current_count - prev_count
                    
                    scroll_attempts += 1
                    
                    if new_items > 0:
                        no_new_data_count = 0
                        print(f"ì‹œë„ {scroll_attempts}íšŒ: {new_items}ê°œ ì¶”ê°€ (ì´ {current_count}ê°œ)")
                    else:
                        no_new_data_count += 1
                        print(f"ì‹œë„ {scroll_attempts}íšŒ: ìŠ¤í¬ë¡¤í–ˆì§€ë§Œ ìƒˆ ë°ì´í„° ì—†ìŒ ({no_new_data_count}/{max_no_new_data})")
                        
                        if no_new_data_count >= max_no_new_data:
                            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ - {max_no_new_data}íšŒ ì—°ì† ìƒˆ ë°ì´í„° ì—†ìŒ")
                            break
                
                if len(all_articles) > initial_count:
                    print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ: ì´ˆê¸° {initial_count}ê°œ â†’ ìµœì¢… {len(all_articles)}ê°œ (ì´ {scroll_attempts}íšŒ ì‹œë„)")
                else:
                    print(f"âš ï¸  ì¶”ê°€ ìˆ˜ì§‘ ì‹¤íŒ¨: {initial_count}ê°œì—ì„œ ë³€í™” ì—†ìŒ (ì´ {scroll_attempts}íšŒ ì‹œë„)")
                    print(f"   â†’ ì‹¤ì œë¡œ {initial_count}ê°œë§Œ ìˆê±°ë‚˜, ìŠ¤í¬ë¡¤/ë²„íŠ¼ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ")
                
            except Exception as e:
                print(f"ìŠ¤í¬ë¡¤ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì‘ë‹µ í•¸ë“¤ëŸ¬ ì œê±°
            self.page.remove_listener('response', handle_articles_response)
            
            if all_articles:
                return {
                    'articleList': all_articles,
                    'totalCount': len(all_articles),
                    'isMoreData': False
                }
            else:
                print("ë§¤ë¬¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            print(f"ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    async def crawl_complex_articles(self, complex_no: str, page_num: int = 1) -> Optional[Dict]:
        """ë‹¨ì§€ ë§¤ë¬¼ ëª©ë¡ í¬ë¡¤ë§"""
        # ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë§¤ë¬¼ ìˆ˜ì§‘
        if page_num == 1:
            return await self.crawl_complex_articles_with_scroll(complex_no)
        else:
            return None

    async def crawl_complex_data(self, complex_no: str) -> Dict:
        """ë‹¨ì§€ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§"""
        print(f"\n{'='*60}")
        print(f"ë‹¨ì§€ ë²ˆí˜¸ {complex_no} í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*60}")
        
        complex_data = {
            'crawling_info': {
                'complex_no': complex_no,
                'crawling_date': datetime.now().isoformat(),
                'crawler_version': '1.0.0'
            }
        }
        
        try:
            # 1. ë‹¨ì§€ ê°œìš” ì •ë³´
            overview = await self.crawl_complex_overview(complex_no)
            if overview:
                complex_data['overview'] = overview
                print(f"ë‹¨ì§€ëª…: {overview.get('complexName', 'Unknown')}")
                print(f"ì„¸ëŒ€ìˆ˜: {overview.get('totalHouseHoldCount', 'Unknown')}")
                print(f"ë™ìˆ˜: {overview.get('totalDongCount', 'Unknown')}")
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            await asyncio.sleep(self.request_delay)
            
            # 2. ë§¤ë¬¼ ëª©ë¡ (ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹)
            articles = await self.crawl_complex_articles(complex_no, 1)
            if articles:
                complex_data['articles'] = articles
                article_count = len(articles.get('articleList', []))
                print(f"ë§¤ë¬¼ ìˆ˜: {article_count}ê°œ")
            
            print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            complex_data['error'] = str(e)
        
        return complex_data

    async def crawl_multiple_complexes(self, complex_numbers: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ ë‹¨ì§€ í¬ë¡¤ë§"""
        results = []
        
        for i, complex_no in enumerate(complex_numbers, 1):
            print(f"\nì§„í–‰ë¥ : {i}/{len(complex_numbers)}")
            
            try:
                complex_data = await self.crawl_complex_data(complex_no)
                results.append(complex_data)
                
                # ë‹¨ì§€ ê°„ ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                if i < len(complex_numbers):
                    await asyncio.sleep(self.request_delay * 2)
                    
            except Exception as e:
                print(f"ë‹¨ì§€ {complex_no} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results.append({
                    'complex_no': complex_no,
                    'error': str(e),
                    'crawling_date': datetime.now().isoformat()
                })
        
        return results

    def save_data(self, data: Any, filename_prefix: str = "naver_complex"):
        """ë°ì´í„° ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # JSON ì €ì¥
            json_filename = self.output_dir / f"{filename_prefix}_{timestamp}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"JSON ë°ì´í„° ì €ì¥: {json_filename}")
            
            # CSV ì €ì¥ (ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ì¸ ê²½ìš°)
            if isinstance(data, list) and data and isinstance(data[0], dict):
                # ë‹¨ì§€ ê°œìš” ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ì €ì¥
                csv_data = []
                for item in data:
                    if 'overview' in item:
                        overview = item['overview']
                        csv_data.append({
                            'ë‹¨ì§€ë²ˆí˜¸': overview.get('complexNo', ''),
                            'ë‹¨ì§€ëª…': overview.get('complexName', ''),
                            'ì„¸ëŒ€ìˆ˜': overview.get('totalHouseHoldCount', ''),
                            'ë™ìˆ˜': overview.get('totalDongCount', ''),
                            'ì‚¬ìš©ìŠ¹ì¸ì¼': overview.get('useApproveYmd', ''),
                            'ìµœì†Œë©´ì ': overview.get('minArea', ''),
                            'ìµœëŒ€ë©´ì ': overview.get('maxArea', ''),
                            'ìµœì†Œê°€ê²©': overview.get('minPrice', ''),
                            'ìµœëŒ€ê°€ê²©': overview.get('maxPrice', ''),
                            'ìœ„ë„': overview.get('latitude', ''),
                            'ê²½ë„': overview.get('longitude', ''),
                            'í¬ë¡¤ë§ì¼ì‹œ': item.get('crawling_info', {}).get('crawling_date', '')
                        })
                
                if csv_data:
                    df = pd.DataFrame(csv_data)
                    csv_filename = self.output_dir / f"{filename_prefix}_{timestamp}.csv"
                    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                    print(f"CSV ë°ì´í„° ì €ì¥: {csv_filename}")
            
        except Exception as e:
            print(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    async def run_crawling(self, complex_numbers: List[str]):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            # ë¸Œë¼ìš°ì € ì„¤ì •
            await self.setup_browser()
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            if len(complex_numbers) == 1:
                data = await self.crawl_complex_data(complex_numbers[0])
                results = [data]
            else:
                results = await self.crawl_multiple_complexes(complex_numbers)
            
            # ë°ì´í„° ì €ì¥
            self.save_data(results, f"complexes_{len(complex_numbers)}")
            
            # ê²°ê³¼ ìš”ì•½
            print(f"\n{'='*60}")
            print("í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"{'='*60}")
            print(f"ì´ {len(results)}ê°œ ë‹¨ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
            
            success_count = len([r for r in results if 'overview' in r])
            error_count = len([r for r in results if 'error' in r])
            print(f"ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {error_count}ê°œ")
            
            return results
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        finally:
            await self.close_browser()


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = NASNaverRealEstateCrawler()
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        complex_numbers = sys.argv[1].split(',')
        complex_numbers = [num.strip() for num in complex_numbers if num.strip()]
    else:
        # ê¸°ë³¸ê°’: ë™íƒ„ì‹œë²”ë‹¤ì€ë§ˆì„ì›”ë“œë©”ë¥´ë””ì•™ë°˜ë„ìœ ë³´ë¼
        complex_numbers = ['22065']
    
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ë‹¨ì§€: {complex_numbers}")
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    await crawler.run_crawling(complex_numbers)


if __name__ == "__main__":
    asyncio.run(main())
